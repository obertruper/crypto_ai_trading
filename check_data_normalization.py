#!/usr/bin/env python3
"""
–ü—Ä–æ–≤–µ—Ä–∫–∞ –∏ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –∫—ç—à–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
"""

import pandas as pd
import numpy as np
from pathlib import Path
from utils.logger import get_logger

def check_data_stats():
    """–ü—Ä–æ–≤–µ—Ä–∫–∞ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –∫—ç—à–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö"""
    logger = get_logger("DataCheck")
    
    logger.info("üîç –ü—Ä–æ–≤–µ—Ä–∫–∞ –∫—ç—à–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö...")
    
    # –ü—É—Ç–∏ –∫ —Ñ–∞–π–ª–∞–º
    data_dir = Path("data/processed")
    files = {
        "train": data_dir / "train_data.parquet",
        "val": data_dir / "val_data.parquet",
        "test": data_dir / "test_data.parquet"
    }
    
    for name, file_path in files.items():
        if not file_path.exists():
            logger.error(f"‚ùå –§–∞–π–ª {file_path} –Ω–µ –Ω–∞–π–¥–µ–Ω!")
            continue
            
        logger.info(f"\nüìä –ê–Ω–∞–ª–∏–∑ {name} –¥–∞–Ω–Ω—ã—Ö:")
        df = pd.read_parquet(file_path)
        
        # –û–±—â–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è
        logger.info(f"  –†–∞–∑–º–µ—Ä: {df.shape}")
        logger.info(f"  –ü–∞–º—è—Ç—å: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB")
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ NaN
        nan_counts = df.isna().sum()
        if nan_counts.any():
            logger.warning(f"  ‚ö†Ô∏è –ù–∞–π–¥–µ–Ω—ã NaN –∑–Ω–∞—á–µ–Ω–∏—è:")
            for col in nan_counts[nan_counts > 0].index:
                logger.warning(f"    - {col}: {nan_counts[col]} NaN")
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –±–µ—Å–∫–æ–Ω–µ—á–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è
        numeric_cols = df.select_dtypes(include=[np.number]).columns
        inf_counts = np.isinf(df[numeric_cols]).sum()
        if inf_counts.any():
            logger.warning(f"  ‚ö†Ô∏è –ù–∞–π–¥–µ–Ω—ã inf –∑–Ω–∞—á–µ–Ω–∏—è:")
            for col in inf_counts[inf_counts > 0].index:
                logger.warning(f"    - {col}: {inf_counts[col]} inf")
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ —á–∏—Å–ª–æ–≤—ã–º –∫–æ–ª–æ–Ω–∫–∞–º
        logger.info("\n  üìà –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –ø—Ä–∏–∑–Ω–∞–∫–∞–º:")
        
        # –û—Ç–¥–µ–ª—å–Ω–æ –¥–ª—è –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –∏ —Ü–µ–ª–µ–≤—ã—Ö –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö
        feature_cols = [col for col in numeric_cols if not any(
            x in col for x in ['future_return', 'long_', 'short_', 'best_direction', 'target_return']
        )]
        target_cols = [col for col in numeric_cols if any(
            x in col for x in ['future_return', 'long_', 'short_', 'best_direction', 'target_return']
        )]
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
        if feature_cols:
            feature_stats = df[feature_cols].describe()
            logger.info(f"\n  –ü—Ä–∏–∑–Ω–∞–∫–∏ ({len(feature_cols)} –∫–æ–ª–æ–Ω–æ–∫):")
            logger.info(f"    Min –∑–Ω–∞—á–µ–Ω–∏—è: {feature_stats.loc['min'].min():.4f}")
            logger.info(f"    Max –∑–Ω–∞—á–µ–Ω–∏—è: {feature_stats.loc['max'].max():.4f}")
            logger.info(f"    –°—Ä–µ–¥–Ω–µ–µ: {feature_stats.loc['mean'].mean():.4f}")
            logger.info(f"    Std: {feature_stats.loc['std'].mean():.4f}")
            
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –æ—á–µ–Ω—å –±–æ–ª—å—à–∏–µ –∑–Ω–∞—á–µ–Ω–∏—è
            large_values = (df[feature_cols].abs() > 1000).any()
            if large_values.any():
                logger.warning(f"\n  ‚ö†Ô∏è –ö–æ–ª–æ–Ω–∫–∏ —Å –æ—á–µ–Ω—å –±–æ–ª—å—à–∏–º–∏ –∑–Ω–∞—á–µ–Ω–∏—è–º–∏ (>1000):")
                for col in large_values[large_values].index:
                    max_val = df[col].abs().max()
                    logger.warning(f"    - {col}: max={max_val:.2f}")
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ —Ü–µ–ª–µ–≤—ã—Ö –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö
        if target_cols:
            target_stats = df[target_cols].describe()
            logger.info(f"\n  –¶–µ–ª–µ–≤—ã–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ ({len(target_cols)} –∫–æ–ª–æ–Ω–æ–∫):")
            logger.info(f"    Min –∑–Ω–∞—á–µ–Ω–∏—è: {target_stats.loc['min'].min():.4f}")
            logger.info(f"    Max –∑–Ω–∞—á–µ–Ω–∏—è: {target_stats.loc['max'].max():.4f}")
            logger.info(f"    –°—Ä–µ–¥–Ω–µ–µ: {target_stats.loc['mean'].mean():.4f}")
            logger.info(f"    Std: {target_stats.loc['std'].mean():.4f}")
            
        logger.info("=" * 80)

def normalize_cached_data():
    """–ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –∫—ç—à–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –µ—Å–ª–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ"""
    logger = get_logger("DataNormalize")
    
    logger.info("üîß –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏...")
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ç–æ–ª—å–∫–æ train –¥–∞–Ω–Ω—ã–µ –¥–ª—è —Ä–µ—à–µ–Ω–∏—è
    train_file = Path("data/processed/train_data.parquet")
    if not train_file.exists():
        logger.error("‚ùå Train —Ñ–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω!")
        return
        
    df = pd.read_parquet(train_file)
    numeric_cols = df.select_dtypes(include=[np.number]).columns
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –º–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è
    max_values = df[numeric_cols].abs().max()
    needs_normalization = (max_values > 1000).any()
    
    if needs_normalization:
        logger.warning("‚ö†Ô∏è –û–±–Ω–∞—Ä—É–∂–µ–Ω—ã –æ—á–µ–Ω—å –±–æ–ª—å—à–∏–µ –∑–Ω–∞—á–µ–Ω–∏—è, —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è –ø–µ—Ä–µ—Å–æ–∑–¥–∞—Ç—å –¥–∞—Ç–∞—Å–µ—Ç!")
        logger.info("–ó–∞–ø—É—Å—Ç–∏—Ç–µ: python prepare_trading_data.py --force-recreate")
    else:
        logger.info("‚úÖ –î–∞–Ω–Ω—ã–µ –≤ –Ω–æ—Ä–º–∞–ª—å–Ω—ã—Ö –ø—Ä–µ–¥–µ–ª–∞—Ö, –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –Ω–µ —Ç—Ä–µ–±—É–µ—Ç—Å—è")
        
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ NaN/Inf
    has_nan = df[numeric_cols].isna().any().any()
    has_inf = np.isinf(df[numeric_cols]).any().any()
    
    if has_nan or has_inf:
        logger.warning("‚ö†Ô∏è –û–±–Ω–∞—Ä—É–∂–µ–Ω—ã NaN –∏–ª–∏ Inf –∑–Ω–∞—á–µ–Ω–∏—è!")
        logger.info("–†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è –ø–µ—Ä–µ—Å–æ–∑–¥–∞—Ç—å –¥–∞—Ç–∞—Å–µ—Ç –∏–ª–∏ –æ—á–∏—Å—Ç–∏—Ç—å –¥–∞–Ω–Ω—ã–µ")

if __name__ == "__main__":
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
    check_data_stats()
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏
    print()
    normalize_cached_data()