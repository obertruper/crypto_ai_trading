"""
–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏ –∏ –∞–Ω–∞–ª–∏–∑ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è
"""
import torch
import numpy as np
from pathlib import Path
import yaml
from tqdm import tqdm
from collections import defaultdict

# –ó–∞–≥—Ä—É–∑–∫–∞ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
with open('config/config.yaml', 'r') as f:
    config = yaml.safe_load(f)

# –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
checkpoint = torch.load('models_saved/best_model.pth', map_location=device)

# –°–æ–∑–¥–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏
from models.patchtst_unified import UnifiedPatchTSTForTrading

# –ò—Å–ø–æ–ª—å–∑—É–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é –∏–∑ checkpoint
if 'config' in checkpoint:
    saved_config = checkpoint['config']
else:
    saved_config = config

model = UnifiedPatchTSTForTrading(saved_config)
model.load_state_dict(checkpoint['model_state_dict'])
model = model.to(device)
model.eval()

print("‚úÖ –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞ —É—Å–ø–µ—à–Ω–æ")

# –ó–∞–≥—Ä—É–∑–∫–∞ —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö
from data.precomputed_dataset import create_precomputed_data_loaders
from data.data_processor import DataProcessor

print("\nüìä –ó–∞–≥—Ä—É–∑–∫–∞ —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö...")
processor = DataProcessor(config)
_, _, test_data = processor.load_cached_data()

# –°–æ–∑–¥–∞–Ω–∏–µ —Ç–æ–ª—å–∫–æ test loader
_, _, test_loader = create_precomputed_data_loaders(
    test_data, test_data, test_data,  # –∏—Å–ø–æ–ª—å–∑—É–µ–º test_data –¥–ª—è –≤—Å–µ—Ö
    config,
    feature_cols=None,
    target_cols=None
)

print(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(test_loader)} –±–∞—Ç—á–µ–π —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö")

# –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏
print("\nüîç –ê–Ω–∞–ª–∏–∑ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π –Ω–∞ —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö...")

all_predictions = []
all_targets = []
direction_predictions = []
direction_targets = []

with torch.no_grad():
    for batch_idx, (inputs, targets, info) in enumerate(tqdm(test_loader, desc="–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ")):
        if batch_idx > 100:  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–ª—è –±—ã—Å—Ç—Ä–æ—Ç—ã
            break
            
        inputs = inputs.to(device)
        targets = targets.to(device)
        
        # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ
        outputs = model(inputs)
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º direction –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è (–∏–Ω–¥–µ–∫—Å 4 - direction_15m)
        if hasattr(outputs, '_direction_logits'):
            dir_logits = outputs._direction_logits[:, 0, :]  # 15m
            dir_preds = torch.argmax(torch.softmax(dir_logits, dim=-1), dim=-1)
            direction_predictions.extend(dir_preds.cpu().numpy())
            direction_targets.extend(targets[:, 0, 4].cpu().numpy())

# –ê–Ω–∞–ª–∏–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
direction_predictions = np.array(direction_predictions)
direction_targets = np.array(direction_targets)

# –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π
pred_counts = np.bincount(direction_predictions.astype(int), minlength=3)
true_counts = np.bincount(direction_targets.astype(int), minlength=3)

print("\nüìä –†–ï–ó–£–õ–¨–¢–ê–¢–´ –¢–ï–°–¢–ò–†–û–í–ê–ù–ò–Ø:")
print("="*50)

print("\nüéØ –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–ª–∞—Å—Å–æ–≤:")
print(f"–ò—Å—Ç–∏–Ω–Ω–æ–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ:  LONG={true_counts[0]/len(direction_targets)*100:.1f}%, SHORT={true_counts[1]/len(direction_targets)*100:.1f}%, FLAT={true_counts[2]/len(direction_targets)*100:.1f}%")
print(f"–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –º–æ–¥–µ–ª–∏:     LONG={pred_counts[0]/len(direction_predictions)*100:.1f}%, SHORT={pred_counts[1]/len(direction_predictions)*100:.1f}%, FLAT={pred_counts[2]/len(direction_predictions)*100:.1f}%")

# –¢–æ—á–Ω–æ—Å—Ç—å –ø–æ –∫–ª–∞—Å—Å–∞–º
from sklearn.metrics import classification_report, confusion_matrix

print("\nüìà –ú–µ—Ç—Ä–∏–∫–∏ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏:")
print(classification_report(direction_targets.astype(int), direction_predictions.astype(int), 
                          target_names=['LONG', 'SHORT', 'FLAT']))

# Confusion matrix
cm = confusion_matrix(direction_targets.astype(int), direction_predictions.astype(int))
print("\nüî¢ Confusion Matrix:")
print("     Pred:  LONG  SHORT  FLAT")
print(f"True LONG:  {cm[0, 0]:4d}  {cm[0, 1]:4d}  {cm[0, 2]:4d}")
print(f"True SHORT: {cm[1, 0]:4d}  {cm[1, 1]:4d}  {cm[1, 2]:4d}")
print(f"True FLAT:  {cm[2, 0]:4d}  {cm[2, 1]:4d}  {cm[2, 2]:4d}")

# Direction accuracy
accuracy = (direction_predictions == direction_targets).mean()
print(f"\n‚úÖ Direction Accuracy: {accuracy*100:.2f}%")

# –ê–Ω–∞–ª–∏–∑ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è
print("\n‚ö†Ô∏è –ê–ù–ê–õ–ò–ó –ü–ï–†–ï–û–ë–£–ß–ï–ù–ò–Ø:")
print("="*50)
print(f"Train Loss (–∏–∑ –ª–æ–≥–æ–≤): 0.619")
print(f"Val Loss (–∏–∑ –ª–æ–≥–æ–≤): 2.218")
print(f"Overfitting Ratio: {2.218/0.619:.2f}x")
print("\nüî¥ –ö–†–ò–¢–ò–ß–ï–°–ö–û–ï –ü–ï–†–ï–û–ë–£–ß–ï–ù–ò–ï! Val Loss –≤ 3.6 —Ä–∞–∑–∞ –±–æ–ª—å—à–µ Train Loss")

print("\nüí° –†–ï–ö–û–ú–ï–ù–î–ê–¶–ò–ò:")
print("1. –£–≤–µ–ª–∏—á–∏—Ç—å dropout —Å 0.5 –¥–æ 0.7")
print("2. –£–≤–µ–ª–∏—á–∏—Ç—å weight_decay —Å 0.01 –¥–æ 0.1") 
print("3. –£–º–µ–Ω—å—à–∏—Ç—å —Ä–∞–∑–º–µ—Ä –º–æ–¥–µ–ª–∏ (d_model, d_ff)")
print("4. –î–æ–±–∞–≤–∏—Ç—å –±–æ–ª—å—à–µ –¥–∞–Ω–Ω—ã—Ö –∏–ª–∏ –∞—É–≥–º–µ–Ω—Ç–∞—Ü–∏–∏")
print("5. –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —Ä–∞–Ω–Ω—é—é –æ—Å—Ç–∞–Ω–æ–≤–∫—É —Å patience=10 (–≤–º–µ—Å—Ç–æ 30)")