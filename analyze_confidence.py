"""–ê–Ω–∞–ª–∏–∑ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–∏ –≤ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è—Ö"""
import torch
import torch.nn.functional as F
import numpy as np
import yaml
from pathlib import Path

# –ó–∞–≥—Ä—É–∑–∫–∞ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
with open('config/config.yaml', 'r') as f:
    config = yaml.safe_load(f)

# –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
checkpoint = torch.load('models_saved/best_model.pth', map_location=device)

print("üîç –ê–ù–ê–õ–ò–ó –£–í–ï–†–ï–ù–ù–û–°–¢–ò –ú–û–î–ï–õ–ò –í –ü–†–ï–î–°–ö–ê–ó–ê–ù–ò–Ø–•")
print("="*60)

# –°–æ–∑–¥–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏
from models.patchtst_unified import UnifiedPatchTSTForTrading
model = UnifiedPatchTSTForTrading(config)
model.load_state_dict(checkpoint['model_state_dict'])
model = model.to(device)
model.eval()

# –ó–∞–≥—Ä—É–∑–∫–∞ –Ω–µ–±–æ–ª—å—à–æ–π –≤—ã–±–æ—Ä–∫–∏ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
from data.precomputed_dataset import PrecomputedDataset
import pandas as pd

print("\nüìä –ó–∞–≥—Ä—É–∑–∫–∞ —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö...")
# –ò—Å–ø–æ–ª—å–∑—É–µ–º –∫—ç—à–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
cache_path = Path('cache/precomputed/val_w96_s4.h5')
if cache_path.exists():
    import h5py
    with h5py.File(cache_path, 'r') as f:
        # –ë–µ—Ä–µ–º –ø–µ—Ä–≤—ã–µ 1000 –ø—Ä–∏–º–µ—Ä–æ–≤
        X_sample = torch.FloatTensor(f['X'][:1000])
        y_sample = torch.FloatTensor(f['y'][:1000])
        
    print(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(X_sample)} –ø—Ä–∏–º–µ—Ä–æ–≤")
    
    # –ê–Ω–∞–ª–∏–∑ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π
    print("\nüéØ –ê–Ω–∞–ª–∏–∑ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏ –≤ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è—Ö –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–π...")
    
    all_confidences = []
    all_entropies = []
    predictions_by_confidence = {
        'high': {'LONG': 0, 'SHORT': 0, 'FLAT': 0},
        'medium': {'LONG': 0, 'SHORT': 0, 'FLAT': 0},
        'low': {'LONG': 0, 'SHORT': 0, 'FLAT': 0}
    }
    
    with torch.no_grad():
        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –±–∞—Ç—á–∞–º–∏
        batch_size = 100
        for i in range(0, len(X_sample), batch_size):
            batch_X = X_sample[i:i+batch_size].to(device)
            
            # –ü–æ–ª—É—á–∞–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
            outputs = model(batch_X)
            
            if hasattr(outputs, '_direction_logits'):
                # –ò–∑–≤–ª–µ–∫–∞–µ–º logits –¥–ª—è direction_15m
                direction_logits = outputs._direction_logits[:, 0, :]  # [batch, 3]
                
                # –ü—Ä–∏–º–µ–Ω—è–µ–º softmax –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–µ–π
                probs = F.softmax(direction_logits, dim=-1)
                
                # –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å = —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å
                max_probs, predictions = torch.max(probs, dim=-1)
                
                # –≠–Ω—Ç—Ä–æ–ø–∏—è —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è
                entropy = -(probs * torch.log(probs + 1e-8)).sum(dim=-1)
                
                all_confidences.extend(max_probs.cpu().numpy())
                all_entropies.extend(entropy.cpu().numpy())
                
                # –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è –ø–æ —É—Ä–æ–≤–Ω—é —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏
                for j, (conf, pred) in enumerate(zip(max_probs, predictions)):
                    conf_val = conf.item()
                    pred_val = pred.item()
                    
                    if conf_val > 0.7:
                        conf_level = 'high'
                    elif conf_val > 0.5:
                        conf_level = 'medium'
                    else:
                        conf_level = 'low'
                    
                    pred_class = ['LONG', 'SHORT', 'FLAT'][pred_val]
                    predictions_by_confidence[conf_level][pred_class] += 1
    
    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏
    confidences = np.array(all_confidences)
    entropies = np.array(all_entropies)
    
    print("\nüìà –°–¢–ê–¢–ò–°–¢–ò–ö–ê –£–í–ï–†–ï–ù–ù–û–°–¢–ò:")
    print(f"–°—Ä–µ–¥–Ω—è—è —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: {confidences.mean():.3f}")
    print(f"–ú–µ–¥–∏–∞–Ω–Ω–∞—è —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: {np.median(confidences):.3f}")
    print(f"–ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: {confidences.min():.3f}")
    print(f"–ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: {confidences.max():.3f}")
    print(f"–°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–µ –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏–µ: {confidences.std():.3f}")
    
    print("\nüé≤ –≠–ù–¢–†–û–ü–ò–Ø –ü–†–ï–î–°–ö–ê–ó–ê–ù–ò–ô:")
    print(f"–°—Ä–µ–¥–Ω—è—è —ç–Ω—Ç—Ä–æ–ø–∏—è: {entropies.mean():.3f}")
    print(f"–ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è —ç–Ω—Ç—Ä–æ–ø–∏—è (–Ω–µ—É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å): {entropies.max():.3f}")
    print(f"–ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è —ç–Ω—Ç—Ä–æ–ø–∏—è (—É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å): {entropies.min():.3f}")
    
    print("\nüìä –†–ê–°–ü–†–ï–î–ï–õ–ï–ù–ò–ï –£–í–ï–†–ï–ù–ù–û–°–¢–ò:")
    high_conf = (confidences > 0.7).sum()
    medium_conf = ((confidences > 0.5) & (confidences <= 0.7)).sum()
    low_conf = (confidences <= 0.5).sum()
    
    print(f"–í—ã—Å–æ–∫–∞—è —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å (>70%): {high_conf/len(confidences)*100:.1f}%")
    print(f"–°—Ä–µ–¥–Ω—è—è —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å (50-70%): {medium_conf/len(confidences)*100:.1f}%")
    print(f"–ù–∏–∑–∫–∞—è —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å (<50%): {low_conf/len(confidences)*100:.1f}%")
    
    print("\nüéØ –ü–†–ï–î–°–ö–ê–ó–ê–ù–ò–Ø –ü–û –£–†–û–í–ù–Ø–ú –£–í–ï–†–ï–ù–ù–û–°–¢–ò:")
    for level in ['high', 'medium', 'low']:
        total = sum(predictions_by_confidence[level].values())
        if total > 0:
            print(f"\n{level.upper()} —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å:")
            for class_name, count in predictions_by_confidence[level].items():
                print(f"  {class_name}: {count/total*100:.1f}%")
    
    # –ì–∏—Å—Ç–æ–≥—Ä–∞–º–º–∞ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏
    print("\nüìä –ì–ò–°–¢–û–ì–†–ê–ú–ú–ê –£–í–ï–†–ï–ù–ù–û–°–¢–ò:")
    hist, bins = np.histogram(confidences, bins=10)
    for i in range(len(hist)):
        bar_width = int(hist[i] / len(confidences) * 50)
        print(f"{bins[i]:.2f}-{bins[i+1]:.2f}: {'‚ñà' * bar_width} {hist[i]}")
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –∫–∞–ª–∏–±—Ä–æ–≤–∫–∏
    print("\nüéØ –ö–ê–õ–ò–ë–†–û–í–ö–ê –ú–û–î–ï–õ–ò:")
    print("(–ù–∞—Å–∫–æ–ª—å–∫–æ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É–µ—Ç —Ä–µ–∞–ª—å–Ω–æ–π —Ç–æ—á–Ω–æ—Å—Ç–∏)")
    
    # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º –ø–æ —É—Ä–æ–≤–Ω—è–º —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏ –∏ –ø—Ä–æ–≤–µ—Ä—è–µ–º —Ç–æ—á–Ω–æ—Å—Ç—å
    confidence_bins = [(0.3, 0.5), (0.5, 0.6), (0.6, 0.7), (0.7, 0.8), (0.8, 0.9), (0.9, 1.0)]
    
    print("\n–£–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å ‚Üí –†–µ–∞–ª—å–Ω–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å:")
    for low, high in confidence_bins:
        mask = (confidences >= low) & (confidences < high)
        if mask.sum() > 0:
            print(f"{low:.1f}-{high:.1f}: –ø—Ä–∏–º–µ—Ä–æ–≤ {mask.sum()}")
            
else:
    print("‚ùå –ö—ç—à –≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –Ω–µ –Ω–∞–π–¥–µ–Ω")

print("\nüí° –í–´–í–û–î–´:")
if confidences.mean() > 0.8:
    print("‚ö†Ô∏è –ú–æ–¥–µ–ª—å –°–õ–ò–®–ö–û–ú –£–í–ï–†–ï–ù–ê - –≤–æ–∑–º–æ–∂–Ω–æ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏–µ")
elif confidences.mean() < 0.5:
    print("‚ö†Ô∏è –ú–æ–¥–µ–ª—å –ù–ï–£–í–ï–†–ï–ù–ê - —Ç—Ä–µ–±—É–µ—Ç—Å—è –±–æ–ª—å—à–µ –æ–±—É—á–µ–Ω–∏—è")
else:
    print("‚úÖ –£—Ä–æ–≤–µ–Ω—å —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏ –≤ –Ω–æ—Ä–º–µ")

if (confidences > 0.9).sum() / len(confidences) > 0.5:
    print("üî¥ –ë–æ–ª–µ–µ 50% –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π —Å —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å—é >90% - –ø—Ä–∏–∑–Ω–∞–∫ —Å—Ö–ª–æ–ø—ã–≤–∞–Ω–∏—è!")