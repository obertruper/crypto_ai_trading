#!/usr/bin/env python3
"""
–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π direction –º–æ–¥–µ–ª–∏
"""

import torch
import yaml
from pathlib import Path
import numpy as np

from models.patchtst_unified import UnifiedPatchTSTForTrading
from utils.logger import get_logger

def main():
    logger = get_logger("TestDirections")
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é
    config_path = "config/config_production.yaml" if Path("config/config_production.yaml").exists() else "config/config.yaml"
    with open(config_path, 'r', encoding='utf-8') as f:
        config = yaml.safe_load(f)
    
    logger.info(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–∞ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –∏–∑ {config_path}")
    
    # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –º–æ–¥–µ–ª–∏
    model_config = config['model']
    # –ò—Å–ø–æ–ª—å–∑—É–µ–º input_size –∏–∑ –∫–æ–Ω—Ñ–∏–≥–∞
    
    # –°–æ–∑–¥–∞–µ–º –º–æ–¥–µ–ª—å
    model = UnifiedPatchTSTForTrading(model_config).cuda()
    logger.info("‚úÖ –ú–æ–¥–µ–ª—å —Å–æ–∑–¥–∞–Ω–∞")
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞—á–∞–ª—å–Ω—ã–µ –≤–µ—Å–∞ direction_head
    for name, param in model.named_parameters():
        if 'direction_head' in name and 'bias' in name:
            logger.info(f"\nüìä {name}:")
            if param.shape[0] == 12:  # 4 —Ç–∞–π–º—Ñ—Ä–µ–π–º–∞ √ó 3 –∫–ª–∞—Å—Å–∞
                bias = param.view(4, 3)
                for i in range(4):
                    logger.info(f"   –¢–∞–π–º—Ñ—Ä–µ–π–º {i}: LONG={bias[i, 0]:.3f}, SHORT={bias[i, 1]:.3f}, FLAT={bias[i, 2]:.3f}")
    
    # –°–æ–∑–¥–∞–µ–º —Ç–µ—Å—Ç–æ–≤—ã–π –±–∞—Ç—á
    batch_size = 32
    seq_len = 168
    n_features = model_config['input_size']  # –ë–µ—Ä–µ–º –∏–∑ –∫–æ–Ω—Ñ–∏–≥–∞
    
    # –°–ª—É—á–∞–π–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
    x = torch.randn(batch_size, seq_len, n_features).cuda()
    
    # Forward pass
    model.eval()
    with torch.no_grad():
        outputs = model(x)
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º direction –ª–æ–≥–∏—Ç—ã
        if hasattr(outputs, '_direction_logits'):
            direction_logits = outputs._direction_logits  # (B, 4, 3)
            
            logger.info("\nüìä –ê–Ω–∞–ª–∏–∑ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π direction:")
            
            for i, tf in enumerate(['15m', '1h', '4h', '12h']):
                logits = direction_logits[:, i, :]  # (B, 3)
                probs = torch.softmax(logits, dim=-1)
                predictions = torch.argmax(probs, dim=-1)
                
                # –ü–æ–¥—Å—á–µ—Ç –∫–ª–∞—Å—Å–æ–≤
                unique, counts = torch.unique(predictions, return_counts=True)
                class_dist = {0: 0, 1: 0, 2: 0}
                for cls, cnt in zip(unique.cpu().numpy(), counts.cpu().numpy()):
                    class_dist[int(cls)] = cnt
                
                logger.info(f"\nüïê –¢–∞–π–º—Ñ—Ä–µ–π–º {tf}:")
                logger.info(f"   LONG (0):  {class_dist[0]:3d} ({class_dist[0]/batch_size*100:5.1f}%)")
                logger.info(f"   SHORT (1): {class_dist[1]:3d} ({class_dist[1]/batch_size*100:5.1f}%)")
                logger.info(f"   FLAT (2):  {class_dist[2]:3d} ({class_dist[2]/batch_size*100:5.1f}%)")
                
                # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ª–æ–≥–∏—Ç–æ–≤
                mean_logits = logits.mean(dim=0)
                logger.info(f"   –°—Ä–µ–¥–Ω–∏–µ –ª–æ–≥–∏—Ç—ã: LONG={mean_logits[0]:.3f}, SHORT={mean_logits[1]:.3f}, FLAT={mean_logits[2]:.3f}")
                
                # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–µ–π
                mean_probs = probs.mean(dim=0)
                logger.info(f"   –°—Ä–µ–¥–Ω–∏–µ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏: LONG={mean_probs[0]:.3f}, SHORT={mean_probs[1]:.3f}, FLAT={mean_probs[2]:.3f}")
    
    # –ü–æ–ø—Ä–æ–±—É–µ–º –∑–∞–≥—Ä—É–∑–∏—Ç—å –æ–±—É—á–µ–Ω–Ω—É—é –º–æ–¥–µ–ª—å –µ—Å–ª–∏ –µ—Å—Ç—å
    checkpoint_path = Path("models_saved/best_model.pth")
    if checkpoint_path.exists():
        logger.info(f"\nüì• –ó–∞–≥—Ä—É–∂–∞–µ–º checkpoint –∏–∑ {checkpoint_path}")
        checkpoint = torch.load(checkpoint_path, map_location='cuda')
        model.load_state_dict(checkpoint['model_state_dict'], strict=False)
        
        logger.info("\nüìä –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –û–ë–£–ß–ï–ù–ù–û–ô –º–æ–¥–µ–ª–∏:")
        
        model.eval()
        with torch.no_grad():
            outputs = model(x)
            
            if hasattr(outputs, '_direction_logits'):
                direction_logits = outputs._direction_logits
                
                for i, tf in enumerate(['15m', '1h', '4h', '12h']):
                    logits = direction_logits[:, i, :]
                    probs = torch.softmax(logits, dim=-1)
                    predictions = torch.argmax(probs, dim=-1)
                    
                    # –ü–æ–¥—Å—á–µ—Ç –∫–ª–∞—Å—Å–æ–≤
                    unique, counts = torch.unique(predictions, return_counts=True)
                    class_dist = {0: 0, 1: 0, 2: 0}
                    for cls, cnt in zip(unique.cpu().numpy(), counts.cpu().numpy()):
                        class_dist[int(cls)] = cnt
                    
                    logger.info(f"\nüïê –¢–∞–π–º—Ñ—Ä–µ–π–º {tf}:")
                    logger.info(f"   LONG (0):  {class_dist[0]:3d} ({class_dist[0]/batch_size*100:5.1f}%)")
                    logger.info(f"   SHORT (1): {class_dist[1]:3d} ({class_dist[1]/batch_size*100:5.1f}%)")
                    logger.info(f"   FLAT (2):  {class_dist[2]:3d} ({class_dist[2]/batch_size*100:5.1f}%)")
                    
                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –µ—Å—Ç—å –ª–∏ –≤–æ–æ–±—â–µ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏–µ
                    if len(unique) == 1:
                        logger.warning(f"   ‚ö†Ô∏è –ú–æ–¥–µ–ª—å –ø—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞–µ—Ç –¢–û–õ–¨–ö–û –∫–ª–∞—Å—Å {unique[0].item()}!")

if __name__ == "__main__":
    main()