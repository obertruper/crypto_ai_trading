#!/usr/bin/env python3
"""
–ö–æ–º–ø–ª–µ–∫—Å–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ –º–æ–¥–µ–ª–∏ –¥–ª—è –ø—Ä–æ–¥–∞–∫—à–µ–Ω–∞
–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –ø—Ä–æ–±–ª–µ–º—ã —Å –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è–º–∏ –∏ –≥–æ—Ç–æ–≤–∏—Ç –º–æ–¥–µ–ª—å –∫ —Ä–∞–∑–≤–µ—Ä—Ç—ã–≤–∞–Ω–∏—é
"""

import torch
import numpy as np
import pandas as pd
from pathlib import Path
import yaml
import json
from datetime import datetime
from collections import Counter
import matplotlib.pyplot as plt
import seaborn as sns
from typing import Dict, List, Tuple, Optional

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏
plt.style.use('dark_background')
sns.set_palette("husl")

class ModelProductionEvaluator:
    """–û—Ü–µ–Ω—â–∏–∫ –º–æ–¥–µ–ª–∏ –¥–ª—è –ø—Ä–æ–¥–∞–∫—à–µ–Ω–∞"""
    
    def __init__(self, config_path: str = "config/config.yaml"):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –æ—Ü–µ–Ω—â–∏–∫–∞"""
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é
        with open(config_path, 'r', encoding='utf-8') as f:
            self.config = yaml.safe_load(f)
        
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        print(f"üñ•Ô∏è –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {self.device}")
        
        # –ü—É—Ç–∏
        self.models_dir = Path("models_saved")
        self.logs_dir = Path("logs")
        self.cache_dir = Path("cache/precomputed")
        self.plots_dir = Path("evaluation_plots")
        self.plots_dir.mkdir(exist_ok=True)
        
        # –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –∞–Ω–∞–ª–∏–∑–∞
        self.evaluation_results = {}
        
    def find_best_model(self) -> Optional[Path]:
        """–ù–∞—Ö–æ–¥–∏—Ç –ª—É—á—à—É—é —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω—É—é –º–æ–¥–µ–ª—å"""
        print("\nüîç –ü–æ–∏—Å–∫ –ª—É—á—à–µ–π –º–æ–¥–µ–ª–∏...")
        
        if not self.models_dir.exists():
            print("‚ùå –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è models_saved –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç!")
            return None
        
        # –ò—â–µ–º —Ñ–∞–π–ª—ã –º–æ–¥–µ–ª–µ–π
        model_files = list(self.models_dir.glob("*.pth"))
        
        if not model_files:
            print("‚ùå –ù–µ –Ω–∞–π–¥–µ–Ω–æ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π!")
            return None
        
        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –≤—Ä–µ–º–µ–Ω–∏ –º–æ–¥–∏—Ñ–∏–∫–∞—Ü–∏–∏ (–ø–æ—Å–ª–µ–¥–Ω—è—è - —Å–∞–º–∞—è –Ω–æ–≤–∞—è)
        model_files.sort(key=lambda x: x.stat().st_mtime, reverse=True)
        
        print(f"‚úÖ –ù–∞–π–¥–µ–Ω–æ –º–æ–¥–µ–ª–µ–π: {len(model_files)}")
        for i, model_file in enumerate(model_files[:5]):  # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Ç–æ–ø-5
            size_mb = model_file.stat().st_size / (1024 * 1024)
            mod_time = datetime.fromtimestamp(model_file.stat().st_mtime)
            print(f"   {i+1}. {model_file.name} ({size_mb:.1f} MB) - {mod_time}")
        
        best_model = model_files[0]
        print(f"\nüì¶ –í—ã–±—Ä–∞–Ω–∞ –º–æ–¥–µ–ª—å: {best_model.name}")
        return best_model
    
    def load_model(self, model_path: Path) -> torch.nn.Module:
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –º–æ–¥–µ–ª—å"""
        print(f"\nüì• –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏: {model_path.name}")
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º checkpoint
        checkpoint = torch.load(model_path, map_location=self.device)
        
        # –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ checkpoint
        if isinstance(checkpoint, dict):
            print("üìä –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ checkpoint:")
            for key in ['epoch', 'best_val_loss', 'model_config']:
                if key in checkpoint:
                    print(f"   {key}: {checkpoint[key]}")
        
        # –°–æ–∑–¥–∞–µ–º –º–æ–¥–µ–ª—å —Å –ø—Ä–∞–≤–∏–ª—å–Ω–æ–π –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–µ–π
        from models.patchtst_unified import UnifiedPatchTSTForTrading
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é –≤ checkpoint
        if isinstance(checkpoint, dict):
            # –ï—Å–ª–∏ –µ—Å—Ç—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω–∞—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è, –∏—Å–ø–æ–ª—å–∑—É–µ–º –µ—ë
            if 'config' in checkpoint:
                saved_config = checkpoint['config']
                # –û–±–Ω–æ–≤–ª—è–µ–º —Ä–∞–∑–º–µ—Ä –≤—Ö–æ–¥–∞ –∏–∑ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω–æ–π –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
                if 'model' in saved_config and 'input_size' in saved_config['model']:
                    self.config['model']['input_size'] = saved_config['model']['input_size']
                    print(f"   üìä –ò—Å–ø–æ–ª—å–∑—É—é input_size –∏–∑ checkpoint: {saved_config['model']['input_size']}")
            
            # –ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω–æ, –æ–ø—Ä–µ–¥–µ–ª—è–µ–º —Ä–∞–∑–º–µ—Ä –∏–∑ –≤–µ—Å–æ–≤
            elif 'model_state_dict' in checkpoint:
                if 'revin.affine_weight' in checkpoint['model_state_dict']:
                    input_size = checkpoint['model_state_dict']['revin.affine_weight'].shape[0]
                    self.config['model']['input_size'] = input_size
                    print(f"   üìä –û–ø—Ä–µ–¥–µ–ª–µ–Ω input_size –∏–∑ –≤–µ—Å–æ–≤ –º–æ–¥–µ–ª–∏: {input_size}")
        
        model = UnifiedPatchTSTForTrading(self.config)
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –≤–µ—Å–∞
        if isinstance(checkpoint, dict) and 'model_state_dict' in checkpoint:
            model.load_state_dict(checkpoint['model_state_dict'])
        else:
            model.load_state_dict(checkpoint)
        
        model.to(self.device)
        model.eval()
        
        print("‚úÖ –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞ —É—Å–ø–µ—à–Ω–æ")
        return model
    
    def analyze_model_weights(self, model: torch.nn.Module):
        """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –≤–µ—Å–∞ –º–æ–¥–µ–ª–∏ –¥–ª—è –ø–æ–Ω–∏–º–∞–Ω–∏—è bias"""
        print("\nüî¨ –ê–Ω–∞–ª–∏–∑ –≤–µ—Å–æ–≤ –º–æ–¥–µ–ª–∏...")
        
        # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º direction head
        if hasattr(model, 'direction_head'):
            print("\nüìä Direction Head –∞–Ω–∞–ª–∏–∑:")
            
            # –ü–æ–ª—É—á–∞–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–π —Å–ª–æ–π
            for name, param in model.direction_head.named_parameters():
                if 'bias' in name and param.shape[0] >= 12:  # 4 —Ç–∞–π–º—Ñ—Ä–µ–π–º–∞ √ó 3 –∫–ª–∞—Å—Å–∞
                    biases = param.detach().cpu().numpy()
                    
                    # –†–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ —Ç–∞–π–º—Ñ—Ä–µ–π–º—ã
                    for tf_idx in range(4):
                        tf_biases = biases[tf_idx*3:(tf_idx+1)*3]
                        print(f"\n   –¢–∞–π–º—Ñ—Ä–µ–π–º {tf_idx+1}:")
                        print(f"      LONG bias:  {tf_biases[0]:.4f}")
                        print(f"      SHORT bias: {tf_biases[1]:.4f}")
                        print(f"      FLAT bias:  {tf_biases[2]:.4f}")
                        
                        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –ø—Ä–µ–¥–ø–æ—á—Ç–∏—Ç–µ–ª—å–Ω—ã–π –∫–ª–∞—Å—Å
                        preferred_class = ['LONG', 'SHORT', 'FLAT'][np.argmax(tf_biases)]
                        print(f"      ‚ö†Ô∏è –ü—Ä–µ–¥–ø–æ—á—Ç–∏—Ç–µ–ª—å–Ω—ã–π –∫–ª–∞—Å—Å: {preferred_class}")
        
        # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –≥—Ä–∞–¥–∏–µ–Ω—Ç—ã (–µ—Å–ª–∏ –µ—Å—Ç—å)
        total_grad_norm = 0
        num_params = 0
        
        for param in model.parameters():
            if param.grad is not None:
                total_grad_norm += param.grad.norm().item()
                num_params += 1
        
        if num_params > 0:
            avg_grad_norm = total_grad_norm / num_params
            print(f"\nüìà –°—Ä–µ–¥–Ω—è—è –Ω–æ—Ä–º–∞ –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤: {avg_grad_norm:.6f}")
            if avg_grad_norm < 0.0001:
                print("   ‚ö†Ô∏è –û—á–µ–Ω—å –º–∞–ª–µ–Ω—å–∫–∏–µ –≥—Ä–∞–¥–∏–µ–Ω—Ç—ã - –º–æ–¥–µ–ª—å –º–æ–∂–µ—Ç –Ω–µ –æ–±—É—á–∞—Ç—å—Å—è!")
    
    def load_validation_data(self) -> Optional[Tuple[torch.Tensor, torch.Tensor]]:
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ"""
        print("\nüìÇ –ó–∞–≥—Ä—É–∑–∫–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö...")
        
        # –ò—â–µ–º –ø—Ä–µ–¥–≤—ã—á–∏—Å–ª–µ–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ - —Å–Ω–∞—á–∞–ª–∞ h5, –ø–æ—Ç–æ–º pt
        val_files = list(self.cache_dir.glob("val_*.h5"))
        if not val_files:
            val_files = list(self.cache_dir.glob("*_val.pt"))
        
        if not val_files:
            print("‚ùå –í–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã!")
            return None
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –ø–µ—Ä–≤—ã–π —Ñ–∞–π–ª –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
        val_file = val_files[0]
        print(f"üìÑ –ó–∞–≥—Ä—É–∂–∞–µ–º: {val_file.name}")
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º h5 —Ñ–∞–π–ª
        import h5py
        
        with h5py.File(val_file, 'r') as f:
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—Ç—Ä—É–∫—Ç—É—Ä—É —Ñ–∞–π–ª–∞
            print(f"   –ö–ª—é—á–∏ –≤ —Ñ–∞–π–ª–µ: {list(f.keys())}")
            
            # –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ - –ø—Ä–æ–±—É–µ–º —Ä–∞–∑–Ω—ã–µ –∫–ª—é—á–∏
            if 'X' in f and 'y' in f:
                # –ë–µ—Ä–µ–º –ø–µ—Ä–≤—ã–µ 1000 —Å—ç–º–ø–ª–æ–≤ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
                num_samples = min(1000, f['X'].shape[0])
                
                inputs = torch.tensor(f['X'][:num_samples], dtype=torch.float32)
                targets = torch.tensor(f['y'][:num_samples], dtype=torch.float32)
                
                print(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ —Å—ç–º–ø–ª–æ–≤: {num_samples}")
                print(f"   –†–∞–∑–º–µ—Ä –≤—Ö–æ–¥–æ–≤: {inputs.shape}")
                print(f"   –†–∞–∑–º–µ—Ä —Ü–µ–ª–µ–π: {targets.shape}")
                
                return inputs, targets
            elif 'windows' in f and 'targets' in f:
                # –ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–µ –∫–ª—é—á–∏
                num_samples = min(1000, f['windows'].shape[0])
                
                inputs = torch.tensor(f['windows'][:num_samples], dtype=torch.float32)
                targets = torch.tensor(f['targets'][:num_samples], dtype=torch.float32)
                
                print(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ —Å—ç–º–ø–ª–æ–≤: {num_samples}")
                print(f"   –†–∞–∑–º–µ—Ä –≤—Ö–æ–¥–æ–≤: {inputs.shape}")
                print(f"   –†–∞–∑–º–µ—Ä —Ü–µ–ª–µ–π: {targets.shape}")
                
                return inputs, targets
            else:
                print("‚ùå –ù–µ–≤–µ—Ä–Ω–∞—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∞ h5 —Ñ–∞–π–ª–∞!")
                return None
    
    def analyze_class_distribution(self, targets: torch.Tensor):
        """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–ª–∞—Å—Å–æ–≤ –≤ –¥–∞–Ω–Ω—ã—Ö"""
        print("\nüìä –ê–Ω–∞–ª–∏–∑ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –∫–ª–∞—Å—Å–æ–≤...")
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å targets –∏ –∫–æ—Ä—Ä–µ–∫—Ç–∏—Ä—É–µ–º –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
        if len(targets.shape) == 3 and targets.shape[1] == 1:
            targets = targets.squeeze(1)  # –£–±–∏—Ä–∞–µ–º –∏–∑–º–µ—Ä–µ–Ω–∏–µ —Ä–∞–∑–º–µ—Ä–∞ 1
        
        # Direction –∫–ª–∞—Å—Å—ã –Ω–∞—Ö–æ–¥—è—Ç—Å—è –≤ –ø–æ–∑–∏—Ü–∏—è—Ö 4-7
        direction_targets = targets[:, 4:8].numpy()
        
        class_counts = {
            'LONG': 0,
            'SHORT': 0,
            'FLAT': 0
        }
        
        # –ü–æ–¥—Å—á–∏—Ç—ã–≤–∞–µ–º –∫–ª–∞—Å—Å—ã –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Ç–∞–π–º—Ñ—Ä–µ–π–º–∞
        for tf_idx in range(4):
            tf_classes = direction_targets[:, tf_idx]
            
            counts = Counter(tf_classes)
            total = len(tf_classes)
            
            print(f"\n   –¢–∞–π–º—Ñ—Ä–µ–π–º {tf_idx+1}:")
            for class_idx, class_name in enumerate(['LONG', 'SHORT', 'FLAT']):
                count = counts.get(float(class_idx), 0)
                percent = (count / total) * 100
                print(f"      {class_name}: {count} ({percent:.1f}%)")
                class_counts[class_name] += count
        
        # –û–±—â–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        total_counts = sum(class_counts.values())
        print("\n   üìà –û–±—â–µ–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ:")
        for class_name, count in class_counts.items():
            percent = (count / total_counts) * 100
            print(f"      {class_name}: {count} ({percent:.1f}%)")
        
        # –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è
        self._plot_class_distribution(class_counts)
        
        # –ê–Ω–∞–ª–∏–∑ –¥–∏—Å–±–∞–ª–∞–Ω—Å–∞
        max_class = max(class_counts.values())
        min_class = min(class_counts.values())
        imbalance_ratio = max_class / min_class if min_class > 0 else float('inf')
        
        print(f"\n   ‚ö†Ô∏è –ö–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç –¥–∏—Å–±–∞–ª–∞–Ω—Å–∞: {imbalance_ratio:.2f}")
        if imbalance_ratio > 2:
            print("   ‚ùå –°–∏–ª—å–Ω—ã–π –¥–∏—Å–±–∞–ª–∞–Ω—Å –∫–ª–∞—Å—Å–æ–≤!")
        
        self.evaluation_results['class_distribution'] = class_counts
        self.evaluation_results['imbalance_ratio'] = imbalance_ratio
    
    def test_model_predictions(self, model: torch.nn.Module, inputs: torch.Tensor, targets: torch.Tensor):
        """–¢–µ—Å—Ç–∏—Ä—É–µ—Ç –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –º–æ–¥–µ–ª–∏"""
        print("\nüß™ –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π –º–æ–¥–µ–ª–∏...")
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å targets –∏ –∫–æ—Ä—Ä–µ–∫—Ç–∏—Ä—É–µ–º –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
        if len(targets.shape) == 3 and targets.shape[1] == 1:
            targets = targets.squeeze(1)  # –£–±–∏—Ä–∞–µ–º –∏–∑–º–µ—Ä–µ–Ω–∏–µ —Ä–∞–∑–º–µ—Ä–∞ 1
        
        model.eval()
        with torch.no_grad():
            # –ü–µ—Ä–µ–Ω–æ—Å–∏–º –Ω–∞ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ
            inputs = inputs.to(self.device)
            targets = targets.to(self.device)
            
            # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
            outputs = model(inputs)
            
            # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º direction –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
            if hasattr(outputs, '_direction_logits'):
                direction_logits = outputs._direction_logits
                print(f"‚úÖ Direction logits shape: {direction_logits.shape}")
                
                # –ü—Ä–∏–º–µ–Ω—è–µ–º softmax –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–µ–π
                direction_probs = torch.softmax(direction_logits, dim=-1)
                
                # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Ç–∞–π–º—Ñ—Ä–µ–π–º–∞
                pred_distribution = {}
                
                for tf_idx in range(4):
                    tf_probs = direction_probs[:, tf_idx, :]
                    tf_preds = torch.argmax(tf_probs, dim=-1)
                    
                    # –ü–æ–¥—Å—á–µ—Ç –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π
                    pred_counts = Counter(tf_preds.cpu().numpy())
                    
                    print(f"\n   –¢–∞–π–º—Ñ—Ä–µ–π–º {tf_idx+1} –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è:")
                    for class_idx, class_name in enumerate(['LONG', 'SHORT', 'FLAT']):
                        count = pred_counts.get(class_idx, 0)
                        percent = (count / len(tf_preds)) * 100
                        print(f"      {class_name}: {count} ({percent:.1f}%)")
                    
                    # –°—Ä–µ–¥–Ω—è—è —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å
                    max_probs = torch.max(tf_probs, dim=-1)[0]
                    avg_confidence = max_probs.mean().item()
                    print(f"      –°—Ä–µ–¥–Ω—è—è —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: {avg_confidence:.3f}")
                    
                    # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º numpy —Ç–∏–ø—ã –≤ Python —Ç–∏–ø—ã –¥–ª—è JSON
                    pred_counts_dict = {}
                    for k, v in pred_counts.items():
                        pred_counts_dict[str(k)] = int(v)
                    pred_distribution[f'tf_{tf_idx}'] = pred_counts_dict
                
                self.evaluation_results['prediction_distribution'] = pred_distribution
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π
                self._check_prediction_diversity(direction_probs)
    
    def _check_prediction_diversity(self, direction_probs: torch.Tensor):
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π"""
        print("\nüé≤ –ê–Ω–∞–ª–∏–∑ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π...")
        
        # –≠–Ω—Ç—Ä–æ–ø–∏—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π
        eps = 1e-8
        entropy = -torch.sum(direction_probs * torch.log(direction_probs + eps), dim=-1)
        avg_entropy = entropy.mean().item()
        
        max_entropy = np.log(3)  # –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è —ç–Ω—Ç—Ä–æ–ø–∏—è –¥–ª—è 3 –∫–ª–∞—Å—Å–æ–≤
        normalized_entropy = avg_entropy / max_entropy
        
        print(f"   –°—Ä–µ–¥–Ω—è—è —ç–Ω—Ç—Ä–æ–ø–∏—è: {avg_entropy:.3f}")
        print(f"   –ù–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω–∞—è —ç–Ω—Ç—Ä–æ–ø–∏—è: {normalized_entropy:.3f}")
        
        if normalized_entropy < 0.3:
            print("   ‚ö†Ô∏è –ù–∏–∑–∫–æ–µ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏–µ - –º–æ–¥–µ–ª—å —Å–ª–∏—à–∫–æ–º —É–≤–µ—Ä–µ–Ω–∞ –≤ –æ–¥–Ω–æ–º –∫–ª–∞—Å—Å–µ!")
        elif normalized_entropy > 0.9:
            print("   ‚ö†Ô∏è –°–ª–∏—à–∫–æ–º –≤—ã—Å–æ–∫–∞—è –Ω–µ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–æ—Å—Ç—å - –º–æ–¥–µ–ª—å –Ω–µ –º–æ–∂–µ—Ç –≤—ã–±—Ä–∞—Ç—å!")
        else:
            print("   ‚úÖ –•–æ—Ä–æ—à–µ–µ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π")
        
        self.evaluation_results['prediction_entropy'] = normalized_entropy
    
    def _plot_class_distribution(self, class_counts: Dict[str, int]):
        """–í–∏–∑—É–∞–ª–∏–∑–∏—Ä—É–µ—Ç —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–ª–∞—Å—Å–æ–≤"""
        plt.figure(figsize=(10, 6))
        
        classes = list(class_counts.keys())
        counts = list(class_counts.values())
        
        bars = plt.bar(classes, counts)
        
        # –†–∞—Å–∫—Ä–∞—à–∏–≤–∞–µ–º –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç –∫–ª–∞—Å—Å–∞
        colors = ['green', 'red', 'gray']
        for bar, color in zip(bars, colors):
            bar.set_color(color)
        
        plt.title('–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–ª–∞—Å—Å–æ–≤ –≤ –¥–∞–Ω–Ω—ã—Ö', fontsize=16)
        plt.xlabel('–ö–ª–∞—Å—Å –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏—è', fontsize=12)
        plt.ylabel('–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ', fontsize=12)
        
        # –î–æ–±–∞–≤–ª—è–µ–º –ø—Ä–æ—Ü–µ–Ω—Ç—ã
        total = sum(counts)
        for i, (bar, count) in enumerate(zip(bars, counts)):
            percent = (count / total) * 100
            plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 100,
                    f'{percent:.1f}%', ha='center', va='bottom')
        
        plt.tight_layout()
        plt.savefig(self.plots_dir / 'class_distribution.png', dpi=150)
        plt.close()
    
    def generate_production_recommendations(self):
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –¥–ª—è –ø—Ä–æ–¥–∞–∫—à–µ–Ω–∞"""
        print("\nüìã –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π –¥–ª—è –ø—Ä–æ–¥–∞–∫—à–µ–Ω–∞...")
        
        recommendations = []
        
        # –ê–Ω–∞–ª–∏–∑ –¥–∏—Å–±–∞–ª–∞–Ω—Å–∞ –∫–ª–∞—Å—Å–æ–≤
        if self.evaluation_results.get('imbalance_ratio', 0) > 2:
            recommendations.append({
                'priority': 'HIGH',
                'issue': '–°–∏–ª—å–Ω—ã–π –¥–∏—Å–±–∞–ª–∞–Ω—Å –∫–ª–∞—Å—Å–æ–≤',
                'solution': '–ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å weighted loss –∏–ª–∏ SMOTE –¥–ª—è –±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∏',
                'file': 'models/patchtst_unified.py',
                'code_change': '–î–æ–±–∞–≤–∏—Ç—å class_weights –≤ CrossEntropyLoss'
            })
        
        # –ê–Ω–∞–ª–∏–∑ —ç–Ω—Ç—Ä–æ–ø–∏–∏ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π
        entropy = self.evaluation_results.get('prediction_entropy', 0)
        if entropy < 0.3:
            recommendations.append({
                'priority': 'HIGH',
                'issue': '–ú–æ–¥–µ–ª—å –ø—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞–µ—Ç —Ç–æ–ª—å–∫–æ –æ–¥–∏–Ω –∫–ª–∞—Å—Å',
                'solution': '–ò–∑–º–µ–Ω–∏—Ç—å –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—é bias –∏ —É–≤–µ–ª–∏—á–∏—Ç—å learning rate',
                'file': 'config/config.yaml',
                'code_change': 'learning_rate: 0.0001 -> 0.001'
            })
        
        # Learning rate
        learning_rate = self.config.get('model', {}).get('learning_rate', 0.0001)
        if learning_rate < 0.0001:
            recommendations.append({
                'priority': 'MEDIUM',
                'issue': '–°–ª–∏—à–∫–æ–º –Ω–∏–∑–∫–∏–π learning rate',
                'solution': '–£–≤–µ–ª–∏—á–∏—Ç—å learning rate –∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å warmup',
                'file': 'config/config.yaml',
                'code_change': '–î–æ–±–∞–≤–∏—Ç—å warmup_steps: 1000'
            })
        
        # Direction loss weight
        direction_weight = self.config['loss']['task_weights'].get('directions', 1.0)
        if direction_weight < 5.0:
            recommendations.append({
                'priority': 'MEDIUM',
                'issue': '–ù–∏–∑–∫–∏–π –≤–µ—Å –¥–ª—è direction loss',
                'solution': '–£–≤–µ–ª–∏—á–∏—Ç—å –≤–µ—Å direction –≤ loss —Ñ—É–Ω–∫—Ü–∏–∏',
                'file': 'config/config.yaml',
                'code_change': 'directions: 3.0 -> 10.0'
            })
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏
        self._save_recommendations(recommendations)
        
        return recommendations
    
    def _save_recommendations(self, recommendations: List[Dict]):
        """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –≤ —Ñ–∞–π–ª"""
        report_path = self.plots_dir / 'production_recommendations.md'
        
        with open(report_path, 'w', encoding='utf-8') as f:
            f.write("# üöÄ –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –¥–ª—è –ø—Ä–æ–¥–∞–∫—à–µ–Ω–∞\n\n")
            f.write(f"–î–∞—Ç–∞: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")
            
            # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º –ø–æ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç—É
            high_priority = [r for r in recommendations if r['priority'] == 'HIGH']
            medium_priority = [r for r in recommendations if r['priority'] == 'MEDIUM']
            
            if high_priority:
                f.write("## üî¥ –í—ã—Å–æ–∫–∏–π –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç\n\n")
                for rec in high_priority:
                    f.write(f"### {rec['issue']}\n")
                    f.write(f"**–†–µ—à–µ–Ω–∏–µ:** {rec['solution']}\n")
                    f.write(f"**–§–∞–π–ª:** `{rec['file']}`\n")
                    f.write(f"**–ò–∑–º–µ–Ω–µ–Ω–∏–µ:** `{rec['code_change']}`\n\n")
            
            if medium_priority:
                f.write("## üü° –°—Ä–µ–¥–Ω–∏–π –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç\n\n")
                for rec in medium_priority:
                    f.write(f"### {rec['issue']}\n")
                    f.write(f"**–†–µ—à–µ–Ω–∏–µ:** {rec['solution']}\n")
                    f.write(f"**–§–∞–π–ª:** `{rec['file']}`\n")
                    f.write(f"**–ò–∑–º–µ–Ω–µ–Ω–∏–µ:** `{rec['code_change']}`\n\n")
            
            # –î–æ–±–∞–≤–ª—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –∞–Ω–∞–ª–∏–∑–∞
            f.write("## üìä –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –∞–Ω–∞–ª–∏–∑–∞\n\n")
            f.write("```json\n")
            f.write(json.dumps(self.evaluation_results, indent=2))
            f.write("\n```\n")
        
        print(f"‚úÖ –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã: {report_path}")
    
    def run_full_evaluation(self):
        """–ó–∞–ø—É—Å–∫–∞–µ—Ç –ø–æ–ª–Ω—É—é –æ—Ü–µ–Ω–∫—É –º–æ–¥–µ–ª–∏"""
        print("="*80)
        print("üöÄ –ü–û–õ–ù–ê–Ø –û–¶–ï–ù–ö–ê –ú–û–î–ï–õ–ò –î–õ–Ø –ü–†–û–î–ê–ö–®–ï–ù–ê")
        print("="*80)
        
        # 1. –ù–∞—Ö–æ–¥–∏–º –º–æ–¥–µ–ª—å
        model_path = self.find_best_model()
        if not model_path:
            return
        
        # 2. –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å
        model = self.load_model(model_path)
        
        # 3. –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –≤–µ—Å–∞
        self.analyze_model_weights(model)
        
        # 4. –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ
        data = self.load_validation_data()
        if data is None:
            print("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –¥–∞–Ω–Ω—ã–µ –¥–ª—è –æ—Ü–µ–Ω–∫–∏")
            return
        
        inputs, targets = data
        
        # 5. –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–ª–∞—Å—Å–æ–≤
        self.analyze_class_distribution(targets)
        
        # 6. –¢–µ—Å—Ç–∏—Ä—É–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
        self.test_model_predictions(model, inputs, targets)
        
        # 7. –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏
        recommendations = self.generate_production_recommendations()
        
        print("\n" + "="*80)
        print("üìä –ò–¢–û–ì–û–í–ê–Ø –û–¶–ï–ù–ö–ê")
        print("="*80)
        
        print(f"\nüéØ –û—Å–Ω–æ–≤–Ω—ã–µ –ø—Ä–æ–±–ª–µ–º—ã:")
        for rec in recommendations[:3]:  # –¢–æ–ø-3 –ø—Ä–æ–±–ª–µ–º—ã
            print(f"   ‚Ä¢ {rec['issue']}")
        
        print(f"\n‚úÖ –°–ª–µ–¥—É—é—â–∏–µ —à–∞–≥–∏:")
        print("   1. –ò—Å–ø—Ä–∞–≤–∏—Ç—å –¥–∏—Å–±–∞–ª–∞–Ω—Å –∫–ª–∞—Å—Å–æ–≤ —á–µ—Ä–µ–∑ weighted loss")
        print("   2. –£–≤–µ–ª–∏—á–∏—Ç—å learning rate –¥–æ 0.001")
        print("   3. –ò–∑–º–µ–Ω–∏—Ç—å –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—é direction head")
        print("   4. –£–≤–µ–ª–∏—á–∏—Ç—å –≤–µ—Å direction loss –¥–æ 10.0")
        print("   5. –î–æ–±–∞–≤–∏—Ç—å –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π")
        
        print(f"\nüìÅ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤: {self.plots_dir}")
        print("="*80)


def main():
    """–ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è"""
    evaluator = ModelProductionEvaluator()
    evaluator.run_full_evaluation()


if __name__ == "__main__":
    main()