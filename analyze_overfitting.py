#!/usr/bin/env python3
"""
–ê–Ω–∞–ª–∏–∑ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏ –∏ –∫–∞—á–µ—Å—Ç–≤–∞ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π
"""

import torch
import pandas as pd
import numpy as np
from pathlib import Path
import matplotlib.pyplot as plt
import seaborn as sns
from datetime import datetime
import logging
from typing import Dict, List, Tuple
import yaml

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(level=logging.INFO, format='%(asctime)s | %(name)s | %(levelname)s | %(message)s')
logger = logging.getLogger('OverfittingAnalysis')

class ModelAnalyzer:
    """–ê–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä –º–æ–¥–µ–ª–∏ –¥–ª—è –≤—ã—è–≤–ª–µ–Ω–∏—è –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è"""
    
    def __init__(self, model_path: str, config_path: str = "config/config.yaml"):
        self.model_path = Path(model_path)
        self.config = self._load_config(config_path)
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        self.model = None
        self.results = {}
        
    def _load_config(self, config_path: str) -> dict:
        """–ó–∞–≥—Ä—É–∑–∫–∞ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏"""
        with open(config_path, 'r') as f:
            return yaml.safe_load(f)
            
    def load_model(self):
        """–ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏"""
        logger.info(f"–ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ –∏–∑ {self.model_path}")
        
        # –ò–º–ø–æ—Ä—Ç –º–æ–¥–µ–ª–∏
        from models.patchtst_unified import UnifiedPatchTST
        
        # –°–æ–∑–¥–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏
        self.model = UnifiedPatchTST(self.config['model'])
        
        # –ó–∞–≥—Ä—É–∑–∫–∞ –≤–µ—Å–æ–≤
        checkpoint = torch.load(self.model_path, map_location=self.device)
        if isinstance(checkpoint, dict) and 'model_state_dict' in checkpoint:
            self.model.load_state_dict(checkpoint['model_state_dict'])
        else:
            self.model.load_state_dict(checkpoint)
            
        self.model.to(self.device)
        self.model.eval()
        logger.info("‚úÖ –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞ —É—Å–ø–µ—à–Ω–æ")
        
    def analyze_predictions(self, data_loader, dataset_name: str = "test"):
        """–ê–Ω–∞–ª–∏–∑ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π –º–æ–¥–µ–ª–∏"""
        logger.info(f"üìä –ê–Ω–∞–ª–∏–∑ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π –Ω–∞ {dataset_name} –¥–∞—Ç–∞—Å–µ—Ç–µ...")
        
        all_predictions = []
        all_targets = []
        
        with torch.no_grad():
            for batch_idx, (X, y, _) in enumerate(data_loader):
                X = X.to(self.device)
                y = y.to(self.device)
                
                # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ
                outputs = self.model(X)
                
                # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
                all_predictions.append(outputs.cpu().numpy())
                all_targets.append(y.cpu().numpy())
                
                if batch_idx % 10 == 0:
                    logger.info(f"  –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ –±–∞—Ç—á–µ–π: {batch_idx + 1}/{len(data_loader)}")
        
        # –û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        predictions = np.concatenate(all_predictions, axis=0)
        targets = np.concatenate(all_targets, axis=0)
        
        # –ê–Ω–∞–ª–∏–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        self.results[dataset_name] = {
            'predictions': predictions,
            'targets': targets,
            'metrics': self._calculate_metrics(predictions, targets)
        }
        
        return predictions, targets
        
    def _calculate_metrics(self, predictions: np.ndarray, targets: np.ndarray) -> Dict:
        """–†–∞—Å—á–µ—Ç –º–µ—Ç—Ä–∏–∫"""
        metrics = {}
        
        # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ç–∏–ø–æ–≤ —Ç–∞—Ä–≥–µ—Ç–æ–≤ –∏–∑ –∫–æ–Ω—Ñ–∏–≥–∞
        regression_targets = ['future_return_15m', 'future_return_1h', 
                            'future_return_4h', 'future_return_12h']
        categorical_targets = ['direction_15m', 'direction_1h', 
                             'direction_4h', 'direction_12h']
        binary_targets = ['long_will_reach_1pct_4h', 'long_will_reach_2pct_4h',
                         'short_will_reach_1pct_4h', 'short_will_reach_2pct_4h']
        
        # –ò–Ω–¥–µ–∫—Å—ã —Ç–∞—Ä–≥–µ—Ç–æ–≤ (–ø–µ—Ä–≤—ã–µ 20 –∏–∑ config v4.0)
        reg_idx = slice(0, 4)  # future_returns
        cat_idx = slice(4, 8)  # directions
        bin_idx = slice(8, 20)  # binary targets
        
        # –ú–µ—Ç—Ä–∏–∫–∏ –¥–ª—è —Ä–µ–≥—Ä–µ—Å—Å–∏–æ–Ω–Ω—ã—Ö —Ç–∞—Ä–≥–µ—Ç–æ–≤
        reg_pred = predictions[:, reg_idx]
        reg_true = targets[:, reg_idx]
        
        metrics['regression'] = {
            'mae': np.mean(np.abs(reg_pred - reg_true)),
            'rmse': np.sqrt(np.mean((reg_pred - reg_true) ** 2)),
            'correlation': np.mean([np.corrcoef(reg_pred[:, i], reg_true[:, i])[0, 1] 
                                   for i in range(reg_pred.shape[1])])
        }
        
        # –ú–µ—Ç—Ä–∏–∫–∏ –¥–ª—è –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã—Ö —Ç–∞—Ä–≥–µ—Ç–æ–≤ (directions)
        cat_pred = np.argmax(predictions[:, cat_idx], axis=1)
        cat_true = np.argmax(targets[:, cat_idx], axis=1) if len(targets.shape) > 1 else targets
        
        # –ú–µ—Ç—Ä–∏–∫–∏ –¥–ª—è –±–∏–Ω–∞—Ä–Ω—ã—Ö —Ç–∞—Ä–≥–µ—Ç–æ–≤
        bin_pred = (predictions[:, bin_idx] > 0.5).astype(int)
        bin_true = targets[:, bin_idx]
        
        metrics['binary'] = {
            'accuracy': np.mean(bin_pred == bin_true),
            'precision': np.sum((bin_pred == 1) & (bin_true == 1)) / (np.sum(bin_pred == 1) + 1e-8),
            'recall': np.sum((bin_pred == 1) & (bin_true == 1)) / (np.sum(bin_true == 1) + 1e-8)
        }
        
        # –û–±—â–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        metrics['overall'] = {
            'mean_prediction': np.mean(predictions),
            'std_prediction': np.std(predictions),
            'mean_target': np.mean(targets),
            'std_target': np.std(targets)
        }
        
        return metrics
        
    def visualize_results(self, save_dir: str = "analysis_results"):
        """–í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤"""
        save_path = Path(save_dir)
        save_path.mkdir(parents=True, exist_ok=True)
        
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        for dataset_name, results in self.results.items():
            predictions = results['predictions']
            targets = results['targets']
            
            # 1. –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π vs —Ä–µ–∞–ª—å–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π
            fig, axes = plt.subplots(2, 2, figsize=(15, 12))
            
            # Future returns (–ø–µ—Ä–≤—ã–µ 4 —Ç–∞—Ä–≥–µ—Ç–∞)
            for i in range(4):
                ax = axes[i // 2, i % 2]
                ax.scatter(targets[:, i], predictions[:, i], alpha=0.5, s=1)
                ax.plot([targets[:, i].min(), targets[:, i].max()], 
                       [targets[:, i].min(), targets[:, i].max()], 'r--', lw=2)
                ax.set_xlabel(f'True Return {["15m", "1h", "4h", "12h"][i]}')
                ax.set_ylabel(f'Predicted Return {["15m", "1h", "4h", "12h"][i]}')
                ax.set_title(f'Predictions vs Targets - {["15m", "1h", "4h", "12h"][i]}')
                
                # –î–æ–±–∞–≤–ª—è–µ–º –∫–æ—Ä—Ä–µ–ª—è—Ü–∏—é
                corr = np.corrcoef(targets[:, i], predictions[:, i])[0, 1]
                ax.text(0.05, 0.95, f'Corr: {corr:.3f}', transform=ax.transAxes,
                       verticalalignment='top', bbox=dict(boxstyle='round', facecolor='wheat'))
                
            plt.tight_layout()
            plt.savefig(save_path / f'{dataset_name}_scatter_{timestamp}.png', dpi=150)
            plt.close()
            
            # 2. –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π
            fig, axes = plt.subplots(2, 2, figsize=(15, 12))
            
            for i in range(4):
                ax = axes[i // 2, i % 2]
                ax.hist(predictions[:, i], bins=50, alpha=0.7, label='Predictions', density=True)
                ax.hist(targets[:, i], bins=50, alpha=0.7, label='Targets', density=True)
                ax.set_xlabel(f'Return {["15m", "1h", "4h", "12h"][i]}')
                ax.set_ylabel('Density')
                ax.set_title(f'Distribution - {["15m", "1h", "4h", "12h"][i]}')
                ax.legend()
                
            plt.tight_layout()
            plt.savefig(save_path / f'{dataset_name}_distributions_{timestamp}.png', dpi=150)
            plt.close()
            
            # 3. –ú–µ—Ç—Ä–∏–∫–∏ –≤ —Ç–µ–∫—Å—Ç–æ–≤–æ–º –≤–∏–¥–µ
            with open(save_path / f'{dataset_name}_metrics_{timestamp}.txt', 'w') as f:
                f.write(f"=== –ê–Ω–∞–ª–∏–∑ {dataset_name} –¥–∞—Ç–∞—Å–µ—Ç–∞ ===\n\n")
                
                metrics = results['metrics']
                
                f.write("üìä –†–µ–≥—Ä–µ—Å—Å–∏–æ–Ω–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏:\n")
                f.write(f"  MAE: {metrics['regression']['mae']:.6f}\n")
                f.write(f"  RMSE: {metrics['regression']['rmse']:.6f}\n")
                f.write(f"  Correlation: {metrics['regression']['correlation']:.3f}\n\n")
                
                f.write("üìä –ë–∏–Ω–∞—Ä–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏:\n")
                f.write(f"  Accuracy: {metrics['binary']['accuracy']:.3f}\n")
                f.write(f"  Precision: {metrics['binary']['precision']:.3f}\n")
                f.write(f"  Recall: {metrics['binary']['recall']:.3f}\n\n")
                
                f.write("üìä –û–±—â–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞:\n")
                f.write(f"  Mean prediction: {metrics['overall']['mean_prediction']:.6f}\n")
                f.write(f"  Std prediction: {metrics['overall']['std_prediction']:.6f}\n")
                f.write(f"  Mean target: {metrics['overall']['mean_target']:.6f}\n")
                f.write(f"  Std target: {metrics['overall']['std_target']:.6f}\n")
                
        logger.info(f"‚úÖ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ {save_path}")
        
    def check_overfitting_signs(self):
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è"""
        logger.info("üîç –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è...")
        
        signs = []
        
        if 'train' in self.results and 'val' in self.results:
            train_metrics = self.results['train']['metrics']
            val_metrics = self.results['val']['metrics']
            
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ MAE
            mae_ratio = val_metrics['regression']['mae'] / (train_metrics['regression']['mae'] + 1e-8)
            if mae_ratio > 2.0:
                signs.append(f"‚ö†Ô∏è MAE ratio (val/train): {mae_ratio:.2f} > 2.0")
                
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–∏
            corr_diff = train_metrics['regression']['correlation'] - val_metrics['regression']['correlation']
            if corr_diff > 0.2:
                signs.append(f"‚ö†Ô∏è Correlation drop: {corr_diff:.3f} > 0.2")
                
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–∏—Å–ø–µ—Ä—Å–∏–∏ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π
            std_ratio = val_metrics['overall']['std_prediction'] / (train_metrics['overall']['std_prediction'] + 1e-8)
            if std_ratio < 0.5 or std_ratio > 2.0:
                signs.append(f"‚ö†Ô∏è Std ratio (val/train): {std_ratio:.2f}")
                
        if signs:
            logger.warning("üö® –û–±–Ω–∞—Ä—É–∂–µ–Ω—ã –ø—Ä–∏–∑–Ω–∞–∫–∏ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è:")
            for sign in signs:
                logger.warning(f"  {sign}")
        else:
            logger.info("‚úÖ –Ø–≤–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è –Ω–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–æ")
            
        return signs


def main():
    """–ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è"""
    import argparse
    
    parser = argparse.ArgumentParser(description='–ê–Ω–∞–ª–∏–∑ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏')
    parser.add_argument('--model', type=str, default='models_saved/best_model.pth',
                       help='–ü—É—Ç—å –∫ –º–æ–¥–µ–ª–∏')
    parser.add_argument('--data-dir', type=str, default='data/processed',
                       help='–î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è —Å –¥–∞–Ω–Ω—ã–º–∏')
    parser.add_argument('--batch-size', type=int, default=1024,
                       help='–†–∞–∑–º–µ—Ä –±–∞—Ç—á–∞ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞')
    
    args = parser.parse_args()
    
    # –°–æ–∑–¥–∞–Ω–∏–µ –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–∞
    analyzer = ModelAnalyzer(args.model)
    analyzer.load_model()
    
    # –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö
    from data.precomputed_dataset import create_precomputed_data_loaders
    
    # –ó–¥–µ—Å—å –Ω—É–∂–Ω–æ –∑–∞–≥—Ä—É–∑–∏—Ç—å –¥–∞–Ω–Ω—ã–µ –∏–∑ parquet —Ñ–∞–π–ª–æ–≤
    # –î–ª—è –ø—Ä–∏–º–µ—Ä–∞ –∏—Å–ø–æ–ª—å–∑—É–µ–º –∑–∞–≥–ª—É—à–∫—É
    logger.info("üì• –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö...")
    
    # TODO: –†–µ–∞–ª–∏–∑–æ–≤–∞—Ç—å –∑–∞–≥—Ä—É–∑–∫—É –¥–∞–Ω–Ω—ã—Ö –∏ —Å–æ–∑–¥–∞–Ω–∏–µ data_loader'–æ–≤
    # train_loader, val_loader, test_loader = create_data_loaders(args.data_dir)
    
    # –ê–Ω–∞–ª–∏–∑ –Ω–∞ —Ä–∞–∑–Ω—ã—Ö –¥–∞—Ç–∞—Å–µ—Ç–∞—Ö
    # analyzer.analyze_predictions(test_loader, "test")
    # analyzer.analyze_predictions(val_loader, "val")
    # analyzer.analyze_predictions(train_loader, "train")
    
    # –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    # analyzer.visualize_results()
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è
    # analyzer.check_overfitting_signs()
    
    logger.info("‚úÖ –ê–Ω–∞–ª–∏–∑ –∑–∞–≤–µ—Ä—à–µ–Ω!")


if __name__ == "__main__":
    main()