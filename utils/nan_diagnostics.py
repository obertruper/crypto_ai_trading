"""
–£—Ç–∏–ª–∏—Ç—ã –¥–ª—è –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∏ –∏ –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–µ–Ω–∏—è NaN –≤ –ø—Ä–æ—Ü–µ—Å—Å–µ –æ–±—É—á–µ–Ω–∏—è
"""

import torch
import numpy as np
import pandas as pd
from typing import Dict, List, Tuple, Optional, Union
import logging

class NaNDiagnostics:
    """–ö–ª–∞—Å—Å –¥–ª—è –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏—è –∏ –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∏ NaN –∑–Ω–∞—á–µ–Ω–∏–π"""
    
    def __init__(self, logger: Optional[logging.Logger] = None):
        self.logger = logger or logging.getLogger(__name__)
        self.nan_history = {
            'inputs': [],
            'outputs': [],
            'gradients': [],
            'losses': []
        }
        self.batch_counter = 0
        
    def check_tensor(self, tensor: torch.Tensor, name: str = "tensor") -> bool:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ —Ç–µ–Ω–∑–æ—Ä–∞ –Ω–∞ NaN/Inf —Å –ø–æ–¥—Ä–æ–±–Ω–æ–π –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–æ–π"""
        has_nan = torch.isnan(tensor).any().item()
        has_inf = torch.isinf(tensor).any().item()
        
        if has_nan or has_inf:
            self.logger.warning(f"‚ö†Ô∏è –û–±–Ω–∞—Ä—É–∂–µ–Ω—ã –ø—Ä–æ–±–ª–µ–º—ã –≤ {name}:")
            if has_nan:
                nan_count = torch.isnan(tensor).sum().item()
                nan_pct = nan_count / tensor.numel() * 100
                self.logger.warning(f"  - NaN: {nan_count} –∑–Ω–∞—á–µ–Ω–∏–π ({nan_pct:.2f}%)")
            
            if has_inf:
                inf_count = torch.isinf(tensor).sum().item()
                inf_pct = inf_count / tensor.numel() * 100
                self.logger.warning(f"  - Inf: {inf_count} –∑–Ω–∞—á–µ–Ω–∏–π ({inf_pct:.2f}%)")
            
            # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –≤–∞–ª–∏–¥–Ω—ã–º –∑–Ω–∞—á–µ–Ω–∏—è–º
            valid_mask = ~(torch.isnan(tensor) | torch.isinf(tensor))
            if valid_mask.any():
                valid_values = tensor[valid_mask]
                self.logger.warning(f"  - –í–∞–ª–∏–¥–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è: min={valid_values.min().item():.4f}, "
                                  f"max={valid_values.max().item():.4f}, "
                                  f"mean={valid_values.mean().item():.4f}, "
                                  f"std={valid_values.std().item():.4f}")
            
            # –î–µ—Ç–∞–ª—å–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ —Ñ–æ—Ä–º–µ –∏ —Ç–∏–ø–µ
            self.logger.warning(f"  - Shape: {tensor.shape}, dtype: {tensor.dtype}")
            
            return True
        
        return False
    
    def check_model_parameters(self, model: torch.nn.Module) -> Dict[str, bool]:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –≤—Å–µ—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –º–æ–¥–µ–ª–∏ –Ω–∞ NaN/Inf"""
        problematic_params = {}
        
        for name, param in model.named_parameters():
            if param is not None:
                has_issues = self.check_tensor(param.data, f"parameter {name}")
                if has_issues:
                    problematic_params[name] = True
                    
                # –ü—Ä–æ–≤–µ—Ä–∫–∞ –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤ –µ—Å–ª–∏ –æ–Ω–∏ –µ—Å—Ç—å
                if param.grad is not None:
                    grad_issues = self.check_tensor(param.grad, f"gradient {name}")
                    if grad_issues:
                        problematic_params[f"{name}_grad"] = True
        
        return problematic_params
    
    def sanitize_tensor(self, tensor: torch.Tensor, 
                       replace_nan: float = 0.0,
                       replace_inf: float = 1e6) -> torch.Tensor:
        """–ó–∞–º–µ–Ω–∞ NaN –∏ Inf –∑–Ω–∞—á–µ–Ω–∏–π –≤ —Ç–µ–Ω–∑–æ—Ä–µ"""
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–æ–µ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ
        device = tensor.device
        
        # –ó–∞–º–µ–Ω–∞ NaN
        if torch.isnan(tensor).any():
            tensor = torch.nan_to_num(tensor, nan=replace_nan)
        
        # –ó–∞–º–µ–Ω–∞ Inf
        if torch.isinf(tensor).any():
            tensor = torch.where(torch.isinf(tensor), 
                               torch.full_like(tensor, replace_inf).to(device), 
                               tensor)
        
        return tensor
    
    def check_dataframe(self, df: pd.DataFrame, name: str = "dataframe") -> Dict[str, int]:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ DataFrame –Ω–∞ NaN –∑–Ω–∞—á–µ–Ω–∏—è"""
        nan_counts = df.isna().sum()
        problematic_cols = nan_counts[nan_counts > 0]
        
        if len(problematic_cols) > 0:
            self.logger.warning(f"‚ö†Ô∏è NaN –∑–Ω–∞—á–µ–Ω–∏—è –≤ {name}:")
            for col, count in problematic_cols.items():
                pct = count / len(df) * 100
                self.logger.warning(f"  - {col}: {count} –∑–Ω–∞—á–µ–Ω–∏–π ({pct:.2f}%)")
        
        return problematic_cols.to_dict()
    
    def log_batch_stats(self, 
                       inputs: torch.Tensor,
                       outputs: torch.Tensor,
                       loss: torch.Tensor,
                       batch_idx: int):
        """–õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –±–∞—Ç—á–∞ –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏"""
        self.batch_counter += 1
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –ø—Ä–æ–±–ª–µ–º—ã
        input_issues = self.check_tensor(inputs, f"batch {batch_idx} inputs")
        output_issues = self.check_tensor(outputs, f"batch {batch_idx} outputs")
        loss_issues = self.check_tensor(loss, f"batch {batch_idx} loss")
        
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∏—Å—Ç–æ—Ä–∏–∏
        if input_issues:
            self.nan_history['inputs'].append(batch_idx)
        if output_issues:
            self.nan_history['outputs'].append(batch_idx)
        if loss_issues:
            self.nan_history['losses'].append(batch_idx)
        
        # –ü–µ—Ä–∏–æ–¥–∏—á–µ—Å–∫–∏–π –æ—Ç—á–µ—Ç
        if self.batch_counter % 100 == 0:
            self.print_summary()
    
    def print_summary(self):
        """–í—ã–≤–æ–¥ —Å–≤–æ–¥–∫–∏ –ø–æ NaN –ø—Ä–æ–±–ª–µ–º–∞–º"""
        total_batches = self.batch_counter
        
        self.logger.info("üìä –°–≤–æ–¥–∫–∞ –ø–æ NaN/Inf –ø—Ä–æ–±–ª–µ–º–∞–º:")
        for category, batch_list in self.nan_history.items():
            if batch_list:
                count = len(batch_list)
                pct = count / total_batches * 100
                self.logger.info(f"  - {category}: {count} –±–∞—Ç—á–µ–π ({pct:.2f}%)")
                if count <= 5:
                    self.logger.info(f"    –ë–∞—Ç—á–∏: {batch_list}")
                else:
                    self.logger.info(f"    –ü–µ—Ä–≤—ã–µ 5: {batch_list[:5]}")
    
    def create_nan_safe_optimizer(self, optimizer: torch.optim.Optimizer) -> torch.optim.Optimizer:
        """–û–±–µ—Ä—Ç–∫–∞ –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä–∞ –¥–ª—è –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–µ–Ω–∏—è NaN –≤ –≥—Ä–∞–¥–∏–µ–Ω—Ç–∞—Ö"""
        original_step = optimizer.step
        
        def safe_step(closure=None):
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤ –ø–µ—Ä–µ–¥ —à–∞–≥–æ–º –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä–∞
            for group in optimizer.param_groups:
                for param in group['params']:
                    if param.grad is not None:
                        # –ó–∞–º–µ–Ω–∞ NaN/Inf –≤ –≥—Ä–∞–¥–∏–µ–Ω—Ç–∞—Ö
                        if torch.isnan(param.grad).any() or torch.isinf(param.grad).any():
                            self.logger.warning("–û–±–Ω–∞—Ä—É–∂–µ–Ω—ã NaN/Inf –≤ –≥—Ä–∞–¥–∏–µ–Ω—Ç–∞—Ö, –∑–∞–º–µ–Ω—è–µ–º –Ω–∞ 0")
                            param.grad = self.sanitize_tensor(param.grad, replace_nan=0.0, replace_inf=0.0)
            
            # –í—ã–∑–æ–≤ –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–æ–≥–æ step
            return original_step(closure)
        
        optimizer.step = safe_step
        return optimizer


def add_gradient_hooks(model: torch.nn.Module, logger: Optional[logging.Logger] = None):
    """–î–æ–±–∞–≤–ª–µ–Ω–∏–µ —Ö—É–∫–æ–≤ –¥–ª—è –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏—è –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤"""
    logger = logger or logging.getLogger(__name__)
    
    def gradient_hook(module, grad_input, grad_output):
        """–•—É–∫ –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤"""
        module_name = module.__class__.__name__
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ grad_output
        for i, grad in enumerate(grad_output):
            if grad is not None:
                if torch.isnan(grad).any():
                    logger.warning(f"NaN –≤ grad_output[{i}] –º–æ–¥—É–ª—è {module_name}")
                if torch.isinf(grad).any():
                    logger.warning(f"Inf –≤ grad_output[{i}] –º–æ–¥—É–ª—è {module_name}")
                    
                # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤
                grad_norm = grad.norm().item()
                if grad_norm > 100:
                    logger.warning(f"–ë–æ–ª—å—à–∞—è –Ω–æ—Ä–º–∞ –≥—Ä–∞–¥–∏–µ–Ω—Ç–∞ –≤ {module_name}: {grad_norm:.4f}")
    
    # –†–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏—è —Ö—É–∫–æ–≤ –¥–ª—è –≤—Å–µ—Ö –º–æ–¥—É–ª–µ–π
    for name, module in model.named_modules():
        if len(list(module.children())) == 0:  # –¢–æ–ª—å–∫–æ –¥–ª—è –ª–∏—Å—Ç–æ–≤—ã—Ö –º–æ–¥—É–ª–µ–π
            module.register_backward_hook(gradient_hook)
    
    logger.info(f"–ó–∞—Ä–µ–≥–∏—Å—Ç—Ä–∏—Ä–æ–≤–∞–Ω—ã —Ö—É–∫–∏ –¥–ª—è –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏—è –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤")


def stabilize_model_initialization(model: torch.nn.Module, method: str = 'xavier'):
    """–°—Ç–∞–±–∏–ª—å–Ω–∞—è –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –≤–µ—Å–æ–≤ –º–æ–¥–µ–ª–∏"""
    
    def init_weights(module):
        if isinstance(module, (torch.nn.Linear, torch.nn.Conv1d, torch.nn.Conv2d)):
            if method == 'xavier':
                torch.nn.init.xavier_uniform_(module.weight, gain=0.5)  # –ú–µ–Ω—å—à–∏–π gain –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏
            elif method == 'kaiming':
                torch.nn.init.kaiming_uniform_(module.weight, mode='fan_out', nonlinearity='relu')
            else:
                torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)
            
            if module.bias is not None:
                torch.nn.init.constant_(module.bias, 0.0)
                
        elif isinstance(module, (torch.nn.BatchNorm1d, torch.nn.BatchNorm2d, torch.nn.LayerNorm)):
            if module.weight is not None:
                torch.nn.init.constant_(module.weight, 1.0)
            if module.bias is not None:
                torch.nn.init.constant_(module.bias, 0.0)
    
    model.apply(init_weights)
    
    # –û—Å–æ–±–∞—è –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –¥–ª—è –ø–æ–∑–∏—Ü–∏–æ–Ω–Ω—ã—Ö —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
    for name, param in model.named_parameters():
        if 'pos_embedding' in name or 'positional' in name:
            torch.nn.init.normal_(param, mean=0.0, std=0.02)
    
    return model