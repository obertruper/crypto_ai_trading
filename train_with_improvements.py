"""
–û–±—É—á–µ–Ω–∏–µ —Å —É–ª—É—á—à–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª—å—é –∏ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ–º —Ü–µ–ª–µ–≤–æ–π –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π
"""

import os
import yaml
import torch
import numpy as np
import pandas as pd
from pathlib import Path
from datetime import datetime

# –ò–º–ø–æ—Ä—Ç—ã –∏–∑ –ø—Ä–æ–µ–∫—Ç–∞
from models.patchtst_improved import ImprovedPatchTST
from data.target_scaler import TargetScaler, scale_targets_in_dataset
from data.data_loader import CryptoDataLoader
from data.dataset import create_data_loaders
from data.feature_engineering import FeatureEngineer
from training.trainer import Trainer
from utils.logger import get_logger
from utils.nan_diagnostics import NaNDiagnostics, stabilize_model_initialization, add_gradient_hooks
# from utils.config import load_config  # –≠—Ç–æ–≥–æ –º–æ–¥—É–ª—è –Ω–µ—Ç, –∏—Å–ø–æ–ª—å–∑—É–µ–º yaml –Ω–∞–ø—Ä—è–º—É—é


def prepare_data_with_scaling(config, logger):
    """–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö —Å –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ–º —Ü–µ–ª–µ–≤–æ–π –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π"""
    
    # 1. –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö
    logger.info("üì• –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –∏–∑ –ë–î...")
    data_loader = CryptoDataLoader(config)
    raw_data = data_loader.load_data(
        symbols=config['data']['symbols'],
        start_date=config['data']['start_date'],
        end_date=config['data']['end_date']
    )
    
    if raw_data.empty:
        raise ValueError("–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –¥–∞–Ω–Ω—ã–µ!")
    
    logger.info(f"–ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(raw_data):,} –∑–∞–ø–∏—Å–µ–π")
    
    # 2. –ü—Ä–æ–≤–µ—Ä–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞ –¥–∞–Ω–Ω—ã—Ö
    logger.info("üîç –ü—Ä–æ–≤–µ—Ä–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞ –¥–∞–Ω–Ω—ã—Ö...")
    quality_report = data_loader.validate_data_quality(raw_data)
    
    # 3. –°–æ–∑–¥–∞–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
    logger.info("üõ†Ô∏è –°–æ–∑–¥–∞–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤...")
    feature_engineer = FeatureEngineer(config)
    
    # –ò—Å–ø–æ–ª—å–∑—É–µ–º –º–µ—Ç–æ–¥ –±–µ–∑ data leakage
    train_data, val_data, test_data = feature_engineer.create_features_with_train_split(
        raw_data, 
        train_ratio=config['data']['train_ratio'],
        val_ratio=config['data']['val_ratio']
    )
    
    # –ü–æ–ª—É—á–∞–µ–º —Å–ø–∏—Å–æ–∫ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
    exclude_cols = ['id', 'symbol', 'datetime', 'timestamp', 'sector']
    feature_cols = [col for col in train_data.columns 
                    if col not in exclude_cols 
                    and not col.startswith(('target_', 'future_', 'optimal_'))]
    
    logger.info(f"–°–æ–∑–¥–∞–Ω–æ {len(feature_cols)} –ø—Ä–∏–∑–Ω–∞–∫–æ–≤")
    
    # 4. –ú–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ —Ü–µ–ª–µ–≤–æ–π –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π
    logger.info("üéØ –ú–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ —Ü–µ–ª–µ–≤–æ–π –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π...")
    target_col = config['model']['target_variable']
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ —Ü–µ–ª–µ–≤–æ–π –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π
    if target_col not in train_data.columns:
        logger.error(f"–¶–µ–ª–µ–≤–∞—è –ø–µ—Ä–µ–º–µ–Ω–Ω–∞—è {target_col} –Ω–µ –Ω–∞–π–¥–µ–Ω–∞!")
        logger.info(f"–î–æ—Å—Ç—É–ø–Ω—ã–µ —Ü–µ–ª–µ–≤—ã–µ: {[col for col in train_data.columns if 'future' in col or 'target' in col]}")
        raise ValueError(f"–¶–µ–ª–µ–≤–∞—è –ø–µ—Ä–µ–º–µ–Ω–Ω–∞—è {target_col} –Ω–µ –Ω–∞–π–¥–µ–Ω–∞ –≤ –¥–∞–Ω–Ω—ã—Ö!")
    
    # –ú–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º —Ü–µ–ª–µ–≤—É—é –ø–µ—Ä–µ–º–µ–Ω–Ω—É—é
    train_scaled, val_scaled, test_scaled, target_scaler = scale_targets_in_dataset(
        train_data, val_data, test_data, target_col,
        scaler_path='models_saved/target_scaler.pkl'
    )
    
    # –û–±–Ω–æ–≤–ª—è–µ–º –∫–æ–Ω—Ñ–∏–≥ –¥–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–Ω–æ–π —Ü–µ–ª–µ–≤–æ–π
    scaled_target_col = f"{target_col}_scaled"
    
    return train_scaled, val_scaled, test_scaled, feature_cols, scaled_target_col, target_scaler


def create_improved_model(config, n_features):
    """–°–æ–∑–¥–∞–Ω–∏–µ —É–ª—É—á—à–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏ PatchTST"""
    
    model_params = {
        'c_in': n_features,
        'c_out': config['model']['output_size'],
        'context_window': config['model']['context_window'],
        'target_window': config['model']['target_window'],
        'patch_len': config['model']['patch_len'],
        'stride': config['model']['stride'],
        'd_model': config['model']['d_model'],
        'n_heads': config['model']['n_heads'],
        'd_ff': config['model']['d_ff'],
        'n_layers': config['model']['e_layers'],
        'dropout': config['model']['dropout'],
        'activation': config['model']['activation']
    }
    
    model = ImprovedPatchTST(**model_params)
    
    # –°—Ç–∞–±–∏–ª—å–Ω–∞—è –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –≤–µ—Å–æ–≤
    model = stabilize_model_initialization(model, method='xavier')
    
    return model


def load_config(config_path):
    """–ó–∞–≥—Ä—É–∑–∫–∞ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –∏–∑ YAML —Ñ–∞–π–ª–∞"""
    with open(config_path, 'r') as f:
        return yaml.safe_load(f)


def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –æ–±—É—á–µ–Ω–∏—è"""
    
    # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
    logger = get_logger("ImprovedTraining")
    
    # –ó–∞–≥—Ä—É–∑–∫–∞ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
    config = load_config('config/config.yaml')
    
    logger.info("="*80)
    logger.info("üöÄ –ó–∞–ø—É—Å–∫ —É–ª—É—á—à–µ–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è Crypto AI Trading")
    logger.info("="*80)
    
    try:
        # 1. –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö
        logger.info("üìä –≠—Ç–∞–ø 1: –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö")
        train_data, val_data, test_data, feature_cols, scaled_target_col, target_scaler = prepare_data_with_scaling(config, logger)
        
        # –í—Ä–µ–º–µ–Ω–Ω–æ –æ–±–Ω–æ–≤–ª—è–µ–º –∫–æ–Ω—Ñ–∏–≥
        original_target = config['model']['target_variable']
        config['model']['target_variable'] = scaled_target_col
        
        # 2. –°–æ–∑–¥–∞–Ω–∏–µ DataLoaders
        logger.info("üèóÔ∏è –≠—Ç–∞–ø 2: –°–æ–∑–¥–∞–Ω–∏–µ DataLoaders")
        train_loader, val_loader, test_loader = create_data_loaders(
            train_data, val_data, test_data, config, feature_cols
        )
        
        logger.info(f"–ë–∞—Ç—á–µ–π –≤ —ç–ø–æ—Ö–µ: Train={len(train_loader)}, Val={len(val_loader)}")
        
        # 3. –°–æ–∑–¥–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏
        logger.info("ü§ñ –≠—Ç–∞–ø 3: –°–æ–∑–¥–∞–Ω–∏–µ —É–ª—É—á—à–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏")
        model = create_improved_model(config, len(feature_cols))
        logger.info(f"–ü–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –º–æ–¥–µ–ª–∏: {sum(p.numel() for p in model.parameters()):,}")
        
        # –î–æ–±–∞–≤–ª—è–µ–º —Ö—É–∫–∏ –¥–ª—è –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏—è –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤
        add_gradient_hooks(model, logger)
        
        # –°–æ–∑–¥–∞–µ–º –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫—É NaN
        nan_diagnostics = NaNDiagnostics(logger)
        
        # 4. –°–æ–∑–¥–∞–Ω–∏–µ —Ç—Ä–µ–Ω–µ—Ä–∞
        logger.info("üéØ –≠—Ç–∞–ø 4: –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –æ–±—É—á–µ–Ω–∏—è")
        trainer = Trainer(model, config)
        
        # –î–µ–ª–∞–µ–º –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä –±–µ–∑–æ–ø–∞—Å–Ω—ã–º –æ—Ç NaN
        trainer.optimizer = nan_diagnostics.create_nan_safe_optimizer(trainer.optimizer)
        
        # 5. –û–±—É—á–µ–Ω–∏–µ
        logger.info("üöÄ –≠—Ç–∞–ø 5: –ó–∞–ø—É—Å–∫ –æ–±—É—á–µ–Ω–∏—è")
        logger.info(f"Learning rate: {config['model']['learning_rate']}")
        logger.info(f"Batch size: {config['model']['batch_size']}")
        logger.info(f"Epochs: {config['model']['epochs']}")
        
        # –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏
        history = trainer.train(train_loader, val_loader)
        
        # –ù–∞—Ö–æ–¥–∏–º –ª—É—á—à—É—é –º–æ–¥–µ–ª—å
        import glob
        model_files = glob.glob("models_saved/best_model_*.pth")
        if model_files:
            best_model_path = max(model_files, key=lambda x: os.path.getmtime(x))
            logger.info(f"‚úÖ –û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ! –õ—É—á—à–∞—è –º–æ–¥–µ–ª—å: {best_model_path}")
        else:
            logger.error("‚ùå –ú–æ–¥–µ–ª—å –Ω–µ –±—ã–ª–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞!")
            return
        
        # 6. –§–∏–Ω–∞–ª—å–Ω–∞—è –æ—Ü–µ–Ω–∫–∞
        logger.info("üìà –≠—Ç–∞–ø 6: –§–∏–Ω–∞–ª—å–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ –Ω–∞ —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö")
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –ª—É—á—à—É—é –º–æ–¥–µ–ª—å
        checkpoint = torch.load(best_model_path)
        model.load_state_dict(checkpoint['model_state_dict'])
        
        # –û—Ü–µ–Ω–∫–∞ –Ω–∞ —Ç–µ—Å—Ç–µ
        test_metrics = trainer.validate(test_loader)
        
        logger.info("üìä –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –Ω–∞ —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö:")
        for metric, value in test_metrics.items():
            logger.info(f"  {metric}: {value:.4f}")
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–∏
        metadata = {
            'original_target': original_target,
            'scaled_target': scaled_target_col,
            'target_scaler_path': 'models_saved/target_scaler.pkl',
            'feature_cols': feature_cols,
            'model_type': 'ImprovedPatchTST',
            'training_completed': datetime.now().isoformat()
        }
        
        import joblib
        metadata_path = Path(best_model_path).parent / 'training_metadata.pkl'
        joblib.dump(metadata, metadata_path)
        logger.info(f"–ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ {metadata_path}")
        
        # –í–æ—Å—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–π –∫–æ–Ω—Ñ–∏–≥
        config['model']['target_variable'] = original_target
        
        logger.info("="*80)
        logger.info("‚ú® –û–±—É—á–µ–Ω–∏–µ —É—Å–ø–µ—à–Ω–æ –∑–∞–≤–µ—Ä—à–µ–Ω–æ!")
        logger.info("="*80)
        
    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏: {str(e)}")
        raise


if __name__ == "__main__":
    main()