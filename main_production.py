#!/usr/bin/env python3
"""
Crypto AI Trading System - Production Ready –≤–µ—Ä—Å–∏—è
–í–∫–ª—é—á–∞–µ—Ç –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥, –≤–∞–ª–∏–¥–∞—Ü–∏—é –∏ –∑–∞—â–∏—Ç—É –æ—Ç –æ—à–∏–±–æ–∫
"""

import argparse
import yaml
from pathlib import Path
import torch
import pandas as pd
import numpy as np
from datetime import datetime
import warnings
import sys
import os
import json
from typing import Dict, List, Tuple, Optional

warnings.filterwarnings('ignore')

from utils.logger import get_logger

# –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è GPU –µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–µ–Ω
if torch.cuda.is_available():
    torch.backends.cudnn.benchmark = True
    torch.backends.cudnn.enabled = True
    torch.set_float32_matmul_precision('high')
    torch.backends.cuda.matmul.allow_tf32 = True
    torch.backends.cudnn.allow_tf32 = True

# –í–µ—Ä—Å–∏—è —Å–∏—Å—Ç–µ–º—ã
__version__ = "3.0.0-production"

class ProductionConfig:
    """–£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ production –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–µ–π"""
    
    def __init__(self, config_path: str):
        self.config = self.load_config(config_path)
        self.validate_config()
        self.apply_production_settings()
    
    def load_config(self, config_path: str) -> dict:
        """–ó–∞–≥—Ä—É–∑–∫–∞ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ —Å –≤–∞–ª–∏–¥–∞—Ü–∏–µ–π"""
        if not Path(config_path).exists():
            raise FileNotFoundError(f"–ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–æ–Ω–Ω—ã–π —Ñ–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω: {config_path}")
        
        with open(config_path, 'r', encoding='utf-8') as f:
            config = yaml.safe_load(f)
        
        return config
    
    def validate_config(self):
        """–í–∞–ª–∏–¥–∞—Ü–∏—è –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤"""
        required_keys = [
            'model', 'loss', 'data', 'performance', 
            'database', 'risk_management'
        ]
        
        for key in required_keys:
            if key not in self.config:
                raise ValueError(f"–û—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã–π —Ä–∞–∑–¥–µ–ª –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏: {key}")
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –¥–ª—è production
        if self.config['model']['learning_rate'] < 0.0001:
            print("‚ö†Ô∏è –ü—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ: –æ—á–µ–Ω—å –Ω–∏–∑–∫–∏–π learning rate –º–æ–∂–µ—Ç –∑–∞–º–µ–¥–ª–∏—Ç—å –æ–±—É—á–µ–Ω–∏–µ")
        
        if self.config['loss']['task_weights']['directions'] < 5.0:
            print("‚ö†Ô∏è –ü—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ: –Ω–∏–∑–∫–∏–π –≤–µ—Å direction loss –º–æ–∂–µ—Ç –ø—Ä–∏–≤–µ—Å—Ç–∏ –∫ –ø–ª–æ—Ö–∏–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è–º –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏—è")
    
    def apply_production_settings(self):
        """–ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ production-—Å–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã—Ö –Ω–∞—Å—Ç—Ä–æ–µ–∫"""
        # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ
        self.config['logging'] = self.config.get('logging', {})
        self.config['logging']['level'] = 'INFO'
        self.config['logging']['save_to_file'] = True
        
        # –í–∫–ª—é—á–∞–µ–º –≤—Å–µ –ø—Ä–æ–≤–µ—Ä–∫–∏
        self.config['validation'] = {
            'check_data_quality': True,
            'check_model_performance': True,
            'minimum_direction_accuracy': 0.6,
            'minimum_win_rate': 0.45,
            'maximum_flat_predictions': 0.7
        }
        
        # –ó–∞—â–∏—Ç–∞ –æ—Ç –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è
        self.config['model']['early_stopping_patience'] = 25
        self.config['model']['min_delta'] = 0.0001
        
        return self.config


class ModelValidator:
    """–í–∞–ª–∏–¥–∞—Ü–∏—è –º–æ–¥–µ–ª–∏ –ø–µ—Ä–µ–¥ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –≤ production"""
    
    def __init__(self, config: dict, logger):
        self.config = config
        self.logger = logger
        self.validation_results = {}
    
    def validate_model(self, model: torch.nn.Module, val_loader) -> bool:
        """–ü–æ–ª–Ω–∞—è –≤–∞–ª–∏–¥–∞—Ü–∏—è –º–æ–¥–µ–ª–∏"""
        self.logger.info("üîç –ó–∞–ø—É—Å–∫ production –≤–∞–ª–∏–¥–∞—Ü–∏–∏ –º–æ–¥–µ–ª–∏...")
        
        # 1. –ü—Ä–æ–≤–µ—Ä–∫–∞ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã
        if not self._validate_architecture(model):
            return False
        
        # 2. –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
        if not self._validate_performance(model, val_loader):
            return False
        
        # 3. –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π
        if not self._validate_prediction_diversity(model, val_loader):
            return False
        
        # 4. –ü—Ä–æ–≤–µ—Ä–∫–∞ —É—Å—Ç–æ–π—á–∏–≤–æ—Å—Ç–∏
        if not self._validate_robustness(model, val_loader):
            return False
        
        self._save_validation_report()
        return True
    
    def _validate_architecture(self, model: torch.nn.Module) -> bool:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ—Å—Ç–∏ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã"""
        self.logger.info("  üìê –ü—Ä–æ–≤–µ—Ä–∫–∞ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã...")
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞–ª–∏—á–∏—è –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã—Ö –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤
        required_modules = ['direction_head', 'future_returns_head', 'long_levels_head']
        
        for module_name in required_modules:
            if not hasattr(model, module_name):
                self.logger.error(f"    ‚ùå –û—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã–π –º–æ–¥—É–ª—å: {module_name}")
                return False
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–µ–π
        try:
            batch_size = 32
            seq_len = self.config['model']['context_window']
            n_features = self.config['model']['input_size']
            
            dummy_input = torch.randn(batch_size, seq_len, n_features).to(next(model.parameters()).device)
            with torch.no_grad():
                output = model(dummy_input)
            
            expected_output_size = self.config['model']['output_size']
            if output.shape[-1] != expected_output_size:
                self.logger.error(f"    ‚ùå –ù–µ–≤–µ—Ä–Ω—ã–π —Ä–∞–∑–º–µ—Ä –≤—ã—Ö–æ–¥–∞: {output.shape[-1]} != {expected_output_size}")
                return False
            
            self.logger.info("    ‚úÖ –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ –∫–æ—Ä—Ä–µ–∫—Ç–Ω–∞")
            return True
            
        except Exception as e:
            self.logger.error(f"    ‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø—Ä–æ–≤–µ—Ä–∫–µ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã: {e}")
            return False
    
    def _validate_performance(self, model: torch.nn.Module, val_loader) -> bool:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–∏"""
        self.logger.info("  üìä –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏...")
        
        from training.optimized_trainer import OptimizedTrainer
        
        # –°–æ–∑–¥–∞–µ–º –≤—Ä–µ–º–µ–Ω–Ω—ã–π trainer –¥–ª—è –æ—Ü–µ–Ω–∫–∏
        trainer = OptimizedTrainer(model, self.config, device=next(model.parameters()).device)
        
        # –ü–æ–ª—É—á–∞–µ–º –º–µ—Ç—Ä–∏–∫–∏
        metrics = trainer.validate_with_enhanced_metrics(val_loader)
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–µ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è
        min_requirements = self.config['validation']
        
        direction_accuracy = metrics.get('direction_accuracy_overall', 0)
        win_rate = metrics.get('win_rate_overall', 0)
        
        self.validation_results['direction_accuracy'] = direction_accuracy
        self.validation_results['win_rate'] = win_rate
        
        if direction_accuracy < min_requirements['minimum_direction_accuracy']:
            self.logger.error(f"    ‚ùå Direction accuracy —Å–ª–∏—à–∫–æ–º –Ω–∏–∑–∫–∞—è: {direction_accuracy:.3f} < {min_requirements['minimum_direction_accuracy']}")
            return False
        
        if win_rate < min_requirements['minimum_win_rate']:
            self.logger.error(f"    ‚ùå Win rate —Å–ª–∏—à–∫–æ–º –Ω–∏–∑–∫–∏–π: {win_rate:.3f} < {min_requirements['minimum_win_rate']}")
            return False
        
        self.logger.info(f"    ‚úÖ –ü—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–∞ (Accuracy: {direction_accuracy:.3f}, Win Rate: {win_rate:.3f})")
        return True
    
    def _validate_prediction_diversity(self, model: torch.nn.Module, val_loader) -> bool:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π"""
        self.logger.info("  üé≤ –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π...")
        
        from training.optimized_trainer import OptimizedTrainer
        
        trainer = OptimizedTrainer(model, self.config, device=next(model.parameters()).device)
        
        # –ü–æ–ª—É—á–∞–µ–º –ø–µ—Ä–≤—ã–π –±–∞—Ç—á –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
        for inputs, targets, _ in val_loader:
            inputs = inputs.to(next(model.parameters()).device)
            targets = targets.to(next(model.parameters()).device)
            break
        
        with torch.no_grad():
            outputs = model(inputs)
        
        # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º direction –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
        direction_metrics = trainer.compute_direction_metrics(outputs, targets)
        
        pred_entropy = direction_metrics.get('pred_entropy_overall', 0)
        flat_ratio = direction_metrics.get('pred_flat_ratio_overall', 1.0)
        
        self.validation_results['prediction_entropy'] = pred_entropy
        self.validation_results['flat_prediction_ratio'] = flat_ratio
        
        max_flat = self.config['validation']['maximum_flat_predictions']
        
        if flat_ratio > max_flat:
            self.logger.error(f"    ‚ùå –°–ª–∏—à–∫–æ–º –º–Ω–æ–≥–æ FLAT –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π: {flat_ratio:.1%} > {max_flat:.1%}")
            return False
        
        if pred_entropy < 0.3:
            self.logger.error(f"    ‚ùå –°–ª–∏—à–∫–æ–º –Ω–∏–∑–∫–∞—è —ç–Ω—Ç—Ä–æ–ø–∏—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π: {pred_entropy:.3f}")
            return False
        
        self.logger.info(f"    ‚úÖ –†–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ (Entropy: {pred_entropy:.3f}, FLAT: {flat_ratio:.1%})")
        return True
    
    def _validate_robustness(self, model: torch.nn.Module, val_loader) -> bool:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ —É—Å—Ç–æ–π—á–∏–≤–æ—Å—Ç–∏ –º–æ–¥–µ–ª–∏ –∫ —à—É–º—É"""
        self.logger.info("  üõ°Ô∏è –ü—Ä–æ–≤–µ—Ä–∫–∞ —É—Å—Ç–æ–π—á–∏–≤–æ—Å—Ç–∏...")
        
        # –ü–æ–ª—É—á–∞–µ–º –ø–µ—Ä–≤—ã–π –±–∞—Ç—á
        for inputs, targets, _ in val_loader:
            inputs = inputs.to(next(model.parameters()).device)
            break
        
        with torch.no_grad():
            # –û–±—ã—á–Ω—ã–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
            outputs_normal = model(inputs)
            
            # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è —Å –Ω–µ–±–æ–ª—å—à–∏–º —à—É–º–æ–º
            noise = torch.randn_like(inputs) * 0.01
            outputs_noisy = model(inputs + noise)
            
            # –°—Ä–∞–≤–Ω–∏–≤–∞–µ–º direction –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
            if hasattr(outputs_normal, '_direction_logits'):
                pred_normal = torch.argmax(outputs_normal._direction_logits, dim=-1)
                pred_noisy = torch.argmax(outputs_noisy._direction_logits, dim=-1)
                
                consistency = (pred_normal == pred_noisy).float().mean().item()
                
                self.validation_results['noise_robustness'] = consistency
                
                if consistency < 0.9:
                    self.logger.error(f"    ‚ùå –ù–∏–∑–∫–∞—è —É—Å—Ç–æ–π—á–∏–≤–æ—Å—Ç—å –∫ —à—É–º—É: {consistency:.3f}")
                    return False
                
                self.logger.info(f"    ‚úÖ –ú–æ–¥–µ–ª—å —É—Å—Ç–æ–π—á–∏–≤–∞ –∫ —à—É–º—É (consistency: {consistency:.3f})")
                return True
            else:
                self.logger.warning("    ‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –ø—Ä–æ–≤–µ—Ä–∏—Ç—å —É—Å—Ç–æ–π—á–∏–≤–æ—Å—Ç—å (–Ω–µ—Ç direction_logits)")
                return True
    
    def _save_validation_report(self):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –æ—Ç—á–µ—Ç–∞ –æ –≤–∞–ª–∏–¥–∞—Ü–∏–∏"""
        report_path = Path("validation_reports") / f"validation_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        report_path.parent.mkdir(exist_ok=True)
        
        report = {
            "timestamp": datetime.now().isoformat(),
            "version": __version__,
            "validation_results": self.validation_results,
            "passed": True
        }
        
        with open(report_path, 'w') as f:
            json.dump(report, f, indent=2)
        
        self.logger.info(f"  üíæ –û—Ç—á–µ—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {report_path}")


class ProductionInference:
    """–ö–ª–∞—Å—Å –¥–ª—è production inference —Å –∑–∞—â–∏—Ç–æ–π –æ—Ç –æ—à–∏–±–æ–∫"""
    
    def __init__(self, model_path: str, config: dict, logger):
        self.config = config
        self.logger = logger
        self.model = self._load_model(model_path)
        self.device = next(self.model.parameters()).device
    
    def _load_model(self, model_path: str) -> torch.nn.Module:
        """–ë–µ–∑–æ–ø–∞—Å–Ω–∞—è –∑–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏"""
        if not Path(model_path).exists():
            raise FileNotFoundError(f"–ú–æ–¥–µ–ª—å –Ω–µ –Ω–∞–π–¥–µ–Ω–∞: {model_path}")
        
        from models.patchtst_unified import create_unified_model
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º checkpoint
        checkpoint = torch.load(model_path, map_location='cpu')
        
        # –û–±–Ω–æ–≤–ª—è–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é –∏–∑ checkpoint –µ—Å–ª–∏ –µ—Å—Ç—å
        if isinstance(checkpoint, dict) and 'config' in checkpoint:
            saved_config = checkpoint['config']
            if 'model' in saved_config:
                self.config['model'].update(saved_config['model'])
        
        # –°–æ–∑–¥–∞–µ–º –º–æ–¥–µ–ª—å
        model = create_unified_model(self.config)
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –≤–µ—Å–∞
        if isinstance(checkpoint, dict) and 'model_state_dict' in checkpoint:
            model.load_state_dict(checkpoint['model_state_dict'])
        else:
            model.load_state_dict(checkpoint)
        
        # –ü–µ—Ä–µ–Ω–æ—Å–∏–º –Ω–∞ –Ω—É–∂–Ω–æ–µ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ
        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        model.to(device)
        model.eval()
        
        self.logger.info(f"‚úÖ –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞: {model_path}")
        return model
    
    def predict(self, data: torch.Tensor) -> Dict[str, torch.Tensor]:
        """–ë–µ–∑–æ–ø–∞—Å–Ω–æ–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ —Å –æ–±—Ä–∞–±–æ—Ç–∫–æ–π –æ—à–∏–±–æ–∫"""
        try:
            self.model.eval()
            with torch.no_grad():
                # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏
                if len(data.shape) == 2:
                    data = data.unsqueeze(0)  # –î–æ–±–∞–≤–ª—è–µ–º batch dimension
                
                # –ü–µ—Ä–µ–Ω–æ—Å –Ω–∞ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ
                data = data.to(self.device)
                
                # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ
                outputs = self.model(data)
                
                # –ü–∞—Ä—Å–∏–Ω–≥ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
                results = self._parse_outputs(outputs)
                
                # –í–∞–ª–∏–¥–∞—Ü–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
                if self._validate_predictions(results):
                    return results
                else:
                    raise ValueError("–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –Ω–µ –ø—Ä–æ—à–ª–∏ –≤–∞–ª–∏–¥–∞—Ü–∏—é")
                    
        except Exception as e:
            self.logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–∏: {e}")
            # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –±–µ–∑–æ–ø–∞—Å–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
            return self._get_safe_defaults()
    
    def _parse_outputs(self, outputs: torch.Tensor) -> Dict[str, torch.Tensor]:
        """–ü–∞—Ä—Å–∏–Ω–≥ –≤—ã—Ö–æ–¥–æ–≤ –º–æ–¥–µ–ª–∏ –≤ —É–¥–æ–±–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç"""
        results = {
            'future_returns': outputs[:, 0:4].cpu(),
            'directions': outputs[:, 4:8].cpu(),
            'long_levels': torch.sigmoid(outputs[:, 8:12]).cpu(),
            'short_levels': torch.sigmoid(outputs[:, 12:16]).cpu(),
            'risk_metrics': outputs[:, 16:20].cpu()
        }
        
        # –î–æ–±–∞–≤–ª—è–µ–º –∫–ª–∞—Å—Å—ã direction –µ—Å–ª–∏ –µ—Å—Ç—å –ª–æ–≥–∏—Ç—ã
        if hasattr(outputs, '_direction_logits'):
            direction_probs = torch.softmax(outputs._direction_logits, dim=-1)
            direction_classes = torch.argmax(direction_probs, dim=-1)
            results['direction_classes'] = direction_classes.cpu()
            results['direction_probs'] = direction_probs.cpu()
        
        return results
    
    def _validate_predictions(self, results: Dict[str, torch.Tensor]) -> bool:
        """–í–∞–ª–∏–¥–∞—Ü–∏—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π –Ω–∞ —Ä–∞–∑—É–º–Ω–æ—Å—Ç—å"""
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º future returns –≤ —Ä–∞–∑—É–º–Ω—ã—Ö –ø—Ä–µ–¥–µ–ª–∞—Ö (-50%, +50%)
        returns = results['future_returns']
        if torch.abs(returns).max() > 0.5:
            self.logger.warning("‚ö†Ô∏è –û–±–Ω–∞—Ä—É–∂–µ–Ω—ã —ç–∫—Å—Ç—Ä–µ–º–∞–ª—å–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è returns")
            return False
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ –≤ [0, 1]
        for key in ['long_levels', 'short_levels']:
            probs = results[key]
            if probs.min() < 0 or probs.max() > 1:
                self.logger.warning(f"‚ö†Ô∏è –ù–µ–¥–æ–ø—É—Å—Ç–∏–º—ã–µ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ –≤ {key}")
                return False
        
        return True
    
    def _get_safe_defaults(self) -> Dict[str, torch.Tensor]:
        """–ë–µ–∑–æ–ø–∞—Å–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é –ø—Ä–∏ –æ—à–∏–±–∫–µ"""
        batch_size = 1
        return {
            'future_returns': torch.zeros(batch_size, 4),
            'directions': torch.full((batch_size, 4), 2),  # FLAT
            'long_levels': torch.zeros(batch_size, 4),
            'short_levels': torch.zeros(batch_size, 4),
            'risk_metrics': torch.zeros(batch_size, 4),
            'direction_classes': torch.full((batch_size, 4), 2),  # FLAT
            'error': True
        }


def main():
    parser = argparse.ArgumentParser(description='Crypto AI Trading System - Production')
    parser.add_argument('--config', type=str, default='config/config.yaml', help='–ü—É—Ç—å –∫ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏')
    parser.add_argument('--mode', type=str, choices=['train', 'inference', 'validate', 'monitor'], 
                       default='train', help='–†–µ–∂–∏–º —Ä–∞–±–æ—Ç—ã')
    parser.add_argument('--model-path', type=str, help='–ü—É—Ç—å –∫ –º–æ–¥–µ–ª–∏ –¥–ª—è inference/validate')
    parser.add_argument('--data-path', type=str, help='–ü—É—Ç—å –∫ –¥–∞–Ω–Ω—ã–º –¥–ª—è inference')
    args = parser.parse_args()
    
    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è
    logger = get_logger('main_production')
    logger.info("="*80)
    logger.info(f"üöÄ Crypto AI Trading System v{__version__}")
    logger.info(f"üìÖ –ó–∞–ø—É—Å–∫: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    logger.info(f"üîß –†–µ–∂–∏–º: {args.mode}")
    logger.info("="*80)
    
    try:
        # –ó–∞–≥—Ä—É–∑–∫–∞ –∏ –≤–∞–ª–∏–¥–∞—Ü–∏—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
        config_manager = ProductionConfig(args.config)
        config = config_manager.config
        
        if args.mode == 'train':
            # –ü–æ–ª–Ω—ã–π —Ü–∏–∫–ª –æ–±—É—á–µ–Ω–∏—è —Å –≤–∞–ª–∏–¥–∞—Ü–∏–µ–π
            logger.info("üéì –ó–∞–ø—É—Å–∫ –æ–±—É—á–µ–Ω–∏—è –≤ production —Ä–µ–∂–∏–º–µ...")
            
            # –ò–º–ø–æ—Ä—Ç –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã—Ö –º–æ–¥—É–ª–µ–π
            from prepare_trading_data import main as prepare_data_main
            from training.unified_trainer import UnifiedTrainingPipeline
            
            # 1. –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö
            logger.info("üìä –≠—Ç–∞–ø 1: –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö...")
            prepare_data_main()
            
            # 2. –û–±—É—á–µ–Ω–∏–µ
            logger.info("üß† –≠—Ç–∞–ø 2: –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏...")
            pipeline = UnifiedTrainingPipeline(config)
            model, model_path, metrics = pipeline.train()
            
            # 3. –í–∞–ª–∏–¥–∞—Ü–∏—è
            logger.info("‚úÖ –≠—Ç–∞–ø 3: Production –≤–∞–ª–∏–¥–∞—Ü–∏—è...")
            validator = ModelValidator(config, logger)
            
            # –ó–∞–≥—Ä—É–∂–∞–µ–º val_loader –¥–ª—è –≤–∞–ª–∏–¥–∞—Ü–∏–∏
            from data.precomputed_dataset import create_precomputed_loaders
            _, val_loader, _ = create_precomputed_loaders(config, logger)
            
            if validator.validate_model(model, val_loader):
                logger.info("üéâ –ú–æ–¥–µ–ª—å –ø—Ä–æ—à–ª–∞ production –≤–∞–ª–∏–¥–∞—Ü–∏—é!")
                logger.info(f"üì¶ –ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: {model_path}")
            else:
                logger.error("‚ùå –ú–æ–¥–µ–ª—å –Ω–µ –ø—Ä–æ—à–ª–∞ production –≤–∞–ª–∏–¥–∞—Ü–∏—é!")
                logger.error("–ù–µ–æ–±—Ö–æ–¥–∏–º–æ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –∏–ª–∏ –∏–∑–º–µ–Ω–µ–Ω–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤")
        
        elif args.mode == 'inference':
            # Production inference
            if not args.model_path:
                logger.error("‚ùå –ù–µ–æ–±—Ö–æ–¥–∏–º–æ —É–∫–∞–∑–∞—Ç—å --model-path –¥–ª—è inference")
                return
            
            logger.info("üîÆ –ó–∞–ø—É—Å–∫ production inference...")
            
            inference = ProductionInference(args.model_path, config, logger)
            
            # –ó–¥–µ—Å—å –¥–æ–ª–∂–Ω–∞ –±—ã—Ç—å –∑–∞–≥—Ä—É–∑–∫–∞ —Ä–µ–∞–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
            # –î–ª—è –ø—Ä–∏–º–µ—Ä–∞ –∏—Å–ø–æ–ª—å–∑—É–µ–º —Å–ª—É—á–∞–π–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
            test_data = torch.randn(1, config['model']['context_window'], config['model']['input_size'])
            
            results = inference.predict(test_data)
            
            if 'error' not in results:
                logger.info("‚úÖ –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –≤—ã–ø–æ–ª–Ω–µ–Ω–æ —É—Å–ø–µ—à–Ω–æ:")
                logger.info(f"   Future Returns: {results['future_returns'].numpy()}")
                if 'direction_classes' in results:
                    classes = ['LONG', 'SHORT', 'FLAT']
                    for i, cls in enumerate(results['direction_classes'][0]):
                        logger.info(f"   Direction {i+1}: {classes[cls]}")
            else:
                logger.error("‚ùå –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω—ã –±–µ–∑–æ–ø–∞—Å–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é")
        
        elif args.mode == 'validate':
            # –û—Ç–¥–µ–ª—å–Ω–∞—è –≤–∞–ª–∏–¥–∞—Ü–∏—è —Å—É—â–µ—Å—Ç–≤—É—é—â–µ–π –º–æ–¥–µ–ª–∏
            if not args.model_path:
                logger.error("‚ùå –ù–µ–æ–±—Ö–æ–¥–∏–º–æ —É–∫–∞–∑–∞—Ç—å --model-path –¥–ª—è –≤–∞–ª–∏–¥–∞—Ü–∏–∏")
                return
            
            logger.info("üîç –ó–∞–ø—É—Å–∫ –≤–∞–ª–∏–¥–∞—Ü–∏–∏ –º–æ–¥–µ–ª–∏...")
            
            # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å
            from models.patchtst_unified import create_unified_model
            model = create_unified_model(config)
            
            checkpoint = torch.load(args.model_path)
            if isinstance(checkpoint, dict) and 'model_state_dict' in checkpoint:
                model.load_state_dict(checkpoint['model_state_dict'])
            else:
                model.load_state_dict(checkpoint)
            
            model.to(torch.device('cuda' if torch.cuda.is_available() else 'cpu'))
            
            # –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ –¥–ª—è –≤–∞–ª–∏–¥–∞—Ü–∏–∏
            from data.precomputed_dataset import create_precomputed_loaders
            _, val_loader, _ = create_precomputed_loaders(config, logger)
            
            # –í–∞–ª–∏–¥–∞—Ü–∏—è
            validator = ModelValidator(config, logger)
            if validator.validate_model(model, val_loader):
                logger.info("‚úÖ –ú–æ–¥–µ–ª—å –ø—Ä–æ—à–ª–∞ –≤–∞–ª–∏–¥–∞—Ü–∏—é!")
            else:
                logger.error("‚ùå –ú–æ–¥–µ–ª—å –Ω–µ –ø—Ä–æ—à–ª–∞ –≤–∞–ª–∏–¥–∞—Ü–∏—é!")
        
        elif args.mode == 'monitor':
            # –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –æ–±—É—á–µ–Ω–∏—è
            logger.info("üìä –ó–∞–ø—É—Å–∫ –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞...")
            
            import subprocess
            subprocess.run(['python', 'monitor_training.py'])
        
        logger.info("="*80)
        logger.info("‚úÖ –í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ —É—Å–ø–µ—à–Ω–æ!")
        logger.info("="*80)
        
    except Exception as e:
        logger.error(f"‚ùå –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞: {e}")
        logger.exception("–ü–æ–¥—Ä–æ–±–Ω–æ—Å—Ç–∏ –æ—à–∏–±–∫–∏:")
        sys.exit(1)


if __name__ == "__main__":
    main()