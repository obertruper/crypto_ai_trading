"""
–£–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è PatchTST –¥–ª—è –º–Ω–æ–≥–æ–∑–∞–¥–∞—á–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è
–†–µ—à–∞–µ—Ç –ø—Ä–æ–±–ª–µ–º—É –Ω–µ—Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏—è —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–µ–π: –º–æ–¥–µ–ª—å –≤—ã–¥–∞–µ—Ç —Ç–æ—á–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –≤—ã—Ö–æ–¥–æ–≤ –∏–∑ –∫–æ–Ω—Ñ–∏–≥–∞
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
from typing import Dict, List, Optional, Tuple
import math

from models.patchtst import (
    PositionalEncoding,
    PatchEmbedding,
    EINOPS_AVAILABLE
)


class RevIN(nn.Module):
    """Reversible Instance Normalization –¥–ª—è –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ä—è–¥–æ–≤"""
    
    def __init__(self, num_features: int, eps: float = 1e-5, affine: bool = True):
        super().__init__()
        self.num_features = num_features
        self.eps = eps
        self.affine = affine
        
        if self.affine:
            self.affine_weight = nn.Parameter(torch.ones(num_features))
            self.affine_bias = nn.Parameter(torch.zeros(num_features))
    
    def forward(self, x: torch.Tensor, mode: str = 'norm') -> torch.Tensor:
        """
        x: [Batch, Length, Features]
        mode: 'norm' –¥–ª—è –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏, 'denorm' –¥–ª—è –¥–µ–Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏
        """
        if mode == 'norm':
            self.mean = x.mean(dim=1, keepdim=True)
            self.std = torch.sqrt(x.var(dim=1, keepdim=True) + self.eps)
            x = (x - self.mean) / self.std
            
            if self.affine:
                x = x * self.affine_weight + self.affine_bias
        
        elif mode == 'denorm':
            if self.affine:
                x = (x - self.affine_bias) / self.affine_weight
                
            x = x * self.std + self.mean
            
        return x


class PatchTSTEncoder(nn.Module):
    """Encoder –¥–ª—è PatchTST —Å –æ—Å—Ç–∞—Ç–æ—á–Ω—ã–º–∏ —Å–æ–µ–¥–∏–Ω–µ–Ω–∏—è–º–∏"""
    
    def __init__(self, 
                 e_layers: int = 3,
                 d_model: int = 256,
                 n_heads: int = 4,
                 d_ff: int = 512,
                 dropout: float = 0.1,
                 activation: str = 'gelu',
                 res_attention: bool = True):
        super().__init__()
        
        self.e_layers = e_layers
        self.d_model = d_model
        self.res_attention = res_attention
        
        # Encoder layers
        self.encoder_layers = nn.ModuleList([
            EncoderLayer(
                d_model=d_model,
                n_heads=n_heads,
                d_ff=d_ff,
                dropout=dropout,
                activation=activation,
                res_attention=res_attention
            ) for _ in range(e_layers)
        ])
        
        self.norm = nn.LayerNorm(d_model)
        
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """
        x: [Batch, Length, Features]
        """
        # Encoder
        for encoder_layer in self.encoder_layers:
            x = encoder_layer(x)
            
        x = self.norm(x)
        
        return x


class EncoderLayer(nn.Module):
    """–°–ª–æ–π —ç–Ω–∫–æ–¥–µ—Ä–∞ —Å multi-head attention"""
    
    def __init__(self,
                 d_model: int,
                 n_heads: int,
                 d_ff: int,
                 dropout: float = 0.1,
                 activation: str = 'gelu',
                 res_attention: bool = True):
        super().__init__()
        
        self.res_attention = res_attention
        
        # Multi-head attention
        self.self_attention = nn.MultiheadAttention(
            embed_dim=d_model,
            num_heads=n_heads,
            dropout=dropout,
            batch_first=True
        )
        
        # Feed forward
        self.ff = nn.Sequential(
            nn.Linear(d_model, d_ff),
            nn.GELU() if activation == 'gelu' else nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(d_ff, d_model)
        )
        
        # Normalization
        self.norm1 = nn.LayerNorm(d_model)
        self.norm2 = nn.LayerNorm(d_model)
        
        # Dropout
        self.dropout1 = nn.Dropout(dropout)
        self.dropout2 = nn.Dropout(dropout)
        
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """
        x: [Batch, Length, Features]
        """
        # Self attention with residual
        attn_out, _ = self.self_attention(x, x, x)
        x = x + self.dropout1(attn_out)
        x = self.norm1(x)
        
        # Feed forward with residual
        ff_out = self.ff(x)
        x = x + self.dropout2(ff_out)
        x = self.norm2(x)
        
        return x


class UnifiedPatchTSTForTrading(nn.Module):
    """
    –ï–¥–∏–Ω–∞—è –º–æ–¥–µ–ª—å PatchTST –¥–ª—è —Ç–æ—Ä–≥–æ–≤–ª–∏ —Å –Ω–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º—ã–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ–º –≤—ã—Ö–æ–¥–æ–≤
    
    –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞:
    1. –û–±—â–∏–π —ç–Ω–∫–æ–¥–µ—Ä –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
    2. –ú–Ω–æ–≥–æ–∑–∞–¥–∞—á–Ω—ã–µ –≥–æ–ª–æ–≤—ã –¥–ª—è —Ä–∞–∑–Ω—ã—Ö —Ç–∏–ø–æ–≤ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π
    3. –ü—Ä–∞–≤–∏–ª—å–Ω–æ–µ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ —Å —Ü–µ–ª–µ–≤—ã–º–∏ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–º–∏ –∏–∑ –¥–∞—Ç–∞—Å–µ—Ç–∞
    """
    
    def __init__(self, config: Dict):
        super().__init__()
        self.config = config
        model_config = config.get('model', {})
        
        # –ë–∞–∑–æ–≤—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
        self.n_features = model_config.get('input_size', 159)
        self.context_window = model_config.get('context_window', 168)
        self.patch_len = model_config.get('patch_len', 16)
        self.stride = model_config.get('stride', 8)
        
        # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–∞
        self.d_model = model_config.get('d_model', 256)
        self.n_heads = model_config.get('n_heads', 4)
        self.e_layers = model_config.get('e_layers', 3)
        self.d_ff = model_config.get('d_ff', 512)
        self.dropout = model_config.get('dropout', 0.1)
        self.activation = model_config.get('activation', 'gelu')
        
        # –í–ê–ñ–ù–û: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –≤—ã—Ö–æ–¥–æ–≤ –±–µ—Ä–µ–º –∏–∑ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –∏–ª–∏ –∏—Å–ø–æ–ª—å–∑—É–µ–º 20 –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é –¥–ª—è v4.0
        self.n_outputs = model_config.get('output_size', 20)
        
        # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è
        self.revin = RevIN(
            num_features=self.n_features,
            eps=1e-5,
            affine=True
        )
        
        # –ü–∞—Ç—á —ç–º–±–µ–¥–¥–∏–Ω–≥–∏
        self.patch_embedding = nn.Conv1d(
            in_channels=self.n_features,
            out_channels=self.d_model,
            kernel_size=self.patch_len,
            stride=self.stride,
            padding=0,
            bias=False
        )
        
        # –ü–æ–∑–∏—Ü–∏–æ–Ω–Ω–æ–µ –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ
        self.n_patches = (self.context_window - self.patch_len) // self.stride + 1
        self.positional_encoding = PositionalEncoding(
            d_model=self.d_model,
            max_len=self.n_patches
        )
        
        # –û—Å–Ω–æ–≤–Ω–æ–π —ç–Ω–∫–æ–¥–µ—Ä
        self.encoder = PatchTSTEncoder(
            e_layers=self.e_layers,
            d_model=self.d_model,
            n_heads=self.n_heads,
            d_ff=self.d_ff,
            dropout=self.dropout,
            activation=self.activation,
            res_attention=True
        )
        
        # –ú–Ω–æ–≥–æ–∑–∞–¥–∞—á–Ω—ã–µ –≤—ã—Ö–æ–¥–Ω—ã–µ —Å–ª–æ–∏ –¥–ª—è v4.0 (20 –≤—ã—Ö–æ–¥–æ–≤)
        # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º –≤—ã—Ö–æ–¥—ã –ø–æ —Ç–∏–ø–∞–º –¥–ª—è –ª—É—á—à–µ–≥–æ –æ–±—É—á–µ–Ω–∏—è
        
        # 1. Future returns (—Ä–µ–≥—Ä–µ—Å—Å–∏—è) - 4 –≤—ã—Ö–æ–¥–∞ [0-3]
        self.future_returns_head = nn.Sequential(
            nn.Linear(self.d_model, self.d_model // 2),
            nn.ReLU(),
            nn.Dropout(self.dropout),
            nn.Linear(self.d_model // 2, 4),
            nn.Tanh()  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –≤—ã—Ö–æ–¥—ã –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏
        )
        
        # 2. –ù–∞–ø—Ä–∞–≤–ª–µ–Ω–∏—è –¥–≤–∏–∂–µ–Ω–∏—è (–∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è 3 –∫–ª–∞—Å—Å–∞) - 4 –≤—ã—Ö–æ–¥–∞ [4-7]
        # –ö–∞–∂–¥—ã–π –≤—ã—Ö–æ–¥ - –ª–æ–≥–∏—Ç—ã –¥–ª—è 3 –∫–ª–∞—Å—Å–æ–≤ [UP=0, DOWN=1, FLAT=2]
        self.direction_head = nn.Sequential(
            nn.Linear(self.d_model, self.d_model // 2),
            nn.ReLU(),
            nn.Dropout(self.dropout),
            nn.Linear(self.d_model // 2, 4 * 3)  # 4 —Ç–∞–π–º—Ñ—Ä–µ–π–º–∞ x 3 –∫–ª–∞—Å—Å–∞ = 12 –ª–æ–≥–∏—Ç–æ–≤
        )
        
        # 3. –î–æ—Å—Ç–∏–∂–µ–Ω–∏–µ —É—Ä–æ–≤–Ω–µ–π LONG (–±–∏–Ω–∞—Ä–Ω–∞—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è) - 4 –≤—ã—Ö–æ–¥–∞ [8-11]
        self.long_levels_head = nn.Sequential(
            nn.Linear(self.d_model, self.d_model // 2),
            nn.ReLU(),
            nn.Dropout(self.dropout),
            nn.Linear(self.d_model // 2, 4)
            # –õ–æ–≥–∏—Ç—ã, sigmoid –ø—Ä–∏–º–µ–Ω—è–µ—Ç—Å—è –≤ loss
        )
        
        # 4. –î–æ—Å—Ç–∏–∂–µ–Ω–∏–µ —É—Ä–æ–≤–Ω–µ–π SHORT (–±–∏–Ω–∞—Ä–Ω–∞—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è) - 4 –≤—ã—Ö–æ–¥–∞ [12-15]
        self.short_levels_head = nn.Sequential(
            nn.Linear(self.d_model, self.d_model // 2),
            nn.ReLU(),
            nn.Dropout(self.dropout),
            nn.Linear(self.d_model // 2, 4)
        )
        
        # 5. –†–∏—Å–∫-–º–µ—Ç—Ä–∏–∫–∏ (—Ä–µ–≥—Ä–µ—Å—Å–∏—è) - 4 –≤—ã—Ö–æ–¥–∞
        self.risk_metrics_head = nn.Sequential(
            nn.Linear(self.d_model, self.d_model // 2),
            nn.ReLU(),
            nn.Dropout(self.dropout),
            nn.Linear(self.d_model // 2, 4)
        )
        
        # –§–∏–Ω–∞–ª—å–Ω—ã–π —Å–ª–æ–π –¥–ª—è –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏—è –≤—Å–µ—Ö –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π
        self.output_projection = nn.Linear(self.d_model, self.d_model)
        
        # Layer normalization
        self.ln = nn.LayerNorm(self.d_model)
        
        # Temperature scaling –¥–ª—è –∫–∞–ª–∏–±—Ä–æ–≤–∫–∏ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏
        if model_config.get('temperature_scaling', False):
            # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä—É –∏–∑ –∫–æ–Ω—Ñ–∏–≥–∞ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é 2.0)
            # –ë–æ–ª—å—à–∞—è —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞ = –º–µ–Ω–µ–µ —É–≤–µ—Ä–µ–Ω–Ω—ã–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è = –º–µ–Ω—å—à–µ FLAT
            temp_value = model_config.get('temperature', 2.0)
            self.temperature = nn.Parameter(torch.ones(1) * temp_value)
        else:
            self.temperature = None
            
        # Confidence head –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è —Å–æ–±—Å—Ç–≤–µ–Ω–Ω–æ–π —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏
        self.confidence_head = nn.Sequential(
            nn.Linear(self.d_model, self.d_model // 2),
            nn.ReLU(),
            nn.Dropout(self.dropout),
            nn.Linear(self.d_model // 2, 4),  # 4 –∑–Ω–∞—á–µ–Ω–∏—è —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏
            nn.Tanh()  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –≤—ã—Ö–æ–¥ –≤ –¥–∏–∞–ø–∞–∑–æ–Ω–µ [-1, 1] –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏
        )
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –≤–µ—Å–æ–≤
        self._init_weights()
        
    def _init_weights(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –≤–µ—Å–æ–≤ –º–æ–¥–µ–ª–∏ —Å —É—á–µ—Ç–æ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏"""
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é –¥–ª—è direction head init
        direction_init_config = self.config.get('model', {}).get('direction_head_init', {})
        init_method = direction_init_config.get('method', 'balanced')
        bias_init = direction_init_config.get('bias_init', 'balanced')
        weight_scale = direction_init_config.get('weight_scale', 0.1)
        
        for name, module in self.named_modules():
            if isinstance(module, nn.Linear):
                # –°–ø–µ—Ü–∏–∞–ª—å–Ω–∞—è –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –¥–ª—è direction head
                if 'direction_head' in name:
                    if module.out_features == 12:  # –§–∏–Ω–∞–ª—å–Ω—ã–π —Å–ª–æ–π direction head
                        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –≤–µ—Å–æ–≤ —Å –º–∞–ª–æ–π –¥–∏—Å–ø–µ—Ä—Å–∏–µ–π –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏
                        nn.init.xavier_uniform_(module.weight, gain=weight_scale)
                        
                        if module.bias is not None and bias_init == 'balanced':
                            # –ê–≥—Ä–µ—Å—Å–∏–≤–Ω–∞—è –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ø—Ä–æ—Ç–∏–≤ —Å—Ö–ª–æ–ø—ã–≤–∞–Ω–∏—è –≤ FLAT
                            # –ü–æ –¥–∞–Ω–Ω—ã–º: LONG ~37.7%, SHORT ~37.0%, FLAT ~25.4%
                            with torch.no_grad():
                                bias = module.bias.view(4, 3)  # 4 —Ç–∞–π–º—Ñ—Ä–µ–π–º–∞ √ó 3 –∫–ª–∞—Å—Å–∞
                                if init_method == 'balanced':
                                    # –£–º–µ—Ä–µ–Ω–Ω–æ–µ —Å–º–µ—â–µ–Ω–∏–µ –≤ —Å—Ç–æ—Ä–æ–Ω—É LONG/SHORT –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏
                                    bias[:, 0] = 0.5    # LONG bias (—É–º–µ—Ä–µ–Ω–Ω–æ–µ —É–≤–µ–ª–∏—á–µ–Ω–∏–µ)
                                    bias[:, 1] = 0.5    # SHORT bias (—É–º–µ—Ä–µ–Ω–Ω–æ–µ —É–≤–µ–ª–∏—á–µ–Ω–∏–µ)  
                                    bias[:, 2] = -0.5   # FLAT bias (—É–º–µ—Ä–µ–Ω–Ω–æ–µ –ø–æ–¥–∞–≤–ª–µ–Ω–∏–µ)
                                elif init_method == 'proportional':
                                    # –ù–£–õ–ï–í–ê–Ø –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è - –º–æ–¥–µ–ª—å —Å–∞–º–∞ –Ω–∞—É—á–∏—Ç—Å—è
                                    bias[:, 0] = 0.0  # LONG
                                    bias[:, 1] = 0.0  # SHORT
                                    bias[:, 2] = 0.0  # FLAT
                        elif module.bias is not None:
                            nn.init.constant_(module.bias, 0)
                    else:
                        # –ü—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω—ã–µ —Å–ª–æ–∏ direction head
                        nn.init.xavier_uniform_(module.weight, gain=0.8)
                        if module.bias is not None:
                            nn.init.constant_(module.bias, 0)
                else:
                    # –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–∞—è –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –¥–ª—è –æ—Å—Ç–∞–ª—å–Ω—ã—Ö —Å–ª–æ–µ–≤
                    nn.init.xavier_uniform_(module.weight, gain=0.5)
                    if module.bias is not None:
                        nn.init.constant_(module.bias, 0)
                        
            elif isinstance(module, nn.Conv1d):
                # Kaiming –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Å —É–º–µ–Ω—å—à–µ–Ω–Ω—ã–º –º–∞—Å—à—Ç–∞–±–æ–º
                nn.init.kaiming_normal_(module.weight, mode='fan_in', nonlinearity='leaky_relu')
                # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–µ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏
                with torch.no_grad():
                    module.weight.mul_(0.7)
                
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """
        Forward pass
        
        Args:
            x: (batch_size, seq_len, n_features)
            
        Returns:
            output: (batch_size, n_outputs) - –≤—Å–µ —Ü–µ–ª–µ–≤—ã–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ
        """
        batch_size = x.shape[0]
        
        # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è
        x = self.revin(x, 'norm')
        
        # –ü–µ—Ä–µ—Å—Ç–∞–Ω–æ–≤–∫–∞ –¥–ª—è Conv1d: (B, L, C) -> (B, C, L)
        x = x.transpose(1, 2)
        
        # –°–æ–∑–¥–∞–Ω–∏–µ –ø–∞—Ç—á–µ–π
        x = self.patch_embedding(x)  # (B, d_model, n_patches)
        x = x.transpose(1, 2)  # (B, n_patches, d_model)
        
        # –ü–æ–∑–∏—Ü–∏–æ–Ω–Ω–æ–µ –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ
        x = self.positional_encoding(x)
        
        # –¢—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä —ç–Ω–∫–æ–¥–µ—Ä
        x = self.encoder(x)  # (B, n_patches, d_model)
        
        # –ì–ª–æ–±–∞–ª—å–Ω–æ–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–µ (—Å—Ä–µ–¥–Ω–µ–µ –ø–æ –ø–∞—Ç—á–∞–º)
        x_global = x.mean(dim=1)  # (B, d_model)
        
        # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è
        x_global = self.ln(x_global)
        
        # –ü—Ä–æ–µ–∫—Ü–∏—è –¥–ª—è –ª—É—á—à–µ–≥–æ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è
        x_projected = self.output_projection(x_global)
        
        # –ú–Ω–æ–≥–æ–∑–∞–¥–∞—á–Ω—ã–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è v4.0 (20 –≤—ã—Ö–æ–¥–æ–≤)
        future_returns = self.future_returns_head(x_projected)  # (B, 4)
        
        # Direction head –≤—ã–¥–∞–µ—Ç –ª–æ–≥–∏—Ç—ã –¥–ª—è 3 –∫–ª–∞—Å—Å–æ–≤ –Ω–∞ –∫–∞–∂–¥—ã–π —Ç–∞–π–º—Ñ—Ä–µ–π–º
        direction_logits = self.direction_head(x_projected)  # (B, 12) = 4 —Ç–∞–π–º—Ñ—Ä–µ–π–º–∞ * 3 –∫–ª–∞—Å—Å–∞
        
        # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –ª–æ–≥–∏—Ç—ã –≤ –∫–ª–∞—Å—Å—ã –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏ —Å–æ —Å—Ç–∞—Ä—ã–º —Ñ–æ—Ä–º–∞—Ç–æ–º
        direction_logits_reshaped = direction_logits.view(batch_size, 4, 3)  # (B, 4, 3)
        
        # –ü—Ä–∏–º–µ–Ω—è–µ–º temperature scaling –µ—Å–ª–∏ –≤–∫–ª—é—á–µ–Ω–æ
        if self.temperature is not None:
            # Temperature scaling –¥–µ–ª–∞–µ—Ç –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –±–æ–ª–µ–µ —É–≤–µ—Ä–µ–Ω–Ω—ã–º–∏
            direction_logits_reshaped = direction_logits_reshaped / self.temperature
        
        # –ü—Ä–∏–º–µ–Ω—è–µ–º softmax –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–µ–π
        direction_probs = torch.softmax(direction_logits_reshaped, dim=-1)  # (B, 4, 3)
        
        # –ü–æ–ª—É—á–∞–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã–µ –∫–ª–∞—Å—Å—ã
        directions = torch.argmax(direction_probs, dim=-1).float()  # (B, 4)
        
        # –î–æ–±–∞–≤–ª—è–µ–º –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–π –ø–æ—Ä–æ–≥ –¥–ª—è FLAT –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π
        if self.training == False:  # –¢–æ–ª—å–∫–æ –≤–æ –≤—Ä–µ–º—è –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞
            # –ü–æ–ª—É—á–∞–µ–º –º–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–µ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
            max_probs, _ = torch.max(direction_probs, dim=-1)  # (B, 4)
            
            # –ï—Å–ª–∏ –º–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å –Ω–∏–∂–µ –ø–æ—Ä–æ–≥–∞, –ø—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞–µ–º FLAT
            confidence_threshold = self.config.get('model', {}).get('direction_confidence_threshold', 0.45)
            low_confidence_mask = max_probs < confidence_threshold
            directions[low_confidence_mask] = 2.0  # FLAT = 2
        
        # –ü—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞–µ–º —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Ç–∞–π–º—Ñ—Ä–µ–π–º–∞
        confidence_scores = self.confidence_head(x_projected)  # (B, 4)
        
        long_levels = self.long_levels_head(x_projected)  # (B, 4)
        short_levels = self.short_levels_head(x_projected)  # (B, 4)
        risk_metrics = self.risk_metrics_head(x_projected)  # (B, 4)
        
        # –û–±—ä–µ–¥–∏–Ω—è–µ–º –≤—Å–µ –≤—ã—Ö–æ–¥—ã –≤ –æ–¥–∏–Ω —Ç–µ–Ω–∑–æ—Ä (20 –≤—ã—Ö–æ–¥–æ–≤)
        outputs = torch.cat([
            future_returns,    # 0-3: future_return_15m, 1h, 4h, 12h (—Ä–µ–≥—Ä–µ—Å—Å–∏—è)
            directions,        # 4-7: direction_15m, 1h, 4h, 12h (–∫–ª–∞—Å—Å—ã 0,1,2)
            long_levels,       # 8-11: long_will_reach_1pct_4h, 2pct_4h, 3pct_12h, 5pct_12h (–≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏)
            short_levels,      # 12-15: short_will_reach_1pct_4h, 2pct_4h, 3pct_12h, 5pct_12h (–≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏)
            risk_metrics       # 16-19: max_drawdown_1h, max_rally_1h, max_drawdown_4h, max_rally_4h (—Ä–µ–≥—Ä–µ—Å—Å–∏—è)
        ], dim=1)
        
        # –ö–†–ò–¢–ò–ß–ù–û: –ö–ª–∏–ø–ø–∏–Ω–≥ –≤—ã—Ö–æ–¥–æ–≤ –¥–ª—è –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–µ–Ω–∏—è –≤–∑—Ä—ã–≤–∞ –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤
        # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –≤—ã—Ö–æ–¥—ã –≤ —Ä–∞–∑—É–º–Ω—ã—Ö –ø—Ä–µ–¥–µ–ª–∞—Ö –ø–µ—Ä–µ–¥ –≤–æ–∑–≤—Ä–∞—Ç–æ–º
        outputs = torch.clamp(outputs, min=-10.0, max=10.0)
        
        # –î–ª—è –æ–±—É—á–µ–Ω–∏—è —Å–æ—Ö—Ä–∞–Ω—è–µ–º –ª–æ–≥–∏—Ç—ã direction –¥–ª—è –ø—Ä–∞–≤–∏–ª—å–Ω–æ–π loss —Ñ—É–Ω–∫—Ü–∏–∏
        # –í–ê–ñ–ù–û: –°–æ—Ö—Ä–∞–Ω—è–µ–º –ü–û–°–õ–ï clamp —á—Ç–æ–±—ã –∞—Ç—Ä–∏–±—É—Ç –Ω–µ –ø–æ—Ç–µ—Ä—è–ª—Å—è
        outputs._direction_logits = direction_logits_reshaped  # (B, 4, 3)
        outputs._confidence_scores = confidence_scores  # (B, 4) - —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Ç–∞–π–º—Ñ—Ä–µ–π–º–∞
        
        # v4.0: –ö–æ–º–º–µ–Ω—Ç–∞—Ä–∏–∏ –ø–æ –∞–∫—Ç–∏–≤–∞—Ü–∏—è–º –¥–ª—è 20 –≤—ã—Ö–æ–¥–æ–≤
        # Future returns (0-3) - –±–µ–∑ –∞–∫—Ç–∏–≤–∞—Ü–∏–∏ (—Ä–µ–≥—Ä–µ—Å—Å–∏—è)
        # Directions (4-7) - –±–µ–∑ –∞–∫—Ç–∏–≤–∞—Ü–∏–∏ (–∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã–µ –∫–∞–∫ —á–∏—Å–ª–∞)
        # Long levels (8-11) - –ª–æ–≥–∏—Ç—ã (sigmoid –≤ BCEWithLogitsLoss)
        # Short levels (12-15) - –ª–æ–≥–∏—Ç—ã (sigmoid –≤ BCEWithLogitsLoss) 
        # Risk metrics (16-19) - –±–µ–∑ –∞–∫—Ç–∏–≤–∞—Ü–∏–∏ (—Ä–µ–≥—Ä–µ—Å—Å–∏—è)
        
        # –í–æ–∑–≤—Ä–∞—â–∞–µ–º —Å—ã—Ä—ã–µ –ª–æ–≥–∏—Ç—ã –¥–ª—è –≤—Å–µ—Ö –≤—ã—Ö–æ–¥–æ–≤
        
        return outputs
    
    def get_direction_l2_loss(self) -> torch.Tensor:
        """–í—ã—á–∏—Å–ª—è–µ—Ç L2 —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—é –¥–ª—è direction head"""
        l2_loss = 0.0
        
        # –î–æ–±–∞–≤–ª—è–µ–º L2 —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—é —Ç–æ–ª—å–∫–æ –¥–ª—è direction head
        for name, param in self.direction_head.named_parameters():
            if 'weight' in name:
                l2_loss += torch.norm(param, 2) ** 2
                
        return l2_loss * self.config.get('model', {}).get('direction_l2_weight', 0.001)
    
    def get_output_names(self) -> List[str]:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –∏–º–µ–Ω–∞ –≤—Å–µ—Ö –≤—ã—Ö–æ–¥–æ–≤ –≤ –ø—Ä–∞–≤–∏–ª—å–Ω–æ–º –ø–æ—Ä—è–¥–∫–µ (20 –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö v4.0)"""
        return [
            # A. –ë–∞–∑–æ–≤—ã–µ –≤–æ–∑–≤—Ä–∞—Ç—ã (0-3)
            'future_return_15m', 'future_return_1h', 'future_return_4h', 'future_return_12h',
            # B. –ù–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –¥–≤–∏–∂–µ–Ω–∏—è (4-7)
            'direction_15m', 'direction_1h', 'direction_4h', 'direction_12h',
            # C. –î–æ—Å—Ç–∏–∂–µ–Ω–∏–µ —É—Ä–æ–≤–Ω–µ–π –ø—Ä–∏–±—ã–ª–∏ LONG (8-11)
            'long_will_reach_1pct_4h', 'long_will_reach_2pct_4h', 
            'long_will_reach_3pct_12h', 'long_will_reach_5pct_12h',
            # D. –î–æ—Å—Ç–∏–∂–µ–Ω–∏–µ —É—Ä–æ–≤–Ω–µ–π –ø—Ä–∏–±—ã–ª–∏ SHORT (12-15)
            'short_will_reach_1pct_4h', 'short_will_reach_2pct_4h',
            'short_will_reach_3pct_12h', 'short_will_reach_5pct_12h',
            # E. –†–∏—Å–∫-–º–µ—Ç—Ä–∏–∫–∏ (16-19)
            'max_drawdown_1h', 'max_rally_1h', 'max_drawdown_4h', 'max_rally_4h'
        ]


class UnifiedTradingLoss(nn.Module):
    """
    –£–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω–∞—è loss —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è 20 –≤—ã—Ö–æ–¥–æ–≤ v4.0
    –ö–æ–º–±–∏–Ω–∏—Ä—É–µ—Ç regression –∏ classification losses —Å weighted focus –Ω–∞ –∫—Ä—É–ø–Ω—ã–µ –¥–≤–∏–∂–µ–Ω–∏—è
    """
    
    def __init__(self, config: Dict):
        super().__init__()
        self.config = config
        loss_config = config.get('loss', {})
        
        # –í–µ—Å–∞ –¥–ª—è —Ä–∞–∑–Ω—ã—Ö –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤
        self.future_return_weight = loss_config.get('future_return_weight', 1.0)
        self.tp_weight = loss_config.get('tp_weight', 0.8)
        self.sl_weight = loss_config.get('sl_weight', 1.2)
        self.signal_weight = loss_config.get('signal_weight', 0.6)
        
        # –í–µ—Å–∞ –¥–ª—è –∫—Ä—É–ø–Ω—ã—Ö –¥–≤–∏–∂–µ–Ω–∏–π
        self.large_move_threshold = loss_config.get('large_move_threshold', 0.02)  # 2%
        self.large_move_weight = loss_config.get('large_move_weight', 5.0)
        
        # –®—Ç—Ä–∞—Ñ –∑–∞ –Ω–µ–ø—Ä–∞–≤–∏–ª—å–Ω–æ–µ –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–µ
        self.wrong_direction_penalty = loss_config.get('wrong_direction_penalty', 2.0)
        
        # Loss —Ñ—É–Ω–∫—Ü–∏–∏ (–∏—Å–ø—Ä–∞–≤–ª–µ–Ω–æ –¥–ª—è mixed precision)
        self.mse_loss = nn.MSELoss(reduction='none')
        self.bce_with_logits_loss = nn.BCEWithLogitsLoss(reduction='none')  # –ë–µ–∑–æ–ø–∞—Å–Ω–æ –¥–ª—è autocast
        
    def forward(self, predictions: torch.Tensor, targets: torch.Tensor, 
                price_changes: Optional[torch.Tensor] = None) -> torch.Tensor:
        """
        –í—ã—á–∏—Å–ª–µ–Ω–∏–µ loss –¥–ª—è v4.0 —Å weighted focus –Ω–∞ –∫—Ä—É–ø–Ω—ã–µ –¥–≤–∏–∂–µ–Ω–∏—è
        
        Args:
            predictions: (batch_size, 20)
            targets: (batch_size, 20)
            price_changes: (batch_size, 4) - –æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–µ –∏–∑–º–µ–Ω–µ–Ω–∏—è —Ü–µ–Ω –¥–ª—è –≤–∑–≤–µ—à–∏–≤–∞–Ω–∏—è
            
        Returns:
            loss: —Å–∫–∞–ª—è—Ä
        """
        assert predictions.shape == targets.shape, \
            f"Shape mismatch: predictions {predictions.shape} vs targets {targets.shape}"
        
        batch_size = predictions.shape[0]
        losses = []
        
        # –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï –¥–ª—è v4.0: –û–±—Ä–∞–±–æ—Ç–∫–∞ 20 –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö
        # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è —Ü–µ–ª–µ–≤—ã—Ö –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –≥–¥–µ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ
        normalized_targets = targets.clone()
        
        # 1. Future returns (–∏–Ω–¥–µ–∫—Å—ã 0-3) - —Ä–µ–≥—Ä–µ—Å—Å–∏—è —Å weighted loss
        # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –∏–∑ –ø—Ä–æ—Ü–µ–Ω—Ç–æ–≤ –≤ –¥–æ–ª–∏
        normalized_targets[:, :4] = targets[:, :4] / 100.0
        
        # –ë–∞–∑–æ–≤—ã–π MSE loss
        future_return_loss = self.mse_loss(
            predictions[:, :4], 
            normalized_targets[:, :4]
        )
        
        # –í–∑–≤–µ—à–∏–≤–∞–Ω–∏–µ –¥–ª—è –∫—Ä—É–ø–Ω—ã—Ö –¥–≤–∏–∂–µ–Ω–∏–π
        if price_changes is not None:
            # price_changes —É–∂–µ –≤ –¥–æ–ª—è—Ö
            large_move_mask = torch.abs(price_changes) > self.large_move_threshold
            
            # –ü—Ä–∏–º–µ–Ω—è–µ–º –±–æ–ª—å—à–∏–π –≤–µ—Å –∫ –∫—Ä—É–ø–Ω—ã–º –¥–≤–∏–∂–µ–Ω–∏—è–º
            weights = torch.ones_like(future_return_loss)
            weights[large_move_mask] = self.large_move_weight
            
            # –í–∑–≤–µ—à–µ–Ω–Ω—ã–π loss
            future_return_loss = (future_return_loss * weights).mean()
        else:
            # –ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–π –ø–æ–¥—Ö–æ–¥ - –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —Å–∞–º–∏ —Ü–µ–ª–µ–≤—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è –¥–ª—è –≤–∑–≤–µ—à–∏–≤–∞–Ω–∏—è
            weights = 1.0 + torch.abs(normalized_targets[:, :4]) * 10.0
            future_return_loss = (future_return_loss * weights).mean()
        
        losses.append(future_return_loss * self.future_return_weight)
        
        # 2. –ù–∞–ø—Ä–∞–≤–ª–µ–Ω–∏—è –¥–≤–∏–∂–µ–Ω–∏—è (–∏–Ω–¥–µ–∫—Å—ã 4-7) - –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è —Å —à—Ç—Ä–∞—Ñ–æ–º –∑–∞ –Ω–µ–ø—Ä–∞–≤–∏–ª—å–Ω–æ–µ –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–µ
        # –ü–æ–ª—É—á–∞–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã–µ –∏ –∏—Å—Ç–∏–Ω–Ω—ã–µ –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏—è
        pred_directions = predictions[:, 4:8]
        true_directions = targets[:, 4:8]
        
        # –°–ø–µ—Ü–∏–∞–ª—å–Ω—ã–π loss –¥–ª—è –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–π
        direction_losses = []
        
        for i in range(4):  # –î–ª—è –∫–∞–∂–¥–æ–≥–æ —Ç–∞–π–º—Ñ—Ä–µ–π–º–∞
            # –ë–∞–∑–æ–≤—ã–π MSE
            base_loss = self.mse_loss(
                pred_directions[:, i],
                true_directions[:, i] / 2.0
            )
            
            # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–π —à—Ç—Ä–∞—Ñ –∑–∞ –ø—Ä–æ—Ç–∏–≤–æ–ø–æ–ª–æ–∂–Ω–æ–µ –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–µ
            # –ï—Å–ª–∏ –∏—Å—Ç–∏–Ω–Ω–æ–µ = 0 (UP), –∞ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–æ = 1 (DOWN) –∏–ª–∏ –Ω–∞–æ–±–æ—Ä–æ—Ç
            pred_class = torch.round(pred_directions[:, i] * 2).clamp(0, 2)
            true_class = true_directions[:, i]
            
            wrong_direction_mask = (
                ((true_class == 0) & (pred_class == 1)) |  # True UP, Pred DOWN
                ((true_class == 1) & (pred_class == 0))    # True DOWN, Pred UP
            )
            
            # –ü—Ä–∏–º–µ–Ω—è–µ–º —à—Ç—Ä–∞—Ñ
            penalty_weights = torch.ones_like(base_loss)
            penalty_weights[wrong_direction_mask] = self.wrong_direction_penalty
            
            weighted_loss = (base_loss * penalty_weights).mean()
            direction_losses.append(weighted_loss)
        
        direction_loss = sum(direction_losses) / len(direction_losses)
        losses.append(direction_loss * self.signal_weight)
        
        # 3. –î–æ—Å—Ç–∏–∂–µ–Ω–∏–µ —É—Ä–æ–≤–Ω–µ–π LONG (–∏–Ω–¥–µ–∫—Å—ã 8-11) - –±–∏–Ω–∞—Ä–Ω–∞—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è
        long_levels_loss = self.bce_with_logits_loss(
            predictions[:, 8:12],
            targets[:, 8:12]
        )
        
        # –í–∑–≤–µ—à–∏–≤–∞–Ω–∏–µ –¥–ª—è –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã—Ö –ø—Ä–∏–º–µ—Ä–æ–≤ (–∫–æ–≥–¥–∞ —Ü–µ–ª—å –¥–æ—Å—Ç–∏–≥–Ω—É—Ç–∞)
        positive_weight = 2.0  # –ë–æ–ª—å—à–µ –≤–µ—Å–∞ –Ω–∞ –ø—Ä–∞–≤–∏–ª—å–Ω–æ–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –¥–æ—Å—Ç–∏–∂–µ–Ω–∏—è —É—Ä–æ–≤–Ω—è
        weights = torch.ones_like(long_levels_loss)
        weights[targets[:, 8:12] == 1] = positive_weight
        
        long_levels_loss = (long_levels_loss * weights).mean()
        losses.append(long_levels_loss * self.tp_weight)
        
        # 4. –î–æ—Å—Ç–∏–∂–µ–Ω–∏–µ —É—Ä–æ–≤–Ω–µ–π SHORT (–∏–Ω–¥–µ–∫—Å—ã 12-15) - –±–∏–Ω–∞—Ä–Ω–∞—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è
        short_levels_loss = self.bce_with_logits_loss(
            predictions[:, 12:16],
            targets[:, 12:16]
        )
        
        # –ê–Ω–∞–ª–æ–≥–∏—á–Ω–æ–µ –≤–∑–≤–µ—à–∏–≤–∞–Ω–∏–µ
        weights = torch.ones_like(short_levels_loss)
        weights[targets[:, 12:16] == 1] = positive_weight
        
        short_levels_loss = (short_levels_loss * weights).mean()
        losses.append(short_levels_loss * self.tp_weight)
        
        # 5. –†–∏—Å–∫-–º–µ—Ç—Ä–∏–∫–∏ (–∏–Ω–¥–µ–∫—Å—ã 16-19) - —Ä–µ–≥—Ä–µ—Å—Å–∏—è —Å —Ñ–æ–∫—É—Å–æ–º –Ω–∞ –±–æ–ª—å—à–∏–µ drawdowns
        risk_metrics_loss = self.mse_loss(
            predictions[:, 16:20],
            targets[:, 16:20]  # –£–∂–µ –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω—ã –≤ feature_engineering
        )
        
        # –ë–æ–ª—å—à–µ –≤–µ—Å–∞ –Ω–∞ –ø—Ä–∞–≤–∏–ª—å–Ω–æ–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –±–æ–ª—å—à–∏—Ö drawdowns
        risk_weights = 1.0 + torch.abs(targets[:, 16:20]) * 5.0
        risk_metrics_loss = (risk_metrics_loss * risk_weights).mean()
        
        losses.append(risk_metrics_loss * self.signal_weight)
        
        # –û–±—â–∏–π loss —Å –∞–¥–∞–ø—Ç–∏–≤–Ω—ã–º –≤–∑–≤–µ—à–∏–≤–∞–Ω–∏–µ–º
        # –ú–æ–∂–Ω–æ –¥–æ–±–∞–≤–∏—Ç—å –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–æ–µ –∏–∑–º–µ–Ω–µ–Ω–∏–µ –≤–µ—Å–æ–≤ –≤ –ø—Ä–æ—Ü–µ—Å—Å–µ –æ–±—É—á–µ–Ω–∏—è
        total_loss = sum(losses) / len(losses)
        
        # L2 —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—è –¥–ª—è –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–µ–Ω–∏—è –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è
        l2_lambda = self.config.get('model', {}).get('l2_regularization', 0.001)
        if l2_lambda > 0 and hasattr(self, 'model') and self.model is not None:
            l2_reg = 0.0
            for param in self.model.parameters():
                if param.requires_grad:
                    l2_reg += torch.norm(param, 2) ** 2
            total_loss += l2_lambda * l2_reg
        
        return total_loss


class DirectionalTradingLoss(nn.Module):
    """
    –°–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è loss —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –º–∞–∫—Å–∏–º–∏–∑–∞—Ü–∏–∏ –ø—Ä–∏–±—ã–ª—å–Ω–æ—Å—Ç–∏
    –£—á–∏—Ç—ã–≤–∞–µ—Ç –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω—ã–π P&L –æ—Ç —Ç–æ—Ä–≥–æ–≤—ã—Ö —Ä–µ—à–µ–Ω–∏–π
    """
    
    def __init__(self, 
                 commission: float = 0.001,
                 class_weights: Optional[List[float]] = None,
                 profit_focus_weight: float = 10.0):
        super().__init__()
        self.commission = commission
        self.profit_focus_weight = profit_focus_weight
        
        # –í–µ—Å–∞ –∫–ª–∞—Å—Å–æ–≤ (–º–æ–∂–Ω–æ –Ω–∞—Å—Ç—Ä–æ–∏—Ç—å –¥–ª—è –±–∞–ª–∞–Ω—Å–∞)
        if class_weights is None:
            self.class_weights = torch.tensor([1.0, 1.0, 0.5])  # –ú–µ–Ω—å—à–µ –≤–µ—Å –¥–ª—è FLAT
        else:
            self.class_weights = torch.tensor(class_weights)
            
    def forward(self, 
                predictions: Dict[str, torch.Tensor], 
                targets: Dict[str, torch.Tensor],
                price_changes: Dict[str, torch.Tensor]) -> torch.Tensor:
        """
        –í—ã—á–∏—Å–ª–µ–Ω–∏–µ loss —Å —É—á–µ—Ç–æ–º –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω–æ–π –ø—Ä–∏–±—ã–ª–∏
        
        Args:
            predictions: Dict —Å –ª–æ–≥–∏—Ç–∞–º–∏ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Ç–∞–π–º—Ñ—Ä–µ–π–º–∞
            targets: Dict —Å –∏—Å—Ç–∏–Ω–Ω—ã–º–∏ –º–µ—Ç–∫–∞–º–∏ (0=UP, 1=DOWN, 2=FLAT)
            price_changes: Dict —Å –ø—Ä–æ—Ü–µ–Ω—Ç–Ω—ã–º–∏ –∏–∑–º–µ–Ω–µ–Ω–∏—è–º–∏ —Ü–µ–Ω—ã
            
        Returns:
            –û–±—â–∏–π loss
        """
        total_loss = 0.0
        timeframe_weights = {'15m': 0.2, '1h': 0.3, '4h': 0.35, '12h': 0.15}
        
        for timeframe in ['15m', '1h', '4h', '12h']:
            key = f'direction_{timeframe}'
            
            # Base cross entropy —Å –≤–µ—Å–∞–º–∏ –∫–ª–∞—Å—Å–æ–≤
            ce_loss = F.cross_entropy(
                predictions[key], 
                targets[key],
                weight=self.class_weights.to(predictions[key].device),
                reduction='none'
            )
            
            # –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ–º –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω—ã–π P&L
            predicted_direction = predictions[key].argmax(dim=1)
            actual_direction = targets[key]
            price_change = price_changes[timeframe]
            
            # –£–±–µ–∂–¥–∞–µ–º—Å—è —á—Ç–æ price_change –∏–º–µ–µ—Ç –ø—Ä–∞–≤–∏–ª—å–Ω—É—é —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å
            if price_change.dim() > 1:
                price_change = price_change.squeeze(-1)
            
            # –ü–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω–∞—è –ø—Ä–∏–±—ã–ª—å/—É–±—ã—Ç–æ–∫ –æ—Ç –∫–∞–∂–¥–æ–≥–æ —Ä–µ—à–µ–Ω–∏—è
            potential_pnl = torch.zeros_like(ce_loss)
            
            # LONG –ø–æ–∑–∏—Ü–∏–∏ (predicted=0)
            long_mask = predicted_direction == 0
            potential_pnl[long_mask] = price_change[long_mask] - self.commission
            
            # SHORT –ø–æ–∑–∏—Ü–∏–∏ (predicted=1)
            short_mask = predicted_direction == 1  
            potential_pnl[short_mask] = -price_change[short_mask] - self.commission
            
            # –í–µ—Å–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ –≤–µ–ª–∏—á–∏–Ω—ã –¥–≤–∏–∂–µ–Ω–∏—è —Ü–µ–Ω—ã
            # –ë–æ–ª—å—à–µ —à—Ç—Ä–∞—Ñ –∑–∞ –æ—à–∏–±–∫–∏ –Ω–∞ –∫—Ä—É–ø–Ω—ã—Ö –¥–≤–∏–∂–µ–Ω–∏—è—Ö
            movement_weight = 1 + torch.abs(price_change) * self.profit_focus_weight
            
            # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–π —à—Ç—Ä–∞—Ñ –∑–∞ –Ω–µ–ø—Ä–∞–≤–∏–ª—å–Ω–æ–µ –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–µ
            wrong_direction_mask = (
                ((predicted_direction == 0) & (actual_direction == 1)) |  # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–ª–∏ UP, –±—ã–ª–æ DOWN
                ((predicted_direction == 1) & (actual_direction == 0))    # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–ª–∏ DOWN, –±—ã–ª–æ UP
            )
            direction_penalty = wrong_direction_mask.float() * torch.abs(price_change) * 2
            
            # –®—Ç—Ä–∞—Ñ –∑–∞ false positives (—Ç–æ—Ä–≥–æ–≤–ª—è –∫–æ–≥–¥–∞ –Ω—É–∂–Ω–æ –±—ã–ª–æ –∂–¥–∞—Ç—å)
            false_positive_penalty = ((predicted_direction != 2) & (actual_direction == 2)).float() * 0.5
            
            # –ö–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–π loss –¥–ª—è —Ç–∞–π–º—Ñ—Ä–µ–π–º–∞
            timeframe_loss = (ce_loss * movement_weight + direction_penalty + false_positive_penalty).mean()
            
            # –í–∑–≤–µ—à–µ–Ω–Ω—ã–π –≤–∫–ª–∞–¥ –≤ –æ–±—â–∏–π loss
            total_loss += timeframe_loss * timeframe_weights[timeframe]
            
        return total_loss


class DirectionalMultiTaskLoss(nn.Module):
    """
    –ì–∏–±—Ä–∏–¥–Ω–∞—è loss —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è multi-task learning:
    - MSE –¥–ª—è regression –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö (returns, risk metrics)
    - CrossEntropy –¥–ª—è classification –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö (directions)
    - BCE –¥–ª—è binary classification (levels)
    
    –û—Å–Ω–æ–≤–∞–Ω–∞ –Ω–∞ –Ω–∞—É—á–Ω—ã—Ö –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è—Ö 2024 –≥–æ–¥–∞ –ø–æ crypto direction prediction
    """
    
    def __init__(self, config: Dict):
        super().__init__()
        self.config = config
        
        # –í–µ—Å–∞ –¥–ª—è —Ä–∞–∑–Ω—ã—Ö —Ç–∏–ø–æ–≤ –∑–∞–¥–∞—á –∏–∑ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
        task_weights = config.get('loss', {}).get('task_weights', {})
        self.future_returns_weight = task_weights.get('future_returns', 1.0)
        self.directions_weight = task_weights.get('directions', 3.0)  # –ë–æ–ª—å—à–∏–π –≤–µ—Å –¥–ª—è –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–π
        self.long_levels_weight = task_weights.get('long_levels', 1.0)
        self.short_levels_weight = task_weights.get('short_levels', 1.0)
        self.risk_metrics_weight = task_weights.get('risk_metrics', 0.5)
        
        # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è direction focus
        self.large_move_weight = config.get('loss', {}).get('large_move_weight', 5.0)
        self.min_movement_threshold = config.get('loss', {}).get('large_move_threshold', 0.003)
        
        # Loss —Ñ—É–Ω–∫—Ü–∏–∏
        self.mse_loss = nn.MSELoss(reduction='none')
        
        # –í–µ—Å–∞ –∫–ª–∞—Å—Å–æ–≤ –¥–ª—è –±–æ—Ä—å–±—ã —Å –¥–∏—Å–±–∞–ª–∞–Ω—Å–æ–º
        # –û—Å–Ω–æ–≤–∞–Ω–æ –Ω–∞ –∞–Ω–∞–ª–∏–∑–µ —Ä–µ–∞–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö: LONG‚âà19%, SHORT‚âà18%, FLAT‚âà63%
        # –†–µ–∫–æ–º–µ–Ω–¥—É–µ–º—ã–µ –≤–µ—Å–∞ –∏–∑ analyze_class_distribution: [1.16, 1.20, 0.64]
        
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º –≤–µ—Å–∞ –∏–∑ –∫–æ–Ω—Ñ–∏–≥–∞ –∏–ª–∏ —Å–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
        config_weights = config.get('loss', {}).get('class_weights', [1.3, 1.3, 0.7])
        class_weights = torch.tensor(config_weights)  # LONG, SHORT, FLAT
        
        # –ú–µ—Ç–æ–¥ 2: –î–∏–Ω–∞–º–∏—á–µ—Å–∫–∞—è –∞–¥–∞–ø—Ç–∞—Ü–∏—è –≤–µ—Å–æ–≤ –Ω–∞ –æ—Å–Ω–æ–≤–µ –±–∞—Ç—á–∞
        # –≠—Ç–æ –ø–æ–∑–≤–æ–ª–∏—Ç –º–æ–¥–µ–ª–∏ –∞–¥–∞–ø—Ç–∏—Ä–æ–≤–∞—Ç—å—Å—è –∫ –ª–æ–∫–∞–ª—å–Ω—ã–º —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è–º
        self.use_dynamic_weights = config.get('loss', {}).get('use_dynamic_class_weights', True)
        self.class_weight_momentum = 0.9  # –≠–∫—Å–ø–æ–Ω–µ–Ω—Ü–∏–∞–ª—å–Ω–æ–µ —Å–∫–æ–ª—å–∑—è—â–µ–µ —Å—Ä–µ–¥–Ω–µ–µ
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –Ω–∞—á–∞–ª—å–Ω—ã–µ –≤–µ—Å–∞
        self.register_buffer('class_weights', class_weights)
        self.register_buffer('running_class_counts', torch.zeros(3))
        self.register_buffer('total_samples', torch.tensor(0.0))
        
        # CrossEntropy —Å –Ω–∞—á–∞–ª—å–Ω—ã–º–∏ –≤–µ—Å–∞–º–∏
        self.cross_entropy_loss = nn.CrossEntropyLoss(
            weight=self.class_weights.cuda() if torch.cuda.is_available() else self.class_weights,
            reduction='none'
        )
        self.bce_with_logits_loss = nn.BCEWithLogitsLoss(reduction='none')
        
        # Focal Loss –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è –Ω–µ—Å–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –∫–ª–∞—Å—Å–æ–≤ direction
        self.focal_alpha = config.get('loss', {}).get('focal_alpha', 0.25)
        self.focal_gamma = config.get('loss', {}).get('focal_gamma', 2.0)
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤–µ—Å–∞ –∫–ª–∞—Å—Å–æ–≤ –¥–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –≤ focal loss
        self.class_weights = class_weights
        
        # –°—á–µ—Ç—á–∏–∫ —ç–ø–æ—Ö –¥–ª—è –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–æ–≥–æ –≤–µ—Å–∞ direction loss
        self.current_epoch = 0
        self.warmup_epochs = 10  # –ü–æ—Å—Ç–µ–ø–µ–Ω–Ω–æ–µ —É–≤–µ–ª–∏—á–µ–Ω–∏–µ –≤–µ—Å–∞ direction loss
        
        # Label smoothing –ø–∞—Ä–∞–º–µ—Ç—Ä  
        self.label_smoothing = config.get('model', {}).get('label_smoothing', 0.2)
        
        # –®—Ç—Ä–∞—Ñ –∑–∞ –Ω–µ–ø—Ä–∞–≤–∏–ª—å–Ω–æ–µ –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–µ
        self.wrong_direction_penalty = config.get('loss', {}).get('wrong_direction_penalty', 3.0)
        
        # –ê–∫—Ç–∏–≤–Ω—ã–µ losses –¥–ª—è –ø–æ—ç—Ç–∞–ø–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è
        self.active_losses = ["all"]  # –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é –≤—Å–µ losses –∞–∫—Ç–∏–≤–Ω—ã
        
    def set_active_losses(self, active_losses: List[str]):
        """
        –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ—Ç –∞–∫—Ç–∏–≤–Ω—ã–µ loss –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã –¥–ª—è –ø–æ—ç—Ç–∞–ø–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è
        
        Args:
            active_losses: —Å–ø–∏—Å–æ–∫ –∞–∫—Ç–∏–≤–Ω—ã—Ö losses, –Ω–∞–ø—Ä–∏–º–µ—Ä:
                ["directions"] - —Ç–æ–ª—å–∫–æ direction loss
                ["directions", "future_returns"] - direction + returns
                ["all"] - –≤—Å–µ losses
        """
        self.active_losses = active_losses
        print(f"üéØ –ê–∫—Ç–∏–≤–Ω—ã–µ losses —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω—ã: {active_losses}")
        
    def set_epoch(self, epoch: int):
        """–£—Å—Ç–∞–Ω–æ–≤–∫–∞ —Ç–µ–∫—É—â–µ–π —ç–ø–æ—Ö–∏ –¥–ª—è –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏—Ö –≤–µ—Å–æ–≤"""
        self.current_epoch = epoch
    
    def update_class_weights(self, targets: torch.Tensor):
        """
        –î–∏–Ω–∞–º–∏—á–µ—Å–∫–æ–µ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ –≤–µ—Å–æ–≤ –∫–ª–∞—Å—Å–æ–≤ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ç–µ–∫—É—â–µ–≥–æ –±–∞—Ç—á–∞
        
        Args:
            targets: –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏—è –∫–ª–∞—Å—Å–æ–≤ –∏–∑ —Ç–µ–∫—É—â–µ–≥–æ –±–∞—Ç—á–∞ (batch_size, 4)
        """
        if not self.use_dynamic_weights:
            return
        
        # –ü–æ–¥—Å—á–∏—Ç—ã–≤–∞–µ–º –∫–ª–∞—Å—Å—ã –≤ —Ç–µ–∫—É—â–µ–º –±–∞—Ç—á–µ
        batch_counts = torch.zeros(3, device=targets.device)
        for i in range(targets.shape[1]):  # –ü–æ –≤—Å–µ–º —Ç–∞–π–º—Ñ—Ä–µ–π–º–∞–º
            for c in range(3):  # –ü–æ –≤—Å–µ–º –∫–ª–∞—Å—Å–∞–º
                batch_counts[c] += (targets[:, i] == c).sum().float()
        
        # –û–±–Ω–æ–≤–ª—è–µ–º —Å–∫–æ–ª—å–∑—è—â–µ–µ —Å—Ä–µ–¥–Ω–µ–µ (–ø–µ—Ä–µ–º–µ—â–∞–µ–º –Ω–∞ –ø—Ä–∞–≤–∏–ª—å–Ω–æ–µ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ)
        self.running_class_counts = self.running_class_counts.to(targets.device)
        self.total_samples = self.total_samples.to(targets.device)
        
        self.running_class_counts = (self.class_weight_momentum * self.running_class_counts + 
                                     (1 - self.class_weight_momentum) * batch_counts)
        self.total_samples = self.total_samples + targets.numel()
        
        # –í—ã—á–∏—Å–ª—è–µ–º —Ç–µ–∫—É—â–∏–µ —á–∞—Å—Ç–æ—Ç—ã
        if self.total_samples > 100:  # –ù–∞—á–∏–Ω–∞–µ–º –∞–¥–∞–ø—Ç–∞—Ü–∏—é –ø–æ—Å–ª–µ 100 –ø—Ä–∏–º–µ—Ä–æ–≤
            current_frequencies = self.running_class_counts / self.running_class_counts.sum()
            current_frequencies = current_frequencies.clamp(min=0.01)  # –ò–∑–±–µ–≥–∞–µ–º –¥–µ–ª–µ–Ω–∏—è –Ω–∞ 0
            
            # –í—ã—á–∏—Å–ª—è–µ–º –Ω–æ–≤—ã–µ –≤–µ—Å–∞: inverse frequency —Å –∫–≤–∞–¥—Ä–∞—Ç–Ω—ã–º –∫–æ—Ä–Ω–µ–º
            new_weights = torch.sqrt(1.0 / current_frequencies)
            new_weights = new_weights / new_weights.mean()  # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è
            
            # –ü–ª–∞–≤–Ω–æ–µ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ –≤–µ—Å–æ–≤
            weight_update_rate = 0.1  # –°–∫–æ—Ä–æ—Å—Ç—å –∞–¥–∞–ø—Ç–∞—Ü–∏–∏
            self.class_weights = self.class_weights.to(targets.device)
            self.class_weights = (1 - weight_update_rate) * self.class_weights + weight_update_rate * new_weights
            
            # –ù–ï –æ–±–Ω–æ–≤–ª—è–µ–º cross_entropy_loss –∑–¥–µ—Å—å - –±—É–¥–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –≤–µ—Å–∞ –Ω–∞–ø—Ä—è–º—É—é –≤ focal_loss
        
    def get_dynamic_direction_weight(self) -> float:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–æ–≥–æ –≤–µ—Å–∞ –¥–ª—è direction loss —Å warmup"""
        base_weight = self.directions_weight
        
        if self.current_epoch < self.warmup_epochs:
            # –ü–æ—Å—Ç–µ–ø–µ–Ω–Ω–æ–µ —É–≤–µ–ª–∏—á–µ–Ω–∏–µ –æ—Ç 1.0 –¥–æ base_weight
            warmup_factor = self.current_epoch / self.warmup_epochs
            return 1.0 + (base_weight - 1.0) * warmup_factor
        else:
            return base_weight
        
    def apply_label_smoothing(self, targets: torch.Tensor, num_classes: int = 3) -> torch.Tensor:
        """
        –ü—Ä–∏–º–µ–Ω—è–µ—Ç label smoothing –∫ —Ü–µ–ª–µ–≤—ã–º –º–µ—Ç–∫–∞–º
        
        Args:
            targets: (batch_size,) - —Ü–µ–ª–µ–≤—ã–µ –º–µ—Ç–∫–∏ –∫–ª–∞—Å—Å–æ–≤
            num_classes: –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–ª–∞—Å—Å–æ–≤
            
        Returns:
            smoothed_targets: (batch_size, num_classes) - —Å–≥–ª–∞–∂–µ–Ω–Ω—ã–µ –º–µ—Ç–∫–∏
        """
        if self.label_smoothing == 0:
            # –ï—Å–ª–∏ label smoothing –≤—ã–∫–ª—é—á–µ–Ω, –≤–æ–∑–≤—Ä–∞—â–∞–µ–º one-hot
            return F.one_hot(targets, num_classes).float()
        
        # –°–æ–∑–¥–∞–µ–º —Å–≥–ª–∞–∂–µ–Ω–Ω–æ–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ
        confidence = 1.0 - self.label_smoothing
        smoothed_targets = torch.full(
            (targets.size(0), num_classes), 
            self.label_smoothing / (num_classes - 1),
            device=targets.device
        )
        
        # –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –æ—Å–Ω–æ–≤–Ω—É—é –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å –¥–ª—è –∏—Å—Ç–∏–Ω–Ω–æ–≥–æ –∫–ª–∞—Å—Å–∞
        smoothed_targets.scatter_(1, targets.unsqueeze(1), confidence)
        
        return smoothed_targets
    
    def focal_loss(self, logits: torch.Tensor, targets: torch.Tensor) -> torch.Tensor:
        """
        Focal Loss –¥–ª—è –±–æ—Ä—å–±—ã —Å –Ω–µ—Å–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã–º–∏ –∫–ª–∞—Å—Å–∞–º–∏ direction
        –° —É—á–µ—Ç–æ–º –≤–µ—Å–æ–≤ –∫–ª–∞—Å—Å–æ–≤ –∏ label smoothing
        """
        device = logits.device
        class_weights = self.class_weights.to(device)
        
        # –ü—Ä–∏–º–µ–Ω—è–µ–º label smoothing
        if self.label_smoothing > 0:
            # –ü–æ–ª—É—á–∞–µ–º —Å–≥–ª–∞–∂–µ–Ω–Ω—ã–µ –º–µ—Ç–∫–∏
            smoothed_targets = self.apply_label_smoothing(targets, num_classes=3)
            
            # –í—ã—á–∏—Å–ª—è–µ–º log probabilities
            log_probs = F.log_softmax(logits, dim=-1)
            
            # –í—ã—á–∏—Å–ª—è–µ–º cross entropy —Å —Å–≥–ª–∞–∂–µ–Ω–Ω—ã–º–∏ –º–µ—Ç–∫–∞–º–∏
            ce_loss = -(smoothed_targets * log_probs).sum(dim=-1)
            
            # –ü—Ä–∏–º–µ–Ω—è–µ–º –≤–µ—Å–∞ –∫–ª–∞—Å—Å–æ–≤
            target_weights = class_weights[targets]
            ce_loss = ce_loss * target_weights
        else:
            # –û–±—ã—á–Ω—ã–π weighted cross entropy - –∏—Å–ø–æ–ª—å–∑—É–µ–º –∞–∫—Ç—É–∞–ª—å–Ω—ã–µ –≤–µ—Å–∞
            ce_loss = F.cross_entropy(logits, targets, weight=self.class_weights.to(device), reduction='none')
        
        # Focal loss –º–æ–¥–∏—Ñ–∏–∫–∞—Ü–∏—è
        pt = torch.exp(-ce_loss)
        focal_loss = self.focal_alpha * (1 - pt) ** self.focal_gamma * ce_loss
        
        return focal_loss
        
    def forward(self, outputs: torch.Tensor, targets: torch.Tensor) -> torch.Tensor:
        """
        –í—ã—á–∏—Å–ª–µ–Ω–∏–µ multi-task loss —Å confidence-aware –º–µ—Ö–∞–Ω–∏–∑–º–æ–º
        
        Args:
            outputs: (batch_size, 20) - –≤—ã—Ö–æ–¥—ã –º–æ–¥–µ–ª–∏
            targets: (batch_size, 20) - —Ü–µ–ª–µ–≤—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è
            
        Returns:
            loss: —Å–∫–∞–ª—è—Ä
        """
        losses = []
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∞–∫—Ç–∏–≤–Ω—ã–µ losses
        use_all_losses = "all" in self.active_losses
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º confidence scores –µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–Ω—ã
        confidence_scores = None
        if hasattr(outputs, '_confidence_scores'):
            confidence_scores = outputs._confidence_scores  # (batch_size, 4)
        
        # 1. Future Returns Loss (–∏–Ω–¥–µ–∫—Å—ã 0-3) - MSE –¥–ª—è —Ä–µ–≥—Ä–µ—Å—Å–∏–∏
        if use_all_losses or "future_returns" in self.active_losses:
            future_returns_pred = outputs[:, 0:4]
            future_returns_target = targets[:, 0:4] / 100.0  # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –∏–∑ % –≤ –¥–æ–ª–∏
            
            # –í–∑–≤–µ—à–∏–≤–∞–Ω–∏–µ –¥–ª—è –∫—Ä—É–ø–Ω—ã—Ö –¥–≤–∏–∂–µ–Ω–∏–π
            abs_returns = torch.abs(future_returns_target)
            large_move_mask = abs_returns > self.min_movement_threshold
            
            mse_loss = self.mse_loss(future_returns_pred, future_returns_target)
            
            # –ü—Ä–∏–º–µ–Ω—è–µ–º –±–æ–ª—å—à–∏–π –≤–µ—Å –∫ –∫—Ä—É–ø–Ω—ã–º –¥–≤–∏–∂–µ–Ω–∏—è–º
            movement_weights = torch.ones_like(mse_loss)
            movement_weights[large_move_mask] = self.large_move_weight
            
            future_returns_loss = (mse_loss * movement_weights).mean()
            losses.append(future_returns_loss * self.future_returns_weight)
        
        # 2. Direction Loss (–∏–Ω–¥–µ–∫—Å—ã 4-7) - CrossEntropy –¥–ª—è 3-–∫–ª–∞—Å—Å–æ–≤–æ–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏
        if use_all_losses or "directions" in self.active_losses:
            if hasattr(outputs, '_direction_logits'):
                # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω—ã–µ –ª–æ–≥–∏—Ç—ã –∏–∑ –º–æ–¥–µ–ª–∏
                direction_logits = outputs._direction_logits  # (batch_size, 4, 3)
                direction_targets = targets[:, 4:8].long()  # (batch_size, 4)
            
                # –û–±–Ω–æ–≤–ª—è–µ–º –≤–µ—Å–∞ –∫–ª–∞—Å—Å–æ–≤ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ç–µ–∫—É—â–µ–≥–æ –±–∞—Ç—á–∞
                self.update_class_weights(direction_targets)
                
                direction_loss = 0
                for i in range(4):  # –î–ª—è –∫–∞–∂–¥–æ–≥–æ —Ç–∞–π–º—Ñ—Ä–µ–π–º–∞
                    # Focal Loss –¥–ª—è –ª—É—á—à–µ–π —Ä–∞–±–æ—Ç—ã —Å –Ω–µ—Å–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã–º–∏ –∫–ª–∞—Å—Å–∞–º–∏
                    focal_loss = self.focal_loss(direction_logits[:, i, :], direction_targets[:, i])
                    
                    # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–π —à—Ç—Ä–∞—Ñ –∑–∞ –ø—Ä–æ—Ç–∏–≤–æ–ø–æ–ª–æ–∂–Ω—ã–µ –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏—è
                    pred_classes = torch.argmax(direction_logits[:, i, :], dim=-1)
                    true_classes = direction_targets[:, i]
                    
                    wrong_direction_penalty = (
                        ((pred_classes == 0) & (true_classes == 1)) |  # UP vs DOWN
                        ((pred_classes == 1) & (true_classes == 0))    # DOWN vs UP
                    ).float() * self.wrong_direction_penalty  # –®—Ç—Ä–∞—Ñ –∏–∑ –∫–æ–Ω—Ñ–∏–≥–∞
                    
                    timeframe_loss = focal_loss + wrong_direction_penalty
                    
                    # Confidence-aware –≤–∑–≤–µ—à–∏–≤–∞–Ω–∏–µ: –±–æ–ª—å—à–µ –≤–µ—Å–∞ –ø—Ä–∏–º–µ—Ä–∞–º —Å –Ω–∏–∑–∫–æ–π —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å—é
                    if confidence_scores is not None:
                        # –ò–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: –Ω–∏–∑–∫–∞—è —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å = –±–æ–ª—å—à–∏–π –≤–µ—Å
                        confidence_weight = 2.0 - confidence_scores[:, i]  # –í–µ—Å –æ—Ç 1.0 –¥–æ 2.0
                        timeframe_loss = timeframe_loss * confidence_weight
                    
                    direction_loss += timeframe_loss.mean()
                
                # –≠–Ω—Ç—Ä–æ–ø–∏–π–Ω–∞—è —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—è –¥–ª—è –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–µ–Ω–∏—è —Å—Ö–ª–æ–ø—ã–≤–∞–Ω–∏—è –≤ –æ–¥–∏–Ω –∫–ª–∞—Å—Å
                entropy_weight = self.config.get('model', {}).get('entropy_weight', 0.1)
                if entropy_weight > 0:
                    # –í—ã—á–∏—Å–ª—è–µ–º —ç–Ω—Ç—Ä–æ–ø–∏—é –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π
                    probs = torch.softmax(direction_logits, dim=-1)  # (batch_size, 4, 3)
                    log_probs = torch.log(probs + 1e-8)  # –î–æ–±–∞–≤–ª—è–µ–º –º–∞–ª—É—é –∫–æ–Ω—Å—Ç–∞–Ω—Ç—É –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏
                    entropy = -torch.sum(probs * log_probs, dim=-1)  # (batch_size, 4)
                    
                    # –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è —ç–Ω—Ç—Ä–æ–ø–∏—è –¥–ª—è 3 –∫–ª–∞—Å—Å–æ–≤ = log(3) ‚âà 1.0986
                    max_entropy = np.log(3)
                    
                    # –®—Ç—Ä–∞—Ñ –∑–∞ –Ω–∏–∑–∫—É—é —ç–Ω—Ç—Ä–æ–ø–∏—é (–ø–æ–æ—â—Ä—è–µ–º —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π)
                    entropy_loss = (max_entropy - entropy).mean()
                    direction_loss += entropy_weight * entropy_loss
                
                direction_loss /= 4  # –£—Å—Ä–µ–¥–Ω—è–µ–º –ø–æ —Ç–∞–π–º—Ñ—Ä–µ–π–º–∞–º
                
                # –ò—Å–ø–æ–ª—å–∑—É–µ–º –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏–π –≤–µ—Å —Å warmup
                dynamic_weight = self.get_dynamic_direction_weight()
                losses.append(direction_loss * dynamic_weight)
            else:
                # Fallback –Ω–∞ –æ–±—ã—á–Ω—ã–π MSE –µ—Å–ª–∏ –ª–æ–≥–∏—Ç—ã –Ω–µ–¥–æ—Å—Ç—É–ø–Ω—ã
                direction_pred = outputs[:, 4:8]
                direction_target = targets[:, 4:8] / 2.0  # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º [0,1,2] -> [0,0.5,1]
                direction_loss = self.mse_loss(direction_pred, direction_target).mean()
                
                # –ò—Å–ø–æ–ª—å–∑—É–µ–º –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏–π –≤–µ—Å —Å warmup (–º–µ–Ω—å—à–∏–π –¥–ª—è MSE)
                dynamic_weight = self.get_dynamic_direction_weight()
                losses.append(direction_loss * dynamic_weight * 0.5)  # –ú–µ–Ω—å—à–∏–π –≤–µ—Å –¥–ª—è MSE
        
        # 3. Long Levels Loss (–∏–Ω–¥–µ–∫—Å—ã 8-11) - BCE –¥–ª—è –±–∏–Ω–∞—Ä–Ω–æ–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏
        if use_all_losses or "long_levels" in self.active_losses:
            long_levels_pred = outputs[:, 8:12]
            long_levels_target = targets[:, 8:12]
            long_levels_loss = self.bce_with_logits_loss(long_levels_pred, long_levels_target).mean()
            losses.append(long_levels_loss * self.long_levels_weight)
        
        # 4. Short Levels Loss (–∏–Ω–¥–µ–∫—Å—ã 12-15) - BCE –¥–ª—è –±–∏–Ω–∞—Ä–Ω–æ–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏
        if use_all_losses or "short_levels" in self.active_losses:
            short_levels_pred = outputs[:, 12:16]
            short_levels_target = targets[:, 12:16]
            short_levels_loss = self.bce_with_logits_loss(short_levels_pred, short_levels_target).mean()
            losses.append(short_levels_loss * self.short_levels_weight)
        
        # 5. Risk Metrics Loss (–∏–Ω–¥–µ–∫—Å—ã 16-19) - MSE –¥–ª—è —Ä–µ–≥—Ä–µ—Å—Å–∏–∏
        if use_all_losses or "risk_metrics" in self.active_losses:
            risk_metrics_pred = outputs[:, 16:20]
            risk_metrics_target = targets[:, 16:20] / 100.0  # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –µ—Å–ª–∏ –≤ –ø—Ä–æ—Ü–µ–Ω—Ç–∞—Ö
            risk_metrics_loss = self.mse_loss(risk_metrics_pred, risk_metrics_target).mean()
            losses.append(risk_metrics_loss * self.risk_metrics_weight)
        
        # 6. Confidence Loss - –æ–±—É—á–∞–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞—Ç—å –ø—Ä–∞–≤–∏–ª—å–Ω–æ—Å—Ç—å –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π
        if confidence_scores is not None and hasattr(outputs, '_direction_logits'):
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø—Ä–∞–≤–∏–ª—å–Ω–æ—Å—Ç—å –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π direction
            direction_logits = outputs._direction_logits
            pred_classes = torch.argmax(direction_logits, dim=-1)  # (batch_size, 4)
            true_classes = targets[:, 4:8].long()
            
            # –ü—Ä–∞–≤–∏–ª—å–Ω–æ—Å—Ç—å –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Ç–∞–π–º—Ñ—Ä–µ–π–º–∞
            correct_predictions = (pred_classes == true_classes).float()  # (batch_size, 4)
            
            # –î–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏ —Å autocast, –∏—Å–ø–æ–ª—å–∑—É–µ–º MSE loss –≤–º–µ—Å—Ç–æ BCE
            # confidence_scores –≤ –¥–∏–∞–ø–∞–∑–æ–Ω–µ [-1, 1] –±–ª–∞–≥–æ–¥–∞—Ä—è Tanh
            # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º correct_predictions –≤ —Ç–æ—Ç –∂–µ –¥–∏–∞–ø–∞–∑–æ–Ω: 0 -> -1, 1 -> 1
            confidence_targets = correct_predictions * 2 - 1  # –ò–∑ [0, 1] –≤ [-1, 1]
            
            # MSE loss –¥–ª—è –æ–±—É—á–µ–Ω–∏—è confidence –ø—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞—Ç—å –ø—Ä–∞–≤–∏–ª—å–Ω–æ—Å—Ç—å
            confidence_loss = F.mse_loss(
                confidence_scores, 
                confidence_targets,
                reduction='mean'
            )
            
            # –î–æ–±–∞–≤–ª—è–µ–º —Å –º–∞–ª–µ–Ω—å–∫–∏–º –≤–µ—Å–æ–º –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏
            losses.append(confidence_loss * 0.1)
        
        # –°—É–º–º–∏—Ä—É–µ–º –≤—Å–µ –ø–æ—Ç–µ—Ä–∏
        if len(losses) > 0:
            total_loss = sum(losses)
            
            # –î–æ–±–∞–≤–ª—è–µ–º L2 —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—é –¥–ª—è direction head –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
            if hasattr(self, 'model') and hasattr(self.model, 'get_direction_l2_loss'):
                l2_loss = self.model.get_direction_l2_loss()
                total_loss = total_loss + l2_loss
        else:
            # –ï—Å–ª–∏ –Ω–µ—Ç –∞–∫—Ç–∏–≤–Ω—ã—Ö losses, –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –Ω–µ–±–æ–ª—å—à–æ–π loss —á—Ç–æ–±—ã –∏–∑–±–µ–∂–∞—Ç—å –æ—à–∏–±–æ–∫
            total_loss = torch.tensor(0.0, device=outputs.device, requires_grad=True)
        
        return total_loss


def create_unified_model(config: Dict) -> UnifiedPatchTSTForTrading:
    """–°–æ–∑–¥–∞–Ω–∏–µ —É–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏"""
    return UnifiedPatchTSTForTrading(config)