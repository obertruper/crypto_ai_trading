#!/usr/bin/env python3
"""
–£–ø—Ä–æ—â–µ–Ω–Ω—ã–π –∞–Ω–∞–ª–∏–∑ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –∫–ª–∞—Å—Å–æ–≤ –≤ –¥–∞–Ω–Ω—ã—Ö
"""

import torch
import numpy as np
import pandas as pd
from collections import Counter
import matplotlib.pyplot as plt
from pathlib import Path
from datetime import datetime
import h5py

from utils.logger import get_logger

logger = get_logger("ClassDistributionAnalysis")

def analyze_h5_file(file_path, dataset_name="Dataset"):
    """
    –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–ª–∞—Å—Å–æ–≤ –≤ HDF5 —Ñ–∞–π–ª–µ
    """
    logger.info(f"\n{'='*60}")
    logger.info(f"üìä –ê–Ω–∞–ª–∏–∑ {dataset_name}: {file_path}")
    logger.info(f"{'='*60}")
    
    if not Path(file_path).exists():
        logger.error(f"–§–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω: {file_path}")
        return None
    
    with h5py.File(file_path, 'r') as f:
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—Ç—Ä—É–∫—Ç—É—Ä—É
        logger.info(f"–ö–ª—é—á–∏ –≤ —Ñ–∞–π–ª–µ: {list(f.keys())}")
        
        if 'y' not in f:
            logger.error("–ö–ª—é—á 'y' –Ω–µ –Ω–∞–π–¥–µ–Ω –≤ —Ñ–∞–π–ª–µ")
            return None
        
        targets = f['y'][:]
        logger.info(f"–†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å targets: {targets.shape}")
        
        # –ï—Å–ª–∏ targets –∏–º–µ—é—Ç —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å (N, 1, 20), —É–±–∏—Ä–∞–µ–º —Å—Ä–µ–¥–Ω—é—é —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å
        if targets.ndim == 3 and targets.shape[1] == 1:
            targets = targets.squeeze(1)
        
        total_samples = targets.shape[0]
        logger.info(f"–í—Å–µ–≥–æ –ø—Ä–∏–º–µ—Ä–æ–≤: {total_samples}")
        
        # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º direction –∫–ª–∞—Å—Å—ã (–∏–Ω–¥–µ–∫—Å—ã 4-7)
        results = {}
        
        for i, timeframe in enumerate(['15m', '1h', '4h', '12h']):
            directions = targets[:, 4 + i].astype(int)
            
            # –ü–æ–¥—Å—á–µ—Ç –∫–ª–∞—Å—Å–æ–≤
            class_counts = Counter(directions)
            
            logger.info(f"\nüìà –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–ª–∞—Å—Å–æ–≤ –¥–ª—è {timeframe}:")
            logger.info(f"   LONG (0):  {class_counts.get(0, 0):6d} ({class_counts.get(0, 0)/total_samples*100:5.1f}%)")
            logger.info(f"   SHORT (1): {class_counts.get(1, 0):6d} ({class_counts.get(1, 0)/total_samples*100:5.1f}%)")
            logger.info(f"   FLAT (2):  {class_counts.get(2, 0):6d} ({class_counts.get(2, 0)/total_samples*100:5.1f}%)")
            
            # –†–∞—Å—á–µ—Ç –≤–µ—Å–æ–≤ –∫–ª–∞—Å—Å–æ–≤
            class_weights = {}
            for cls in [0, 1, 2]:
                if class_counts.get(cls, 0) > 0:
                    weight = np.sqrt(total_samples / (3.0 * class_counts.get(cls, 1)))
                    class_weights[cls] = weight
                else:
                    class_weights[cls] = 1.0
            
            # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –≤–µ—Å–æ–≤
            mean_weight = np.mean(list(class_weights.values()))
            normalized_weights = {k: v/mean_weight for k, v in class_weights.items()}
            
            logger.info(f"   –†–µ–∫–æ–º–µ–Ω–¥—É–µ–º—ã–µ –≤–µ—Å–∞:")
            logger.info(f"   LONG:  {normalized_weights[0]:.3f}")
            logger.info(f"   SHORT: {normalized_weights[1]:.3f}")
            logger.info(f"   FLAT:  {normalized_weights[2]:.3f}")
            
            # –ê–Ω–∞–ª–∏–∑ returns
            returns = targets[:, i]
            logger.info(f"\n   –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ returns:")
            logger.info(f"   Mean: {np.mean(returns):.4f}%")
            logger.info(f"   Std:  {np.std(returns):.4f}%")
            
            results[timeframe] = {
                'class_distribution': dict(class_counts),
                'class_weights': normalized_weights,
                'total_samples': total_samples
            }
    
    return results

def create_summary_plot(all_results, save_path="experiments/class_distribution"):
    """
    –°–æ–∑–¥–∞–µ—Ç —Å–≤–æ–¥–Ω—ã–π –≥—Ä–∞—Ñ–∏–∫ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –∫–ª–∞—Å—Å–æ–≤
    """
    Path(save_path).mkdir(parents=True, exist_ok=True)
    
    # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –≥—Ä–∞—Ñ–∏–∫–∞
    datasets = list(all_results.keys())
    timeframes = ['15m', '1h', '4h', '12h']
    
    # –°–æ–∑–¥–∞–µ–º –≥—Ä–∞—Ñ–∏–∫
    fig, axes = plt.subplots(len(datasets), len(timeframes), figsize=(16, 4*len(datasets)))
    if len(datasets) == 1:
        axes = axes.reshape(1, -1)
    
    for ds_idx, dataset in enumerate(datasets):
        for tf_idx, timeframe in enumerate(timeframes):
            ax = axes[ds_idx, tf_idx]
            
            if all_results[dataset] and timeframe in all_results[dataset]:
                dist = all_results[dataset][timeframe]['class_distribution']
                classes = ['LONG', 'SHORT', 'FLAT']
                counts = [dist.get(0, 0), dist.get(1, 0), dist.get(2, 0)]
                total = sum(counts)
                
                # –ë–∞—Ä–ø–ª–æ—Ç
                bars = ax.bar(classes, counts, color=['green', 'red', 'gray'])
                
                # –î–æ–±–∞–≤–ª—è–µ–º –ø—Ä–æ—Ü–µ–Ω—Ç—ã
                for bar, count in zip(bars, counts):
                    height = bar.get_height()
                    if total > 0:
                        ax.annotate(f'{count/total*100:.1f}%',
                                   xy=(bar.get_x() + bar.get_width() / 2, height),
                                   xytext=(0, 3),
                                   textcoords="offset points",
                                   ha='center', va='bottom')
                
                ax.set_title(f'{dataset} - {timeframe}')
                ax.set_ylabel('Count')
    
    plt.tight_layout()
    plt.savefig(f"{save_path}/class_distribution_summary.png", dpi=300)
    plt.close()
    
    logger.info(f"‚úÖ –ì—Ä–∞—Ñ–∏–∫ —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {save_path}/class_distribution_summary.png")

def generate_final_recommendations(all_results):
    """
    –ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç —Ñ–∏–Ω–∞–ª—å–Ω—ã–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏
    """
    logger.info(f"\n{'='*60}")
    logger.info("üéØ –§–ò–ù–ê–õ–¨–ù–´–ï –†–ï–ö–û–ú–ï–ù–î–ê–¶–ò–ò")
    logger.info(f"{'='*60}")
    
    # –°–æ–±–∏—Ä–∞–µ–º –æ–±—â—É—é —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
    all_weights = []
    all_distributions = []
    
    for dataset, results in all_results.items():
        if results:
            for tf, data in results.items():
                dist = data['class_distribution']
                total = sum(dist.values())
                if total > 0:
                    all_distributions.append({
                        'LONG': dist.get(0, 0) / total,
                        'SHORT': dist.get(1, 0) / total,
                        'FLAT': dist.get(2, 0) / total
                    })
                    all_weights.append(data['class_weights'])
    
    # –°—Ä–µ–¥–Ω–∏–µ –∑–Ω–∞—á–µ–Ω–∏—è
    if all_distributions:
        avg_long = np.mean([d['LONG'] for d in all_distributions])
        avg_short = np.mean([d['SHORT'] for d in all_distributions])
        avg_flat = np.mean([d['FLAT'] for d in all_distributions])
        
        logger.info(f"\nüìä –û–±—â–∏–π –¥–∏—Å–±–∞–ª–∞–Ω—Å –∫–ª–∞—Å—Å–æ–≤:")
        logger.info(f"   LONG:  {avg_long*100:.1f}%")
        logger.info(f"   SHORT: {avg_short*100:.1f}%")
        logger.info(f"   FLAT:  {avg_flat*100:.1f}%")
        
        # –°—Ä–µ–¥–Ω–∏–µ –≤–µ—Å–∞
        if all_weights:
            avg_weights = {
                0: np.mean([w[0] for w in all_weights]),
                1: np.mean([w[1] for w in all_weights]),
                2: np.mean([w[2] for w in all_weights])
            }
            
            logger.info(f"\nüìù –†–µ–∫–æ–º–µ–Ω–¥—É–µ–º—ã–µ –≤–µ—Å–∞ –∫–ª–∞—Å—Å–æ–≤ –¥–ª—è loss —Ñ—É–Ω–∫—Ü–∏–∏:")
            logger.info(f"   class_weights = [{avg_weights[0]:.3f}, {avg_weights[1]:.3f}, {avg_weights[2]:.3f}]")
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ —Ñ–∞–π–ª
            save_path = Path("experiments/class_distribution")
            save_path.mkdir(parents=True, exist_ok=True)
            
            with open(save_path / "recommended_weights.txt", "w") as f:
                f.write(f"–ê–Ω–∞–ª–∏–∑ –æ—Ç {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")
                f.write(f"–î–∏—Å–±–∞–ª–∞–Ω—Å –∫–ª–∞—Å—Å–æ–≤:\n")
                f.write(f"LONG:  {avg_long*100:.1f}%\n")
                f.write(f"SHORT: {avg_short*100:.1f}%\n")
                f.write(f"FLAT:  {avg_flat*100:.1f}%\n\n")
                f.write(f"–†–µ–∫–æ–º–µ–Ω–¥—É–µ–º—ã–µ –≤–µ—Å–∞:\n")
                f.write(f"class_weights = [{avg_weights[0]:.3f}, {avg_weights[1]:.3f}, {avg_weights[2]:.3f}]\n")
            
            logger.info(f"\n‚úÖ –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ {save_path}/recommended_weights.txt")

def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –∞–Ω–∞–ª–∏–∑–∞"""
    logger.info("üöÄ –ó–∞–ø—É—Å–∫ –∞–Ω–∞–ª–∏–∑–∞ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –∫–ª–∞—Å—Å–æ–≤...")
    
    # –ü—É—Ç–∏ –∫ –ø—Ä–µ–¥–≤—ã—á–∏—Å–ª–µ–Ω–Ω—ã–º –¥–∞–Ω–Ω—ã–º
    cache_dir = Path("cache/precomputed")
    
    # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –¥–æ—Å—Ç—É–ø–Ω—ã–µ —Ñ–∞–π–ª—ã
    h5_files = {
        'train': cache_dir / "train_w168_s1.h5",
        'val': cache_dir / "val_w168_s4.h5",
        'test': cache_dir / "test_w168_s4.h5"
    }
    
    all_results = {}
    
    for dataset_name, file_path in h5_files.items():
        if file_path.exists():
            results = analyze_h5_file(file_path, dataset_name.upper())
            all_results[dataset_name.upper()] = results
        else:
            logger.warning(f"‚ö†Ô∏è –§–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω: {file_path}")
    
    if all_results:
        # –°–æ–∑–¥–∞–µ–º –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏
        create_summary_plot(all_results)
        
        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏
        generate_final_recommendations(all_results)
    else:
        logger.error("‚ùå –ù–µ –Ω–∞–π–¥–µ–Ω–æ –Ω–∏ –æ–¥–Ω–æ–≥–æ —Ñ–∞–π–ª–∞ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞!")
        logger.info("üí° –°–Ω–∞—á–∞–ª–∞ –∑–∞–ø—É—Å—Ç–∏—Ç–µ –ø–æ–¥–≥–æ—Ç–æ–≤–∫—É –¥–∞–Ω–Ω—ã—Ö: python main.py --mode data")

if __name__ == "__main__":
    main()