#!/usr/bin/env python3
"""
–ö–æ–º–ø–ª–µ–∫—Å–Ω–∞—è –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞ –ø—Ä–æ–±–ª–µ–º –æ–±—É—á–µ–Ω–∏—è
"""

import torch
import numpy as np
import pandas as pd
import pickle
from pathlib import Path
import matplotlib.pyplot as plt
import seaborn as sns
from rich.console import Console
from rich.table import Table
from rich.panel import Panel

console = Console()

class TrainingDiagnostics:
    def __init__(self):
        self.issues = []
        self.recommendations = []
        
    def analyze_checkpoint(self, checkpoint_path="models_saved/best_model_20250701_120952.pth"):
        """–î–µ—Ç–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ checkpoint"""
        console.print("\n[bold cyan]üîç –ê–Ω–∞–ª–∏–∑ checkpoint –º–æ–¥–µ–ª–∏...[/bold cyan]")
        
        if not Path(checkpoint_path).exists():
            self.issues.append("Checkpoint –Ω–µ –Ω–∞–π–¥–µ–Ω")
            return
            
        ckpt = torch.load(checkpoint_path, map_location='cpu')
        
        # –ê–Ω–∞–ª–∏–∑ –≤–µ—Å–æ–≤
        model_state = ckpt['model_state_dict']
        dead_weights = []
        gradient_issues = []
        
        for name, param in model_state.items():
            if 'weight' in name and param.dim() > 1:
                std = param.std().item()
                mean = param.mean().item()
                
                # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –º–µ—Ä—Ç–≤—ã–µ –≤–µ—Å–∞
                if std < 0.001:
                    dead_weights.append(name)
                    
                # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –≤–∑—Ä—ã–≤–∞—é—â–∏–µ/–∏—Å—á–µ–∑–∞—é—â–∏–µ –≥—Ä–∞–¥–∏–µ–Ω—Ç—ã
                if std > 10 or std < 0.0001:
                    gradient_issues.append((name, std))
        
        if dead_weights:
            self.issues.append(f"–ù–∞–π–¥–µ–Ω–æ {len(dead_weights)} —Å–ª–æ–µ–≤ —Å –º–µ—Ä—Ç–≤—ã–º–∏ –≤–µ—Å–∞–º–∏")
            
        if gradient_issues:
            self.issues.append(f"–ü—Ä–æ–±–ª–µ–º—ã —Å –≥—Ä–∞–¥–∏–µ–Ω—Ç–∞–º–∏ –≤ {len(gradient_issues)} —Å–ª–æ—è—Ö")
            
        # –ê–Ω–∞–ª–∏–∑ learning rate
        if 'optimizer_state_dict' in ckpt:
            lr = ckpt['optimizer_state_dict']['param_groups'][0]['lr']
            if lr < 0.0001:
                self.issues.append(f"–°–ª–∏—à–∫–æ–º –Ω–∏–∑–∫–∏–π learning rate: {lr}")
                self.recommendations.append("–£–≤–µ–ª–∏—á–∏—Ç—å learning rate –¥–æ 0.001-0.003")
                
        console.print(f"‚úÖ Val loss: {ckpt.get('best_val_loss', 'N/A')}")
        console.print(f"‚úÖ –≠–ø–æ—Ö–∞: {ckpt.get('epoch', 'N/A')}")
        
    def analyze_data_distribution(self):
        """–ê–Ω–∞–ª–∏–∑ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö"""
        console.print("\n[bold cyan]üìä –ê–Ω–∞–ª–∏–∑ –¥–∞–Ω–Ω—ã—Ö...[/bold cyan]")
        
        train_path = Path("data/processed/train_data.pkl")
        if not train_path.exists():
            self.issues.append("–î–∞–Ω–Ω—ã–µ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –Ω–µ –Ω–∞–π–¥–µ–Ω—ã")
            return
            
        with open(train_path, 'rb') as f:
            train_data = pickle.load(f)
            
        # –ê–Ω–∞–ª–∏–∑ —Ü–µ–ª–µ–≤—ã—Ö –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö
        target_cols = [col for col in train_data.columns if any(x in col for x in ['tp', 'sl', 'hit', 'reached', 'direction'])]
        
        imbalanced_targets = []
        for col in target_cols:
            if col in train_data.columns:
                value_counts = train_data[col].value_counts(normalize=True)
                # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –¥–∏—Å–±–∞–ª–∞–Ω—Å –∫–ª–∞—Å—Å–æ–≤
                if len(value_counts) == 2:  # –ë–∏–Ω–∞—Ä–Ω–∞—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è
                    min_class = value_counts.min()
                    if min_class < 0.1:  # –ú–µ–Ω–µ–µ 10%
                        imbalanced_targets.append((col, min_class))
                        
        if imbalanced_targets:
            self.issues.append(f"–î–∏—Å–±–∞–ª–∞–Ω—Å –∫–ª–∞—Å—Å–æ–≤ –≤ {len(imbalanced_targets)} —Ü–µ–ª–µ–≤—ã—Ö –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö")
            self.recommendations.append("–ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å weighted loss –∏–ª–∏ SMOTE –¥–ª—è –±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∏")
            
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ NaN –∏ –≤—ã–±—Ä–æ—Å—ã
        nan_cols = train_data.columns[train_data.isna().any()].tolist()
        if nan_cols:
            self.issues.append(f"NaN –∑–Ω–∞—á–µ–Ω–∏—è –≤ {len(nan_cols)} –∫–æ–ª–æ–Ω–∫–∞—Ö")
            
        console.print(f"‚úÖ –†–∞–∑–º–µ—Ä –¥–∞–Ω–Ω—ã—Ö: {train_data.shape}")
        console.print(f"‚úÖ –¶–µ–ª–µ–≤—ã—Ö –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö: {len(target_cols)}")
        
    def analyze_training_dynamics(self):
        """–ê–Ω–∞–ª–∏–∑ –¥–∏–Ω–∞–º–∏–∫–∏ –æ–±—É—á–µ–Ω–∏—è"""
        console.print("\n[bold cyan]üìà –ê–Ω–∞–ª–∏–∑ –¥–∏–Ω–∞–º–∏–∫–∏ –æ–±—É—á–µ–Ω–∏—è...[/bold cyan]")
        
        # –ò–∑ –ª–æ–≥–æ–≤ –≤–∏–¥–Ω–æ:
        # –≠–ø–æ—Ö–∞ 1: train_loss=0.3814, val_loss=0.3308
        # –≠–ø–æ—Ö–∞ 2: train_loss=0.3754, val_loss=0.3308
        # –≠–ø–æ—Ö–∞ 3: train_loss=0.3754, val_loss=0.3308
        
        train_losses = [0.3814, 0.3754, 0.3754]
        val_losses = [0.3308, 0.3308, 0.3308]
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –∑–∞—Å—Ç—Ä–µ–≤–∞–Ω–∏–µ
        if len(set(val_losses)) == 1:
            self.issues.append("Val loss –Ω–µ –º–µ–Ω—è–µ—Ç—Å—è - –º–æ–¥–µ–ª—å –∑–∞—Å—Ç—Ä—è–ª–∞")
            self.recommendations.append("–ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å OneCycleLR –∏–ª–∏ CosineAnnealingLR scheduler")
            
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –Ω–µ–¥–æ–æ–±—É—á–µ–Ω–∏–µ
        if all(tl > vl for tl, vl in zip(train_losses, val_losses)):
            self.issues.append("Train loss –≤—ã—à–µ val loss - –≤–æ–∑–º–æ–∂–Ω–æ –Ω–µ–¥–æ–æ–±—É—á–µ–Ω–∏–µ")
            self.recommendations.append("–£–≤–µ–ª–∏—á–∏—Ç—å capacity –º–æ–¥–µ–ª–∏ –∏–ª–∏ —É–º–µ–Ω—å—à–∏—Ç—å regularization")
            
    def generate_report(self):
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç—á–µ—Ç–∞ —Å —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è–º–∏"""
        console.print("\n[bold red]‚ùå –û–ë–ù–ê–†–£–ñ–ï–ù–ù–´–ï –ü–†–û–ë–õ–ï–ú–´:[/bold red]")
        for i, issue in enumerate(self.issues, 1):
            console.print(f"{i}. {issue}")
            
        console.print("\n[bold green]üí° –†–ï–ö–û–ú–ï–ù–î–ê–¶–ò–ò:[/bold green]")
        for i, rec in enumerate(self.recommendations, 1):
            console.print(f"{i}. {rec}")
            
        # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏
        console.print("\n[bold yellow]üöÄ –ö–û–ú–ü–õ–ï–ö–°–ù–û–ï –†–ï–®–ï–ù–ò–ï:[/bold yellow]")
        console.print("1. –û—Å—Ç–∞–Ω–æ–≤–∏—Ç—å —Ç–µ–∫—É—â–µ–µ –æ–±—É—á–µ–Ω–∏–µ (Ctrl+C)")
        console.print("2. –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –∫–æ–Ω—Ñ–∏–≥ (–±—É–¥–µ—Ç —Å–æ–∑–¥–∞–Ω)")
        console.print("3. –ü–µ—Ä–µ–∑–∞–ø—É—Å—Ç–∏—Ç—å —Å –Ω–æ–≤—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏")
        console.print("4. –ú–æ–Ω–∏—Ç–æ—Ä–∏—Ç—å –≥—Ä–∞–¥–∏–µ–Ω—Ç—ã –∏ loss dynamics")
        
        return self.issues, self.recommendations

if __name__ == "__main__":
    console.print("[bold magenta]üîß –î–ò–ê–ì–ù–û–°–¢–ò–ö–ê –ü–†–û–ë–õ–ï–ú –û–ë–£–ß–ï–ù–ò–Ø[/bold magenta]")
    
    diagnostics = TrainingDiagnostics()
    diagnostics.analyze_checkpoint()
    diagnostics.analyze_data_distribution()
    diagnostics.analyze_training_dynamics()
    
    issues, recommendations = diagnostics.generate_report()
    
    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –æ—Ç—á–µ—Ç–∞
    report = {
        'issues': issues,
        'recommendations': recommendations,
        'timestamp': pd.Timestamp.now()
    }
    
    with open('training_diagnostics_report.pkl', 'wb') as f:
        pickle.dump(report, f)
        
    console.print("\n‚úÖ –û—Ç—á–µ—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤ training_diagnostics_report.pkl")