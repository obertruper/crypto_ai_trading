#!/usr/bin/env python3
"""
–ü–æ–ª–Ω–æ—Ü–µ–Ω–Ω—ã–π –∑–∞–ø—É—Å–∫ Crypto AI Trading System
"""

import sys
import os
import argparse
import yaml
from pathlib import Path
from datetime import datetime, timedelta
import warnings
warnings.filterwarnings('ignore')

# –î–æ–±–∞–≤–ª—è–µ–º —Ç–µ–∫—É—â—É—é –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –≤ –ø—É—Ç—å
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

# –ò–º–ø–æ—Ä—Ç—ã –ø—Ä–æ–µ–∫—Ç–∞
from utils.logger import get_logger, setup_logging
from data.data_loader import CryptoDataLoader
from data.feature_engineering import FeatureEngineer
from data.dataset import TradingDataset
from models.patchtst import PatchTSTForPrediction
from models.ensemble import BaseEnsemble
from training.trainer import Trainer
from training.validator import ModelValidator
from trading.risk_manager import RiskManager
from trading.signals import SignalGenerator
from trading.backtester import Backtester
from utils.visualization import TradingVisualizer
from torch.utils.data import DataLoader

def load_config(config_path: str = "config/config.yaml") -> dict:
    """–ó–∞–≥—Ä—É–∑–∫–∞ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏"""
    with open(config_path, 'r', encoding='utf-8') as f:
        config = yaml.safe_load(f)
    return config

def run_data_pipeline(config: dict, logger):
    """–≠—Ç–∞–ø 1: –ó–∞–≥—Ä—É–∑–∫–∞ –∏ –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö"""
    logger.info("="*80)
    logger.info("üìä –≠–¢–ê–ü 1: –ó–ê–ì–†–£–ó–ö–ê –ò –ü–û–î–ì–û–¢–û–í–ö–ê –î–ê–ù–ù–´–•")
    logger.info("="*80)
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ñ–ª–∞–≥ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –∫—ç—à–∞
    use_cache_only = os.environ.get('USE_CACHE_ONLY', '0')
    logger.info(f"üîç USE_CACHE_ONLY = '{use_cache_only}'")
    
    if use_cache_only == '1':
        logger.info("üì¶ –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —Ä–µ–∂–∏–º —Ä–∞–±–æ—Ç—ã —Å –∫—ç—à–µ–º (–±–µ–∑ –ë–î)")
        cache_path = Path("cache/features_cache.pkl")
        
        if not cache_path.exists():
            logger.error(f"‚ùå –§–∞–π–ª –∫—ç—à–∞ –Ω–µ –Ω–∞–π–¥–µ–Ω: {cache_path}")
            logger.error(f"–¢–µ–∫—É—â–∞—è –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è: {os.getcwd()}")
            logger.error(f"–°–æ–¥–µ—Ä–∂–∏–º–æ–µ cache/: {list(Path('cache').glob('*')) if Path('cache').exists() else '–î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç'}")
            raise FileNotFoundError(f"–§–∞–π–ª –∫—ç—à–∞ –Ω–µ –Ω–∞–π–¥–µ–Ω: {cache_path}")
        
        logger.info(f"‚úÖ –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –∏–∑ –∫—ç—à–∞: {cache_path}")
        logger.info(f"üìè –†–∞–∑–º–µ—Ä —Ñ–∞–π–ª–∞: {cache_path.stat().st_size / (1024*1024):.1f} MB")
        
        import pickle
        with open(cache_path, 'rb') as f:
            features_df = pickle.load(f)
        
        logger.info(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(features_df):,} –∑–∞–ø–∏—Å–µ–π –∏–∑ –∫—ç—à–∞")
        logger.info(f"üìä –§–æ—Ä–º–∞ –¥–∞–Ω–Ω—ã—Ö: {features_df.shape}")
        logger.info(f"üìÖ –ü–µ—Ä–∏–æ–¥: {features_df['datetime'].min()} - {features_df['datetime'].max()}")
        logger.info(f"üè∑Ô∏è –°–∏–º–≤–æ–ª—ã: {sorted(features_df['symbol'].unique())}")
        
        return features_df
    
    # –û–±—ã—á–Ω—ã–π —Ä–µ–∂–∏–º —Ä–∞–±–æ—Ç—ã —Å –ë–î
    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∑–∞–≥—Ä—É–∑—á–∏–∫–∞ –¥–∞–Ω–Ω—ã—Ö
    data_loader = CryptoDataLoader(config)
    
    # –ü–æ–ª—É—á–µ–Ω–∏–µ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö —Å–∏–º–≤–æ–ª–æ–≤
    available_symbols = data_loader.get_available_symbols()
    logger.info(f"–ù–∞–π–¥–µ–Ω–æ {len(available_symbols)} —Å–∏–º–≤–æ–ª–æ–≤ –≤ –ë–î")
    
    # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Å–∏–º–≤–æ–ª—ã –∏–∑ –∫–æ–Ω—Ñ–∏–≥–∞ –∏–ª–∏ –≤—Å–µ –¥–æ—Å—Ç—É–ø–Ω—ã–µ
    symbols = config['data']['symbols']
    if symbols == 'all' or symbols == 'ALL' or (isinstance(symbols, list) and 'ALL' in symbols):
        symbols = available_symbols
    else:
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ —Å–∏–º–≤–æ–ª—ã –µ—Å—Ç—å –≤ –ë–î
        symbols = [s for s in symbols if s in available_symbols]
    
    logger.info(f"–ë—É–¥–µ—Ç –∑–∞–≥—Ä—É–∂–µ–Ω–æ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è {len(symbols)} —Å–∏–º–≤–æ–ª–æ–≤: {symbols[:5]}...")
    
    # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–µ—Ä–∏–æ–¥–∞ –¥–∞–Ω–Ω—ã—Ö - –∏—Å–ø–æ–ª—å–∑—É–µ–º –≤—Å–µ –¥–æ—Å—Ç—É–ø–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
    end_date = datetime.now()
    start_date = datetime(2022, 6, 8)  # –ù–∞—á–∞–ª–æ –¥–∞–Ω–Ω—ã—Ö –≤ –ë–î
    
    # –ó–∞–≥—Ä—É–∑–∫–∞ —Å—ã—Ä—ã—Ö –¥–∞–Ω–Ω—ã—Ö
    raw_data = data_loader.load_data(
        symbols=symbols,
        start_date=start_date.strftime('%Y-%m-%d'),
        end_date=end_date.strftime('%Y-%m-%d')
    )
    
    logger.info(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(raw_data):,} –∑–∞–ø–∏—Å–µ–π")
    
    # –°–æ–∑–¥–∞–Ω–∏–µ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏—Ö –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–æ–≤
    logger.info("\nüîß –°–æ–∑–¥–∞–Ω–∏–µ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏—Ö –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–æ–≤...")
    feature_engineer = FeatureEngineer(config)
    
    # –û–±—Ä–∞–±–æ—Ç–∫–∞ –ø–æ —Å–∏–º–≤–æ–ª–∞–º –¥–ª—è –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ–≥–æ —Ä–∞—Å—á–µ—Ç–∞ –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–æ–≤
    processed_data = []
    for symbol in symbols:
        symbol_data = raw_data[raw_data['symbol'] == symbol].copy()
        if len(symbol_data) < 100:
            logger.warning(f"–ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è {symbol} ({len(symbol_data)} –∑–∞–ø–∏—Å–µ–π), –ø—Ä–æ–ø—É—Å–∫–∞–µ–º")
            continue
            
        features = feature_engineer.create_features(symbol_data)
        processed_data.append(features)
        logger.info(f"  {symbol}: {len(features)} –∑–∞–ø–∏—Å–µ–π, {features.shape[1]} –ø—Ä–∏–∑–Ω–∞–∫–æ–≤")
    
    # –û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ –≤—Å–µ—Ö –¥–∞–Ω–Ω—ã—Ö
    all_features = pd.concat(processed_data, ignore_index=True)
    logger.info(f"\n‚úÖ –°–æ–∑–¥–∞–Ω–æ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤: {all_features.shape}")
    
    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –ø—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    cache_path = Path("cache/features_cache.pkl")
    cache_path.parent.mkdir(exist_ok=True)
    all_features.to_pickle(cache_path)
    logger.info(f"üíæ –ü—Ä–∏–∑–Ω–∞–∫–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ {cache_path}")
    
    return all_features

def run_training_pipeline(config: dict, features_data, logger):
    """–≠—Ç–∞–ø 2: –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π"""
    logger.info("\n" + "="*80)
    logger.info("üß† –≠–¢–ê–ü 2: –û–ë–£–ß–ï–ù–ò–ï –ú–û–î–ï–õ–ï–ô")
    logger.info("="*80)
    
    # –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö
    train_ratio = config['data']['train_ratio']
    val_ratio = config['data']['val_ratio']
    
    n_samples = len(features_data)
    train_size = int(n_samples * train_ratio)
    val_size = int(n_samples * val_ratio)
    
    # –°–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ –ø–æ –≤—Ä–µ–º–µ–Ω–∏
    features_data = features_data.sort_values(['symbol', 'datetime'])
    
    train_data = features_data.iloc[:train_size]
    val_data = features_data.iloc[train_size:train_size+val_size]
    test_data = features_data.iloc[train_size+val_size:]
    
    logger.info(f"–†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö:")
    logger.info(f"  Train: {len(train_data):,} –∑–∞–ø–∏—Å–µ–π ({train_ratio*100:.0f}%)")
    logger.info(f"  Val: {len(val_data):,} –∑–∞–ø–∏—Å–µ–π ({val_ratio*100:.0f}%)")
    logger.info(f"  Test: {len(test_data):,} –∑–∞–ø–∏—Å–µ–π ({(1-train_ratio-val_ratio)*100:.0f}%)")
    
    # –°–æ–∑–¥–∞–Ω–∏–µ –¥–∞—Ç–∞—Å–µ—Ç–æ–≤
    train_dataset = TradingDataset(
        data=train_data,
        context_window=config['model']['context_window'],
        prediction_window=config['model']['pred_len']
    )
    val_dataset = TradingDataset(
        data=val_data,
        context_window=config['model']['context_window'],
        prediction_window=config['model']['pred_len']
    )
    test_dataset = TradingDataset(
        data=test_data,
        context_window=config['model']['context_window'],
        prediction_window=config['model']['pred_len']
    )
    
    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–æ–¥–µ–ª–∏
    logger.info("\nüèóÔ∏è –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–æ–¥–µ–ª–∏ PatchTST...")
    # –ü–æ–ª—É—á–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –∏ —Ü–µ–ª–µ–≤—ã—Ö –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –∏–∑ –¥–∞–Ω–Ω—ã—Ö
    n_features = len(train_dataset.feature_cols)
    n_targets = len(train_dataset.target_cols)
    
    logger.info(f"–í—Ö–æ–¥–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏: {n_features}, –¶–µ–ª–µ–≤—ã–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ: {n_targets}")
    
    model = PatchTSTForPrediction(
        c_in=n_features,
        c_out=n_targets,
        context_window=config['model']['context_window'],
        target_window=config['model']['pred_len'],
        patch_len=config['model']['patch_len'],
        stride=config['model']['stride'],
        n_layers=config['model']['e_layers'],
        d_model=config['model']['d_model'],
        n_heads=config['model']['n_heads'],
        d_ff=config['model']['d_ff'],
        dropout=config['model']['dropout']
    )
    
    # –°–æ–∑–¥–∞–Ω–∏–µ —Ç—Ä–µ–π–Ω–µ—Ä–∞
    trainer = Trainer(model, config)
    
    # –°–æ–∑–¥–∞–Ω–∏–µ DataLoader'–æ–≤
    train_loader = DataLoader(
        train_dataset,
        batch_size=config['model']['batch_size'],
        shuffle=True,
        num_workers=config['performance'].get('num_workers', 4),
        pin_memory=True
    )
    
    val_loader = DataLoader(
        val_dataset,
        batch_size=config['model']['batch_size'],
        shuffle=False,
        num_workers=config['performance'].get('num_workers', 4),
        pin_memory=True
    )
    
    # –û–±—É—á–µ–Ω–∏–µ
    logger.info("\nüöÄ –ù–∞—á–∞–ª–æ –æ–±—É—á–µ–Ω–∏—è...")
    training_results = trainer.train(
        train_loader=train_loader,
        val_loader=val_loader
    )
    
    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –ª—É—á—à–µ–π –º–æ–¥–µ–ª–∏
    best_model_path = trainer.checkpoint_dir / "best_model.pth"
    
    logger.info(f"\n‚úÖ –û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ. –õ—É—á—à–∞—è –º–æ–¥–µ–ª—å: {best_model_path}")
    
    # –í–∞–ª–∏–¥–∞—Ü–∏—è –Ω–∞ —Ç–µ—Å—Ç–æ–≤–æ–º –Ω–∞–±–æ—Ä–µ
    logger.info("\nüìä –í–∞–ª–∏–¥–∞—Ü–∏—è –Ω–∞ —Ç–µ—Å—Ç–æ–≤–æ–º –Ω–∞–±–æ—Ä–µ...")
    validator = ModelValidator(config)
    test_metrics = validator.validate(model, test_dataset)
    
    logger.info("–ú–µ—Ç—Ä–∏–∫–∏ –Ω–∞ —Ç–µ—Å—Ç–æ–≤–æ–º –Ω–∞–±–æ—Ä–µ:")
    for metric, value in test_metrics.items():
        logger.info(f"  {metric}: {value:.4f}")
    
    return model, test_data

def run_backtesting_pipeline(config: dict, model, test_data, logger):
    """–≠—Ç–∞–ø 3: –ë—ç–∫—Ç–µ—Å—Ç–∏–Ω–≥ —Ç–æ—Ä–≥–æ–≤–æ–π —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏"""
    logger.info("\n" + "="*80)
    logger.info("üí∞ –≠–¢–ê–ü 3: –ë–≠–ö–¢–ï–°–¢–ò–ù–ì –¢–û–†–ì–û–í–û–ô –°–¢–†–ê–¢–ï–ì–ò–ò")
    logger.info("="*80)
    
    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤ —Ç–æ—Ä–≥–æ–≤–ª–∏
    risk_manager = RiskManager(config)
    signal_generator = SignalGenerator(config)
    backtester = Backtester(config)
    
    # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π –º–æ–¥–µ–ª–∏
    logger.info("üîÆ –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π –º–æ–¥–µ–ª–∏...")
    predictions = model.predict(test_data)
    
    # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ç–æ—Ä–≥–æ–≤—ã—Ö —Å–∏–≥–Ω–∞–ª–æ–≤
    logger.info("üì° –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ç–æ—Ä–≥–æ–≤—ã—Ö —Å–∏–≥–Ω–∞–ª–æ–≤...")
    signals = signal_generator.generate_signals(predictions, test_data)
    logger.info(f"  –°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–æ {len(signals)} —Å–∏–≥–Ω–∞–ª–æ–≤")
    
    # –ó–∞–ø—É—Å–∫ –±—ç–∫—Ç–µ—Å—Ç–∏–Ω–≥–∞
    logger.info("\nüèÉ –ó–∞–ø—É—Å–∫ –±—ç–∫—Ç–µ—Å—Ç–∏–Ω–≥–∞...")
    backtest_results = backtester.run_backtest(
        market_data=test_data,
        features=test_data,
        model_predictions=predictions
    )
    
    # –û—Ç–æ–±—Ä–∞–∂–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    logger.info("\nüìà –†–ï–ó–£–õ–¨–¢–ê–¢–´ –ë–≠–ö–¢–ï–°–¢–ò–ù–ì–ê:")
    logger.info(f"  –ù–∞—á–∞–ª—å–Ω—ã–π –∫–∞–ø–∏—Ç–∞–ª: ${backtest_results['initial_capital']:,.2f}")
    logger.info(f"  –§–∏–Ω–∞–ª—å–Ω—ã–π –∫–∞–ø–∏—Ç–∞–ª: ${backtest_results['final_capital']:,.2f}")
    logger.info(f"  –û–±—â–∞—è –¥–æ—Ö–æ–¥–Ω–æ—Å—Ç—å: {backtest_results['total_return_pct']:.2f}%")
    logger.info(f"  –ö–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç –®–∞—Ä–ø–∞: {backtest_results['sharpe_ratio']:.2f}")
    logger.info(f"  –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –ø—Ä–æ—Å–∞–¥–∫–∞: {backtest_results['max_drawdown_pct']:.2f}%")
    logger.info(f"  Win Rate: {backtest_results['win_rate_pct']:.2f}%")
    logger.info(f"  –í—Å–µ–≥–æ —Å–¥–µ–ª–æ–∫: {backtest_results['total_trades']}")
    
    return backtest_results

def run_visualization_pipeline(config: dict, backtest_results, logger):
    """–≠—Ç–∞–ø 4: –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤"""
    logger.info("\n" + "="*80)
    logger.info("üìä –≠–¢–ê–ü 4: –í–ò–ó–£–ê–õ–ò–ó–ê–¶–ò–Ø –†–ï–ó–£–õ–¨–¢–ê–¢–û–í")
    logger.info("="*80)
    
    visualizer = TradingVisualizer(config)
    
    # –°–æ–∑–¥–∞–Ω–∏–µ –≥—Ä–∞—Ñ–∏–∫–æ–≤
    logger.info("üìà –°–æ–∑–¥–∞–Ω–∏–µ –≥—Ä–∞—Ñ–∏–∫–æ–≤...")
    
    # –ì—Ä–∞—Ñ–∏–∫ —ç–∫–≤–∏—Ç–∏
    equity_plot = visualizer.plot_equity_curve(backtest_results['equity_curve'])
    logger.info(f"  ‚úÖ –ì—Ä–∞—Ñ–∏–∫ —ç–∫–≤–∏—Ç–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {equity_plot}")
    
    # –ì—Ä–∞—Ñ–∏–∫ –ø—Ä–æ—Å–∞–¥–æ–∫
    drawdown_plot = visualizer.plot_drawdown(backtest_results['equity_curve'])
    logger.info(f"  ‚úÖ –ì—Ä–∞—Ñ–∏–∫ –ø—Ä–æ—Å–∞–¥–æ–∫ —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {drawdown_plot}")
    
    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ —Å–∏–º–≤–æ–ª–∞–º
    if 'performance_by_symbol' in backtest_results:
        symbol_plot = visualizer.plot_symbol_performance(backtest_results['performance_by_symbol'])
        logger.info(f"  ‚úÖ –ì—Ä–∞—Ñ–∏–∫ –ø–æ —Å–∏–º–≤–æ–ª–∞–º —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {symbol_plot}")
    
    # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç—á–µ—Ç–∞
    report_path = Path("results/backtest_report.txt")
    report_path.parent.mkdir(exist_ok=True)
    
    with open(report_path, 'w', encoding='utf-8') as f:
        f.write(backtester.generate_report(backtest_results))
    
    logger.info(f"\nüìÑ –ü–æ–ª–Ω—ã–π –æ—Ç—á–µ—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {report_path}")

def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –∑–∞–ø—É—Å–∫–∞"""
    parser = argparse.ArgumentParser(description='Crypto AI Trading System')
    parser.add_argument('--config', type=str, default='config/config.yaml',
                        help='–ü—É—Ç—å –∫ —Ñ–∞–π–ª—É –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏')
    parser.add_argument('--mode', type=str, default='full',
                        choices=['full', 'data', 'train', 'backtest', 'demo'],
                        help='–†–µ–∂–∏–º –∑–∞–ø—É—Å–∫–∞ —Å–∏—Å—Ç–µ–º—ã')
    parser.add_argument('--skip-cache', action='store_true',
                        help='–ü—Ä–æ–ø—É—Å—Ç–∏—Ç—å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –∫—ç—à–∞')
    
    args = parser.parse_args()
    
    # –í–∞–ª–∏–¥–∞—Ü–∏—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –ø–µ—Ä–µ–¥ –∑–∞–≥—Ä—É–∑–∫–æ–π
    from utils.config_validator_main import validate_and_exit_on_error
    validate_and_exit_on_error(args.config)
    
    # –ó–∞–≥—Ä—É–∑–∫–∞ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
    config = load_config(args.config)
    
    # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
    setup_logging(config)
    logger = get_logger("CryptoAI")
    
    logger.info("="*80)
    logger.info("üöÄ –ó–ê–ü–£–°–ö CRYPTO AI TRADING SYSTEM")
    logger.info(f"üìã –†–µ–∂–∏–º: {args.mode}")
    logger.info(f"‚öôÔ∏è –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è: {args.config}")
    logger.info(f"üñ•Ô∏è –£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {config['performance']['device']}")
    logger.info(f"üì¶ USE_CACHE_ONLY: {os.environ.get('USE_CACHE_ONLY', '–Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–æ')}")
    logger.info("="*80)
    
    try:
        if args.mode == 'demo':
            # –î–µ–º–æ —Ä–µ–∂–∏–º - —Ç–æ–ª—å–∫–æ –ø—Ä–æ–≤–µ—Ä–∫–∞ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏–π
            logger.info("üéØ –î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–æ–Ω–Ω—ã–π —Ä–µ–∂–∏–º")
            data_loader = CryptoDataLoader(config)
            symbols = data_loader.get_available_symbols()
            logger.info(f"‚úÖ –°–∏—Å—Ç–µ–º–∞ –≥–æ—Ç–æ–≤–∞ –∫ —Ä–∞–±–æ—Ç–µ. –ù–∞–π–¥–µ–Ω–æ {len(symbols)} —Å–∏–º–≤–æ–ª–æ–≤")
            
        elif args.mode == 'data':
            # –¢–æ–ª—å–∫–æ –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö
            features = run_data_pipeline(config, logger)
            logger.info(f"\n‚úÖ –î–∞–Ω–Ω—ã–µ –ø–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω—ã: {features.shape}")
            
        elif args.mode == 'train':
            # –û–±—É—á–µ–Ω–∏–µ –Ω–∞ –ø–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
            cache_path = Path("cache/features_cache.pkl")
            if cache_path.exists() and not args.skip_cache:
                logger.info(f"üìÇ –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –∏–∑ –∫—ç—à–∞: {cache_path}")
                features = pd.read_pickle(cache_path)
            else:
                features = run_data_pipeline(config, logger)
            
            model, test_data = run_training_pipeline(config, features, logger)
            logger.info("\n‚úÖ –ú–æ–¥–µ–ª—å –æ–±—É—á–µ–Ω–∞ –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞")
            
        elif args.mode == 'backtest':
            # –ë—ç–∫—Ç–µ—Å—Ç–∏–Ω–≥ —Å –æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª—å—é
            # –ó–¥–µ—Å—å –Ω—É–∂–Ω–æ –∑–∞–≥—Ä—É–∑–∏—Ç—å –æ–±—É—á–µ–Ω–Ω—É—é –º–æ–¥–µ–ª—å
            logger.info("üîÑ –†–µ–∂–∏–º –±—ç–∫—Ç–µ—Å—Ç–∏–Ω–≥–∞ (–≤ —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–µ)")
            
        else:  # full
            # –ü–æ–ª–Ω—ã–π —Ü–∏–∫–ª
            features = run_data_pipeline(config, logger)
            model, test_data = run_training_pipeline(config, features, logger)
            backtest_results = run_backtesting_pipeline(config, model, test_data, logger)
            run_visualization_pipeline(config, backtest_results, logger)
            
            logger.info("\n" + "="*80)
            logger.info("‚úÖ –ü–û–õ–ù–´–ô –¶–ò–ö–õ –ó–ê–í–ï–†–®–ï–ù –£–°–ü–ï–®–ù–û!")
            logger.info("="*80)
            
    except Exception as e:
        logger.error(f"‚ùå –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞: {e}", exc_info=True)
        sys.exit(1)

if __name__ == "__main__":
    import pandas as pd
    import numpy as np
    main()