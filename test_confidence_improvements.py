#!/usr/bin/env python3
"""
–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —É–ª—É—á—à–µ–Ω–∏–π –¥–ª—è –ø–æ–≤—ã—à–µ–Ω–∏—è —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–∏
"""

import torch
import yaml
import numpy as np
from pathlib import Path

def test_confidence_improvements():
    """–ü—Ä–æ–≤–µ—Ä–∫–∞ –≤—Å–µ—Ö –º–µ—Ö–∞–Ω–∏–∑–º–æ–≤ –ø–æ–≤—ã—à–µ–Ω–∏—è —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏"""
    print("\n" + "="*80)
    print("üß™ –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —É–ª—É—á—à–µ–Ω–∏–π –¥–ª—è –ø–æ–≤—ã—à–µ–Ω–∏—è —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–∏")
    print("="*80)
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é
    with open('config/config.yaml', 'r') as f:
        config = yaml.safe_load(f)
    
    print("\n‚úÖ –ü—Ä–æ–≤–µ—Ä–∫–∞ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏:")
    print(f"   - Label smoothing: {config['model'].get('label_smoothing', 0)}")
    print(f"   - Temperature scaling: {config['model'].get('temperature_scaling', False)}")
    print(f"   - Confidence threshold: {config['model'].get('confidence_threshold', 0.6)}")
    print(f"   - Dropout schedule: {config['model'].get('dropout_schedule', False)}")
    print(f"   - Mixup alpha: {config['model'].get('mixup_alpha', 0)}")
    print(f"   - Early stopping patience: {config['model'].get('early_stopping_patience', 25)}")
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –º–æ–¥–µ–ª—å
    from models.patchtst_unified import UnifiedPatchTSTForTrading
    
    print("\nüèóÔ∏è –°–æ–∑–¥–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏ —Å –Ω–æ–≤—ã–º–∏ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–∞–º–∏...")
    model = UnifiedPatchTSTForTrading(config)
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –Ω–æ–≤—ã—Ö –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤
    print("\nüìã –ü—Ä–æ–≤–µ—Ä–∫–∞ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤ –º–æ–¥–µ–ª–∏:")
    
    # Temperature scaling
    if hasattr(model, 'temperature'):
        if model.temperature is not None:
            print(f"   ‚úÖ Temperature parameter: {model.temperature.item():.3f}")
        else:
            print("   ‚ùå Temperature scaling –≤—ã–∫–ª—é—á–µ–Ω")
    else:
        print("   ‚ùå Temperature parameter –Ω–µ –Ω–∞–π–¥–µ–Ω")
    
    # Confidence head
    if hasattr(model, 'confidence_head'):
        print("   ‚úÖ Confidence head –Ω–∞–π–¥–µ–Ω")
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å
        dummy_input = torch.randn(1, model.d_model)
        with torch.no_grad():
            conf_output = model.confidence_head(dummy_input)
        print(f"      –í—ã—Ö–æ–¥–Ω–∞—è —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å: {conf_output.shape}")
    else:
        print("   ‚ùå Confidence head –Ω–µ –Ω–∞–π–¥–µ–Ω")
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º Loss —Ñ—É–Ω–∫—Ü–∏—é
    from models.patchtst_unified import DirectionalMultiTaskLoss
    
    print("\nüéØ –ü—Ä–æ–≤–µ—Ä–∫–∞ Loss —Ñ—É–Ω–∫—Ü–∏–∏:")
    loss_fn = DirectionalMultiTaskLoss(config)
    
    print(f"   - Label smoothing: {loss_fn.label_smoothing}")
    print(f"   - Class weights: {loss_fn.class_weights}")
    print(f"   - Warmup epochs: {loss_fn.warmup_epochs}")
    
    # –¢–µ—Å—Ç label smoothing
    if loss_fn.label_smoothing > 0:
        print("\n   üìä –¢–µ—Å—Ç label smoothing:")
        targets = torch.tensor([0, 1, 2, 0])  # LONG, SHORT, FLAT, LONG
        smoothed = loss_fn.apply_label_smoothing(targets, num_classes=3)
        print(f"      Original targets: {targets}")
        print(f"      Smoothed targets shape: {smoothed.shape}")
        print(f"      Smoothed example (class 0): {smoothed[0]}")
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º Trainer
    print("\nüèãÔ∏è –ü—Ä–æ–≤–µ—Ä–∫–∞ OptimizedTrainer:")
    
    from training.optimized_trainer import OptimizedTrainer
    
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    model = model.to(device)
    
    trainer = OptimizedTrainer(model, config, device)
    
    if hasattr(trainer, 'use_dropout_schedule'):
        print(f"   ‚úÖ Dropout schedule: {trainer.use_dropout_schedule}")
        if trainer.use_dropout_schedule:
            print(f"      Initial dropout: {trainer.initial_dropout}")
            print(f"      Final dropout: {trainer.final_dropout}")
            print(f"      Warmup epochs: {trainer.dropout_warmup_epochs}")
    
    if hasattr(trainer, 'use_mixup'):
        print(f"   ‚úÖ Mixup augmentation: {trainer.use_mixup}")
        if trainer.use_mixup:
            print(f"      Alpha: {trainer.mixup_alpha}")
    
    # –¢–µ—Å—Ç forward pass —Å confidence
    print("\nüîÑ –¢–µ—Å—Ç forward pass –º–æ–¥–µ–ª–∏:")
    
    batch_size = 4
    seq_len = config['model']['context_window']
    n_features = config['model']['input_size']
    
    dummy_input = torch.randn(batch_size, seq_len, n_features).to(device)
    
    with torch.no_grad():
        outputs = model(dummy_input)
    
    print(f"   - –í—ã—Ö–æ–¥–Ω–∞—è —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å: {outputs.shape}")
    
    if hasattr(outputs, '_direction_logits'):
        print(f"   ‚úÖ Direction logits: {outputs._direction_logits.shape}")
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º temperature scaling
        if model.temperature is not None:
            # –í—ã—á–∏—Å–ª—è–µ–º softmax —Å –∏ –±–µ–∑ temperature
            logits = outputs._direction_logits[0, 0, :]  # –ü–µ—Ä–≤—ã–π –±–∞—Ç—á, –ø–µ—Ä–≤—ã–π —Ç–∞–π–º—Ñ—Ä–µ–π–º
            probs_normal = torch.softmax(logits, dim=-1)
            probs_scaled = torch.softmax(logits / model.temperature, dim=-1)
            
            print(f"\n   üìä –≠—Ñ—Ñ–µ–∫—Ç Temperature Scaling:")
            print(f"      –ë–µ–∑ scaling: {probs_normal}")
            print(f"      –° scaling:   {probs_scaled}")
            print(f"      –≠–Ω—Ç—Ä–æ–ø–∏—è –±–µ–∑: {-(probs_normal * torch.log(probs_normal + 1e-8)).sum().item():.3f}")
            print(f"      –≠–Ω—Ç—Ä–æ–ø–∏—è —Å:  {-(probs_scaled * torch.log(probs_scaled + 1e-8)).sum().item():.3f}")
    
    if hasattr(outputs, '_confidence_scores'):
        print(f"   ‚úÖ Confidence scores: {outputs._confidence_scores.shape}")
        conf_logits = outputs._confidence_scores
        conf_probs = torch.sigmoid(conf_logits)  # –ü—Ä–∏–º–µ–Ω—è–µ–º sigmoid –∫ –ª–æ–≥–∏—Ç–∞–º
        print(f"      –°—Ä–µ–¥–Ω—è—è —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: {conf_probs.mean().item():.3f}")
        print(f"      –ú–∏–Ω/–ú–∞–∫—Å: {conf_probs.min().item():.3f} / {conf_probs.max().item():.3f}")
    
    # –¢–µ—Å—Ç confidence filter
    print("\nüéØ –¢–µ—Å—Ç —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏ –ø–æ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏:")
    
    from utils.confidence_filter import filter_predictions_by_confidence, get_high_confidence_signals
    
    # –°–æ–∑–¥–∞–µ–º —Ç–µ—Å—Ç–æ–≤—ã–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
    test_predictions = {
        'direction_classes': torch.randint(0, 3, (batch_size, 4)),
        'confidence_scores': torch.rand(batch_size, 4),
        'future_returns': torch.randn(batch_size, 4) * 0.05,
        'long_levels': torch.rand(batch_size, 4),
        'short_levels': torch.rand(batch_size, 4)
    }
    
    # –§–∏–ª—å—Ç—Ä—É–µ–º
    filtered = filter_predictions_by_confidence(test_predictions, confidence_threshold=0.6)
    
    # –ò–∑–≤–ª–µ–∫–∞–µ–º –≤—ã—Å–æ–∫–æ—É–≤–µ—Ä–µ–Ω–Ω—ã–µ —Å–∏–≥–Ω–∞–ª—ã
    signals = get_high_confidence_signals(test_predictions, min_confidence=0.7)
    
    print("\n" + "="*80)
    print("‚úÖ –í—Å–µ –º–µ—Ö–∞–Ω–∏–∑–º—ã –ø–æ–≤—ã—à–µ–Ω–∏—è —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏ —É—Å–ø–µ—à–Ω–æ —Ä–µ–∞–ª–∏–∑–æ–≤–∞–Ω—ã!")
    print("\nüéØ –û–∂–∏–¥–∞–µ–º—ã–µ —É–ª—É—á—à–µ–Ω–∏—è:")
    print("   1. Temperature scaling —Å–¥–µ–ª–∞–µ—Ç –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –±–æ–ª–µ–µ '–æ—Å—Ç—Ä—ã–º–∏'")
    print("   2. Label smoothing —É–ª—É—á—à–∏—Ç –≥–µ–Ω–µ—Ä–∞–ª–∏–∑–∞—Ü–∏—é")
    print("   3. Confidence head –ø–æ–∑–≤–æ–ª–∏—Ç —Ñ–∏–ª—å—Ç—Ä–æ–≤–∞—Ç—å –Ω–µ—É–≤–µ—Ä–µ–Ω–Ω—ã–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è")
    print("   4. Dropout schedule –ø–æ—Å—Ç–µ–ø–µ–Ω–Ω–æ —É–≤–µ–ª–∏—á–∏—Ç —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å")
    print("   5. Confidence-aware loss –∑–∞—Å—Ç–∞–≤–∏—Ç –º–æ–¥–µ–ª—å –±—ã—Ç—å –±–æ–ª–µ–µ —Ä–µ—à–∏—Ç–µ–ª—å–Ω–æ–π")
    print("\nüöÄ –ó–∞–ø—É—Å—Ç–∏—Ç–µ –æ–±—É—á–µ–Ω–∏–µ –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏:")
    print("   python main.py --mode train")
    print("="*80)


if __name__ == "__main__":
    test_confidence_improvements()