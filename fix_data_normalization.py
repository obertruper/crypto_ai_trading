#!/usr/bin/env python3
"""
–ò—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –∏ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –∫—ç—à–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
"""

import pandas as pd
import numpy as np
from pathlib import Path
from utils.logger import get_logger
from sklearn.preprocessing import RobustScaler
import warnings
warnings.filterwarnings('ignore')

def fix_cached_data():
    """–ò—Å–ø—Ä–∞–≤–ª—è–µ—Ç –ø—Ä–æ–±–ª–µ–º—ã —Å –∫—ç—à–∏—Ä–æ–≤–∞–Ω–Ω—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏"""
    logger = get_logger("DataFix")
    
    logger.info("üîß –ò—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –∫—ç—à–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö...")
    
    # –ü—É—Ç–∏ –∫ —Ñ–∞–π–ª–∞–º
    data_dir = Path("data/processed")
    files = {
        "train": data_dir / "train_data.parquet",
        "val": data_dir / "val_data.parquet", 
        "test": data_dir / "test_data.parquet"
    }
    
    # –ö–æ–ª–æ–Ω–∫–∏, –∫–æ—Ç–æ—Ä—ã–µ –Ω–µ –Ω—É–∂–Ω–æ –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞—Ç—å
    skip_columns = ['symbol', 'datetime', 'id']
    
    # –ö–æ–ª–æ–Ω–∫–∏ —Å –ø—Ä–æ–±–ª–µ–º–Ω—ã–º–∏ –∑–Ω–∞—á–µ–Ω–∏—è–º–∏
    problematic_columns = [
        'close_vwap_ratio', 'amihud_illiquidity', 'amihud_ma',
        'volume_volatility_ratio', 'liquidity_score'
    ]
    
    for name, file_path in files.items():
        if not file_path.exists():
            logger.error(f"‚ùå –§–∞–π–ª {file_path} –Ω–µ –Ω–∞–π–¥–µ–Ω!")
            continue
            
        logger.info(f"\nüìä –û–±—Ä–∞–±–æ—Ç–∫–∞ {name} –¥–∞–Ω–Ω—ã—Ö...")
        df = pd.read_parquet(file_path)
        original_shape = df.shape
        
        # 1. –ó–∞–º–µ–Ω–∞ inf –Ω–∞ NaN
        numeric_cols = df.select_dtypes(include=[np.number]).columns
        logger.info("  –ó–∞–º–µ–Ω–∞ inf –∑–Ω–∞—á–µ–Ω–∏–π...")
        df[numeric_cols] = df[numeric_cols].replace([np.inf, -np.inf], np.nan)
        
        # 2. –û–±—Ä–∞–±–æ—Ç–∫–∞ –ø—Ä–æ–±–ª–µ–º–Ω—ã—Ö –∫–æ–ª–æ–Ω–æ–∫
        for col in problematic_columns:
            if col in df.columns:
                logger.info(f"  –û–±—Ä–∞–±–æ—Ç–∫–∞ {col}...")
                # –ó–∞–º–µ–Ω—è–µ–º —ç–∫—Å—Ç—Ä–µ–º–∞–ª—å–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è –Ω–∞ –º–µ–¥–∏–∞–Ω—É
                median_val = df[col].median()
                std_val = df[col].std()
                upper_bound = median_val + 10 * std_val
                lower_bound = median_val - 10 * std_val
                
                # –ö–ª–∏–ø–ø–∏–Ω–≥ —ç–∫—Å—Ç—Ä–µ–º–∞–ª—å–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π
                df[col] = df[col].clip(lower=lower_bound, upper=upper_bound)
                
                # –ó–∞–ø–æ–ª–Ω–µ–Ω–∏–µ NaN –º–µ–¥–∏–∞–Ω–æ–π
                df[col] = df[col].fillna(median_val)
        
        # 3. –û–±—Ä–∞–±–æ—Ç–∫–∞ –æ—Å—Ç–∞–ª—å–Ω—ã—Ö —á–∏—Å–ª–æ–≤—ã—Ö –∫–æ–ª–æ–Ω–æ–∫
        for col in numeric_cols:
            if col in skip_columns or col in problematic_columns:
                continue
                
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –±–æ–ª—å—à–∏–µ –∑–Ω–∞—á–µ–Ω–∏—è
            if df[col].abs().max() > 10000:
                logger.info(f"  –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è {col} (max={df[col].abs().max():.2f})...")
                
                # –î–ª—è —Ü–µ–Ω –∏ –æ–±—ä–µ–º–æ–≤ –∏—Å–ø–æ–ª—å–∑—É–µ–º log-—Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏—é
                if any(x in col for x in ['price', 'volume', 'turnover', 'open', 'high', 'low', 'close']):
                    # Log transform –¥–ª—è –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π
                    mask = df[col] > 0
                    df.loc[mask, col] = np.log1p(df.loc[mask, col])
                else:
                    # –î–ª—è –æ—Å—Ç–∞–ª—å–Ω—ã—Ö - –∫–ª–∏–ø–ø–∏–Ω–≥ –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø–µ—Ä—Å–µ–Ω—Ç–∏–ª–µ–π
                    p1 = df[col].quantile(0.001)
                    p99 = df[col].quantile(0.999)
                    df[col] = df[col].clip(lower=p1, upper=p99)
        
        # 4. –§–∏–Ω–∞–ª—å–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –∏ –∑–∞–ø–æ–ª–Ω–µ–Ω–∏–µ NaN
        logger.info("  –§–∏–Ω–∞–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ NaN...")
        for col in numeric_cols:
            if col in skip_columns:
                continue
            
            nan_count = df[col].isna().sum()
            if nan_count > 0:
                # –ó–∞–ø–æ–ª–Ω—è–µ–º –Ω—É–ª—è–º–∏ –¥–ª—è —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏—Ö –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–æ–≤
                if any(x in col for x in ['rsi', 'macd', 'bb_', 'ema', 'sma']):
                    df[col] = df[col].fillna(0)
                else:
                    # –î–ª—è –æ—Å—Ç–∞–ª—å–Ω—ã—Ö –∏—Å–ø–æ–ª—å–∑—É–µ–º forward fill –∑–∞—Ç–µ–º backward fill
                    df[col] = df[col].fillna(method='ffill').fillna(method='bfill').fillna(0)
        
        # 5. –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ (–∫—Ä–æ–º–µ —Ü–µ–ª–µ–≤—ã—Ö)
        logger.info("  –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ RobustScaler –∫ –ø—Ä–∏–∑–Ω–∞–∫–∞–º...")
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –ø—Ä–∏–∑–Ω–∞–∫–∏ –∏ —Ü–µ–ª–µ–≤—ã–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ
        feature_cols = [col for col in numeric_cols if not any(
            x in col for x in ['future_return', 'long_', 'short_', 'best_direction', 'target_return']
        ) and col not in skip_columns]
        
        if feature_cols:
            scaler = RobustScaler()
            df[feature_cols] = scaler.fit_transform(df[feature_cols])
        
        # 6. –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞
        logger.info("  –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞...")
        max_val = df[numeric_cols].abs().max().max()
        nan_count = df[numeric_cols].isna().sum().sum()
        
        logger.info(f"  ‚úÖ –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {original_shape} -> {df.shape}")
        logger.info(f"  ‚úÖ Max –∑–Ω–∞—á–µ–Ω–∏–µ: {max_val:.4f}")
        logger.info(f"  ‚úÖ NaN –∑–Ω–∞—á–µ–Ω–∏–π: {nan_count}")
        
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Å backup
        backup_path = file_path.with_suffix('.parquet.backup')
        logger.info(f"  üíæ –°–æ–∑–¥–∞–Ω–∏–µ backup: {backup_path}")
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ—Ä–∏–≥–∏–Ω–∞–ª –∫–∞–∫ backup –µ—Å–ª–∏ –µ–≥–æ –µ—â–µ –Ω–µ—Ç
        if not backup_path.exists():
            import shutil
            shutil.copy(file_path, backup_path)
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
        logger.info(f"  üíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö...")
        df.to_parquet(file_path, index=False)
        
        logger.info(f"  ‚úÖ {name} –¥–∞–Ω–Ω—ã–µ —É—Å–ø–µ—à–Ω–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω—ã!")
    
    logger.info("\n‚úÖ –í—Å–µ –¥–∞–Ω–Ω—ã–µ –æ–±—Ä–∞–±–æ—Ç–∞–Ω—ã!")
    logger.info("–¢–µ–ø–µ—Ä—å –º–æ–∂–Ω–æ –∑–∞–ø—É—Å–∫–∞—Ç—å –æ–±—É—á–µ–Ω–∏–µ: python main.py --mode train")

if __name__ == "__main__":
    fix_cached_data()