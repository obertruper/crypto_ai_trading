#!/usr/bin/env python3
"""
–ü–æ–ª–Ω—ã–π —Å–∫—Ä–∏–ø—Ç –¥–ª—è –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∏ –¥–∞–Ω–Ω—ã—Ö –∏ –∑–∞–ø—É—Å–∫–∞ –æ–±—É—á–µ–Ω–∏—è —Ç–æ—Ä–≥–æ–≤–æ–π –º–æ–¥–µ–ª–∏
–í–µ—Ä—Å–∏—è 2.0 - —Å –≤–µ—Ä—Å–∏–æ–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ–º –∫—ç—à–∞ –∏ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–µ–π
"""

import argparse
import yaml
import pandas as pd
import numpy as np
from datetime import datetime, timedelta
from pathlib import Path
import warnings
warnings.filterwarnings('ignore')
import multiprocessing as mp
from multiprocessing import RLock
from functools import partial
import concurrent.futures
import psutil
import time
import hashlib
import gc

from data.data_loader import CryptoDataLoader
from data.feature_engineering import FeatureEngineer
from utils.logger import get_logger
from tqdm import tqdm

# –í–ê–ñ–ù–û: –£–≤–µ–ª–∏—á–∏–≤–∞—Ç—å –ø—Ä–∏ –∏–∑–º–µ–Ω–µ–Ω–∏–∏ –ª–æ–≥–∏–∫–∏ —Å–æ–∑–¥–∞–Ω–∏—è –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
FEATURE_VERSION = "2.5"  # –ò—Å–ø—Ä–∞–≤–ª–µ–Ω—ã: volume_volatility_ratio, OBV —Å log-—Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏–µ–π


def check_database_connection(config: dict, logger):
    """–ü—Ä–æ–≤–µ—Ä–∫–∞ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è –∫ –ë–î –∏ –Ω–∞–ª–∏—á–∏—è –¥–∞–Ω–Ω—ã—Ö"""
    logger.info("üîç –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è –∫ PostgreSQL...")
    
    try:
        data_loader = CryptoDataLoader(config)
        stats = data_loader.get_data_stats()
        
        logger.info(f"‚úÖ –ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ —É—Å–ø–µ—à–Ω–æ!")
        logger.info(f"üìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ë–î: {stats['total_records']:,} –∑–∞–ø–∏—Å–µ–π, {stats['unique_symbols']} —Å–∏–º–≤–æ–ª–æ–≤")
        
        return True, data_loader
        
    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è –∫ –ë–î: {e}")
        logger.error("–ü—Ä–æ–≤–µ—Ä—å—Ç–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –≤ config.yaml –∏ –∑–∞–ø—É—Å—Ç–∏—Ç–µ PostgreSQL")
        return False, None


def process_symbol_features(symbol: str, symbol_data: pd.DataFrame, config: dict, 
                           logger_name: str, use_cache: bool = True, 
                           position: int = None, disable_progress: bool = False) -> pd.DataFrame:
    """–û–±—Ä–∞–±–æ—Ç–∫–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è –æ–¥–Ω–æ–≥–æ —Å–∏–º–≤–æ–ª–∞ (–¥–ª—è –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏)"""
    logger = get_logger(f"{logger_name}_{symbol}", is_subprocess=True)
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–µ—à —Å —É—á–µ—Ç–æ–º –≤–µ—Ä—Å–∏–∏
    cache_dir = Path("cache/features")
    cache_dir.mkdir(parents=True, exist_ok=True)
    
    # –í–∫–ª—é—á–∞–µ–º –≤–µ—Ä—Å–∏—é –≤ —Ö—ç—à –¥–ª—è –∏–Ω–≤–∞–ª–∏–¥–∞—Ü–∏–∏ –ø—Ä–∏ –∏–∑–º–µ–Ω–µ–Ω–∏–∏ –∫–æ–¥–∞
    cache_key = f"{symbol}_{len(symbol_data)}_{symbol_data.index[0]}_{symbol_data.index[-1]}_{FEATURE_VERSION}"
    data_hash = hashlib.md5(cache_key.encode()).hexdigest()
    cache_file = cache_dir / f"{symbol}_{data_hash}.parquet"
    
    if use_cache and cache_file.exists():
        try:
            logger.info(f"üì¶ –ó–∞–≥—Ä—É–∑–∫–∞ {symbol} –∏–∑ –∫–µ—à–∞ (v{FEATURE_VERSION})...")
            return pd.read_parquet(cache_file)
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è –∫–µ—à–∞ –¥–ª—è {symbol}: {e}")
    
    try:
        # –°–æ–∑–¥–∞–µ–º –æ—Ç–¥–µ–ª—å–Ω—ã–π —ç–∫–∑–µ–º–ø–ª—è—Ä FeatureEngineer –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –ø—Ä–æ—Ü–µ—Å—Å–∞
        feature_engineer = FeatureEngineer(config)
        feature_engineer.process_position = position
        feature_engineer.disable_progress = disable_progress
        
        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –¥–∞–Ω–Ω—ã–µ
        symbol_data = symbol_data.sort_values('datetime')
        
        # –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è —Ç–∏–ø–æ–≤ –¥–∞–Ω–Ω—ã—Ö
        symbol_data = optimize_memory_usage(symbol_data, logger)
        
        # –ò–°–ü–†–ê–í–õ–ï–ù–û: –ò—Å–ø–æ–ª—å–∑—É–µ–º create_features –≤–º–µ—Å—Ç–æ –æ—Ç–¥–µ–ª—å–Ω—ã—Ö –º–µ—Ç–æ–¥–æ–≤
        # –≠—Ç–æ –æ–±–µ—Å–ø–µ—á–∏–≤–∞–µ—Ç –ø—Ä–∞–≤–∏–ª—å–Ω—É—é –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å –æ–±—Ä–∞–±–æ—Ç–∫–∏
        symbol_data = feature_engineer.create_features(symbol_data)
        
        # –§–∏–Ω–∞–ª—å–Ω–∞—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –ø–∞–º—è—Ç–∏
        symbol_data = optimize_memory_usage(symbol_data, logger)
        
        logger.info(f"‚úÖ {symbol}: –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ {len(symbol_data)} –∑–∞–ø–∏—Å–µ–π")
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ –∫–µ—à
        if use_cache:
            try:
                symbol_data.to_parquet(cache_file, compression='snappy')
                logger.debug(f"üíæ {symbol} —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤ –∫–µ—à v{FEATURE_VERSION}")
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å –∫–µ—à –¥–ª—è {symbol}: {e}")
        
        # –Ø–≤–Ω–∞—è –æ—á–∏—Å—Ç–∫–∞ –ø–∞–º—è—Ç–∏
        gc.collect()
        
        return symbol_data
        
    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ {symbol}: {e}")
        import traceback
        logger.error(f"Traceback: {traceback.format_exc()}")
        return pd.DataFrame()


def optimize_memory_usage(df: pd.DataFrame, logger) -> pd.DataFrame:
    """–û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è —Ç–∏–ø–æ–≤ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —ç–∫–æ–Ω–æ–º–∏–∏ –ø–∞–º—è—Ç–∏"""
    start_mem = df.memory_usage().sum() / 1024**2
    
    # –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è —á–∏—Å–ª–æ–≤—ã—Ö —Ç–∏–ø–æ–≤
    for col in df.select_dtypes(include=['float64']).columns:
        df[col] = df[col].astype('float32')
    
    for col in df.select_dtypes(include=['int64']).columns:
        if df[col].min() >= 0 and df[col].max() <= 255:
            df[col] = df[col].astype('uint8')
        elif df[col].min() >= -32768 and df[col].max() <= 32767:
            df[col] = df[col].astype('int16')
        elif df[col].min() >= -2147483648 and df[col].max() <= 2147483647:
            df[col] = df[col].astype('int32')
    
    end_mem = df.memory_usage().sum() / 1024**2
    
    if end_mem < start_mem:
        logger.debug(f"üíæ –ü–∞–º—è—Ç—å: {start_mem:.1f}MB ‚Üí {end_mem:.1f}MB")
    
    return df


def prepare_features_for_trading(config: dict, logger, force_recreate: bool = False):
    """–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è —Ç–æ—Ä–≥–æ–≤–æ–π –º–æ–¥–µ–ª–∏"""
    
    logger.info("\n" + "="*80)
    logger.info("üöÄ –ü–û–î–ì–û–¢–û–í–ö–ê –î–ê–ù–ù–´–• –î–õ–Ø –¢–û–†–ì–û–í–û–ô –ú–û–î–ï–õ–ò")
    logger.info(f"üì¶ –í–µ—Ä—Å–∏—è –ø—Ä–∏–∑–Ω–∞–∫–æ–≤: {FEATURE_VERSION}")
    if force_recreate:
        logger.info("üîÑ –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ–µ –ø–µ—Ä–µ—Å–æ–∑–¥–∞–Ω–∏–µ –∫—ç—à–∞ –≤–∫–ª—é—á–µ–Ω–æ")
    logger.info("="*80)
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –ë–î
    success, data_loader = check_database_connection(config, logger)
    if not success:
        return None
    
    # –ó–∞–≥—Ä—É–∑–∫–∞ —Å—ã—Ä—ã—Ö –¥–∞–Ω–Ω—ã—Ö
    logger.info("\nüì• –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –∏–∑ –ë–î...")
    
    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Å–∏–º–≤–æ–ª—ã –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏
    symbols_to_load = config['data']['symbols']
    if symbols_to_load == 'all':
        available = data_loader.get_available_symbols()
        symbols_to_load = available[:20]  # –¢–æ–ø 20 –¥–ª—è –Ω–∞—á–∞–ª–∞
        logger.info(f"üìä –ó–∞–≥—Ä—É–∂–∞–µ–º —Ç–æ–ø-20 —Å–∏–º–≤–æ–ª–æ–≤")
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ
    raw_data = data_loader.load_data(
        symbols=symbols_to_load,
        start_date=config['data']['start_date'],
        end_date=config['data']['end_date']
    )
    
    logger.info(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(raw_data):,} –∑–∞–ø–∏—Å–µ–π")
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞ –¥–∞–Ω–Ω—ã—Ö
    logger.info("\nüîç –ü—Ä–æ–≤–µ—Ä–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞ –¥–∞–Ω–Ω—ã—Ö...")
    quality_report = data_loader.validate_data_quality(raw_data)
    
    total_issues = 0
    for symbol, report in quality_report.items():
        missing_count = sum(report['missing_values'].values())
        anomalies_count = len(report['anomalies'])
        
        if missing_count > 0 or anomalies_count > 0:
            logger.warning(f"‚ö†Ô∏è {symbol}: –ø—Ä–æ–ø—É—â–µ–Ω–æ {missing_count}, –∞–Ω–æ–º–∞–ª–∏–π {anomalies_count}")
            total_issues += missing_count + anomalies_count
    
    if total_issues == 0:
        logger.info("‚úÖ –ö–∞—á–µ—Å—Ç–≤–æ –¥–∞–Ω–Ω—ã—Ö –æ—Ç–ª–∏—á–Ω–æ–µ!")
    
    # –°–æ–∑–¥–∞–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ —Å –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–æ–π
    logger.info("\nüõ†Ô∏è –°–æ–∑–¥–∞–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤...")
    
    # –ü—Ä–æ—Å—Ç–æ–π —Ä–∞—Å—á–µ—Ç –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –ø—Ä–æ—Ü–µ—Å—Å–æ–≤
    cpu_count = mp.cpu_count()
    n_processes = min(cpu_count - 1, 8)
    logger.info(f"‚ö° –ò—Å–ø–æ–ª—å–∑—É–µ–º {n_processes} –ø—Ä–æ—Ü–µ—Å—Å–æ–≤")
    
    # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º –¥–∞–Ω–Ω—ã–µ –ø–æ —Å–∏–º–≤–æ–ª–∞–º
    symbols = raw_data['symbol'].unique()
    logger.info(f"üìä –û–±—Ä–∞–±–æ—Ç–∫–∞ {len(symbols)} —Å–∏–º–≤–æ–ª–æ–≤...")
    
    start_time = time.time()
    
    # –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –≥–ª–æ–±–∞–ª—å–Ω—É—é –±–ª–æ–∫–∏—Ä–æ–≤–∫—É –¥–ª—è tqdm
    tqdm.set_lock(RLock())
    
    # –ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ —Å–∏–º–≤–æ–ª–æ–≤
    with concurrent.futures.ProcessPoolExecutor(max_workers=n_processes, 
                                                initializer=tqdm.set_lock, 
                                                initargs=(tqdm.get_lock(),)) as executor:
        # –°–æ–∑–¥–∞–µ–º —Ñ—É–Ω–∫—Ü–∏—é —Å —á–∞—Å—Ç–∏—á–Ω—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏
        process_func = partial(process_symbol_features, 
                              config=config, 
                              logger_name="FeatureEngineering",
                              use_cache=not force_recreate)  # –û—Ç–∫–ª—é—á–∞–µ–º –∫–µ—à –ø—Ä–∏ force_recreate
        
        # –°–æ–∑–¥–∞–µ–º –∑–∞–¥–∞—á–∏ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Å–∏–º–≤–æ–ª–∞
        future_to_symbol = {}
        for idx, symbol in enumerate(symbols):
            future = executor.submit(
                process_func, 
                symbol, 
                raw_data[raw_data['symbol'] == symbol].copy(),
                position=idx * 2,
                disable_progress=True
            )
            future_to_symbol[future] = symbol
        
        # –°–æ–±–∏—Ä–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å –ø—Ä–æ–≥—Ä–µ—Å—Å-–±–∞—Ä–æ–º
        featured_dfs = []
        
        with tqdm(total=len(symbols), desc="üöÄ –û–±—Ä–∞–±–æ—Ç–∫–∞ —Å–∏–º–≤–æ–ª–æ–≤", unit="—Å–∏–º–≤–æ–ª") as pbar:
            for future in concurrent.futures.as_completed(future_to_symbol):
                symbol = future_to_symbol[future]
                try:
                    result = future.result()
                    if not result.empty:
                        featured_dfs.append(result)
                        pbar.set_postfix({'–°–∏–º–≤–æ–ª': symbol})
                    pbar.update(1)
                except Exception as e:
                    logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ {symbol}: {e}")
                    import traceback
                    logger.error(f"Traceback: {traceback.format_exc()}")
                    pbar.update(1)
    
    # –û–±—ä–µ–¥–∏–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
    logger.info("\nüìä –û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤...")
    processed_data = pd.concat(featured_dfs, ignore_index=True)
    
    elapsed_time = time.time() - start_time
    logger.info(f"‚è±Ô∏è –û–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–Ω—è–ª–∞ {elapsed_time:.1f} —Å–µ–∫—É–Ω–¥")
    
    # –ò–°–ü–†–ê–í–õ–ï–ù–û: cross-asset features —Ç—Ä–µ–±—É—é—Ç –í–°–ï —Å–∏–º–≤–æ–ª—ã –≤–º–µ—Å—Ç–µ
    # –í process_symbol_features –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç—Å—è –∫–∞–∂–¥—ã–π —Å–∏–º–≤–æ–ª –æ—Ç–¥–µ–ª—å–Ω–æ
    # –ü–æ—ç—Ç–æ–º—É –Ω—É–∂–Ω–æ –≤—ã–ø–æ–ª–Ω–∏—Ç—å cross-asset features –∑–¥–µ—Å—å
    feature_engineer = FeatureEngineer(config)
    
    logger.info("üîÑ –°–æ–∑–¥–∞–Ω–∏–µ cross-asset –ø—Ä–∏–∑–Ω–∞–∫–æ–≤...")
    processed_data = feature_engineer._create_cross_asset_features(processed_data)
    
    # –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –Ω–∞ train/val/test
    logger.info("üìä –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –Ω–∞ –≤—ã–±–æ—Ä–∫–∏...")
    
    # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –≤—Ä–µ–º–µ–Ω–∏
    processed_data = processed_data.sort_values('datetime')
    
    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –∏–Ω–¥–µ–∫—Å—ã –¥–ª—è —Ä–∞–∑–¥–µ–ª–µ–Ω–∏—è
    n_samples = len(processed_data)
    train_end_idx = int(n_samples * config['data']['train_ratio'])
    val_end_idx = train_end_idx + int(n_samples * config['data']['val_ratio'])
    
    train_data = processed_data.iloc[:train_end_idx].copy()
    val_data = processed_data.iloc[train_end_idx:val_end_idx].copy()
    test_data = processed_data.iloc[val_end_idx:].copy()
    
    # –ò–°–ü–†–ê–í–õ–ï–ù–û: –î–æ–±–∞–≤–ª—è–µ–º –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—é –¥–∞–Ω–Ω—ã—Ö –ø–µ—Ä–µ–¥ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º
    # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –±—É–¥–µ—Ç –ø—Ä–∏–º–µ–Ω—è—Ç—å—Å—è –≤ dataset.py –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ
    logger.info("üìè –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏ –¥–∞–Ω–Ω—ã—Ö...")
    
    # –ò–º–ø–æ—Ä—Ç–∏—Ä—É–µ–º –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–µ –º–æ–¥—É–ª–∏ –¥–ª—è –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏
    from data.dataset import TimeSeriesDataset
    
    # –°–æ–∑–¥–∞–µ–º –≤—Ä–µ–º–µ–Ω–Ω—ã–π –¥–∞—Ç–∞—Å–µ—Ç –¥–ª—è –æ–±—É—á–µ–Ω–∏—è scaler
    temp_dataset = TimeSeriesDataset(
        data=train_data,
        context_window=config['model']['context_window'],
        prediction_window=config['model']['target_window'],
        stride=config['model']['stride'],
        normalize=True,
        scaler_path='models_saved/data_scaler.pkl',
        fit_scaler=True  # –û–±—É—á–∞–µ–º scaler –Ω–∞ train –¥–∞–Ω–Ω—ã—Ö
    )
    
    logger.info("‚úÖ Scaler –æ–±—É—á–µ–Ω –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤ models_saved/data_scaler.pkl")
    
    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –¥–∞–Ω–Ω—ã–º
    logger.info("\nüìä –°–¢–ê–¢–ò–°–¢–ò–ö–ê –ü–û–î–ì–û–¢–û–í–õ–ï–ù–ù–´–• –î–ê–ù–ù–´–•:")
    logger.info(f"   - –û–±—É—á–∞—é—â–∞—è –≤—ã–±–æ—Ä–∫–∞: {len(train_data):,} –∑–∞–ø–∏—Å–µ–π")
    logger.info(f"   - –í–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω–∞—è: {len(val_data):,} –∑–∞–ø–∏—Å–µ–π")
    logger.info(f"   - –¢–µ—Å—Ç–æ–≤–∞—è: {len(test_data):,} –∑–∞–ø–∏—Å–µ–π")
    logger.info(f"   - –í—Å–µ–≥–æ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤: {len(train_data.columns)}")
    
    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö
    logger.info("\nüíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –ø–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö...")
    
    data_dir = Path("data/processed")
    data_dir.mkdir(parents=True, exist_ok=True)
    
    train_path = data_dir / "train_data.parquet"
    val_path = data_dir / "val_data.parquet"
    test_path = data_dir / "test_data.parquet"
    
    train_data.to_parquet(train_path, compression='snappy')
    val_data.to_parquet(val_path, compression='snappy')
    test_data.to_parquet(test_path, compression='snappy')
    
    logger.info(f"‚úÖ –î–∞–Ω–Ω—ã–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ {data_dir}")
    
    return {
        'train_data': train_data,
        'val_data': val_data,
        'test_data': test_data,
        'feature_count': len(train_data.columns),
        'symbols': symbols_to_load
    }


def main():
    """–ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è"""
    parser = argparse.ArgumentParser(description='–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Ç–æ—Ä–≥–æ–≤–æ–π –º–æ–¥–µ–ª–∏')
    parser.add_argument('--config', type=str, default='config/config.yaml',
                       help='–ü—É—Ç—å –∫ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏')
    parser.add_argument('--test', action='store_true',
                       help='–¢–µ—Å—Ç–æ–≤—ã–π —Ä–µ–∂–∏–º —Å 3 –º–æ–Ω–µ—Ç–∞–º–∏ (BTC, ETH, SOL)')
    parser.add_argument('--force-recreate', action='store_true',
                       help='–ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ –ø–µ—Ä–µ—Å–æ–∑–¥–∞—Ç—å –∫—ç—à (–∏–≥–Ω–æ—Ä–∏—Ä–æ–≤–∞—Ç—å —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–π)')
    parser.add_argument('--analyze-only', action='store_true',
                       help='–¢–æ–ª—å–∫–æ –∞–Ω–∞–ª–∏–∑ –±–µ–∑ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è')
    
    args = parser.parse_args()
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é
    with open(args.config, 'r') as f:
        config = yaml.safe_load(f)
    
    logger = get_logger("DataPreparation")
    
    # –í —Ç–µ—Å—Ç–æ–≤–æ–º —Ä–µ–∂–∏–º–µ –∏—Å–ø–æ–ª—å–∑—É–µ–º —Ç–æ–ª—å–∫–æ 3 –º–æ–Ω–µ—Ç—ã
    if args.test:
        config['data']['symbols'] = ['BTCUSDT', 'ETHUSDT', 'SOLUSDT']
        logger.info("üß™ –¢–ï–°–¢–û–í–´–ô –†–ï–ñ–ò–ú: –∏—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è —Ç–æ–ª—å–∫–æ 3 –º–æ–Ω–µ—Ç—ã")
    
    # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –¥–∞–Ω–Ω—ã–µ
    result = prepare_features_for_trading(config, logger, force_recreate=args.force_recreate)
    
    if result and not args.analyze_only:
        logger.info("\n" + "="*80)
        logger.info("‚úÖ –î–ê–ù–ù–´–ï –ì–û–¢–û–í–´ –ö –û–ë–£–ß–ï–ù–ò–Æ!")
        logger.info("="*80)
        logger.info("\nüöÄ –ó–∞–ø—É—Å—Ç–∏—Ç–µ –æ–±—É—á–µ–Ω–∏–µ:")
        logger.info("   python main.py --mode train")


if __name__ == "__main__":
    main()