#!/usr/bin/env python3
"""
–ü—Ä–æ–≤–µ—Ä–∫–∞ –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ—Å—Ç–∏ –¥–∞–Ω–Ω—ã—Ö –≤–µ—Ä—Å–∏–∏ 4.0 - —Å —Ñ–æ–∫—É—Å–æ–º –Ω–∞ —É—Ç–µ—á–∫–∏ –∏ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏–µ
"""

import pandas as pd
import numpy as np
from pathlib import Path
from datetime import datetime
import warnings
warnings.filterwarnings('ignore')
from scipy import stats
import matplotlib.pyplot as plt
import seaborn as sns

# –û–ø—Ä–µ–¥–µ–ª—è–µ–º –∫–æ–Ω—Å—Ç–∞–Ω—Ç—ã –ª–æ–∫–∞–ª—å–Ω–æ –¥–ª—è –≤–µ—Ä—Å–∏–∏ 4.0 –ë–ï–ó –£–¢–ï–ß–ï–ö
TRADING_TARGET_VARIABLES = [
    # A. –ë–∞–∑–æ–≤—ã–µ –≤–æ–∑–≤—Ä–∞—Ç—ã (4)
    'future_return_15m',   # —á–µ—Ä–µ–∑ 1 —Å–≤–µ—á—É (15 –º–∏–Ω—É—Ç)
    'future_return_1h',    # —á–µ—Ä–µ–∑ 4 —Å–≤–µ—á–∏ (1 —á–∞—Å) 
    'future_return_4h',    # —á–µ—Ä–µ–∑ 16 —Å–≤–µ—á–µ–π (4 —á–∞—Å–∞)
    'future_return_12h',   # —á–µ—Ä–µ–∑ 48 —Å–≤–µ—á–µ–π (12 —á–∞—Å–æ–≤)
    
    # B. –ù–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –¥–≤–∏–∂–µ–Ω–∏—è (4)
    'direction_15m',       # UP/DOWN/FLAT
    'direction_1h',        
    'direction_4h',        
    'direction_12h',       
    
    # C. –î–æ—Å—Ç–∏–∂–µ–Ω–∏–µ —É—Ä–æ–≤–Ω–µ–π –ø—Ä–∏–±—ã–ª–∏ LONG (4)
    'long_will_reach_1pct_4h',   
    'long_will_reach_2pct_4h',   
    'long_will_reach_3pct_12h',  
    'long_will_reach_5pct_12h',  
    
    # D. –î–æ—Å—Ç–∏–∂–µ–Ω–∏–µ —É—Ä–æ–≤–Ω–µ–π –ø—Ä–∏–±—ã–ª–∏ SHORT (4)
    'short_will_reach_1pct_4h',   
    'short_will_reach_2pct_4h',   
    'short_will_reach_3pct_12h',  
    'short_will_reach_5pct_12h',  
    
    # E. –†–∏—Å–∫-–º–µ—Ç—Ä–∏–∫–∏ (4)
    'max_drawdown_1h',     
    'max_rally_1h',        
    'max_drawdown_4h',     
    'max_rally_4h'        
    
    # –£–î–ê–õ–ï–ù–û: best_action, signal_strength, risk_reward_ratio, optimal_hold_time
    # –≠—Ç–∏ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ —Å–æ–¥–µ—Ä–∂–∞–ª–∏ —É—Ç–µ—á–∫–∏ –¥–∞–Ω–Ω—ã—Ö
]

# –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ —Ü–µ–ª–µ–≤—ã—Ö –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –ø–æ —Ç–∏–ø–∞–º
REGRESSION_TARGETS = [
    'future_return_15m', 'future_return_1h', 'future_return_4h', 'future_return_12h',
    'max_drawdown_1h', 'max_rally_1h', 'max_drawdown_4h', 'max_rally_4h'
]

BINARY_TARGETS = [
    'long_will_reach_1pct_4h', 'long_will_reach_2pct_4h', 
    'long_will_reach_3pct_12h', 'long_will_reach_5pct_12h',
    'short_will_reach_1pct_4h', 'short_will_reach_2pct_4h',
    'short_will_reach_3pct_12h', 'short_will_reach_5pct_12h'
]

MULTICLASS_TARGETS = [
    'direction_15m', 'direction_1h', 'direction_4h', 'direction_12h'
]

SERVICE_COLUMNS = ['datetime', 'symbol', 'id', 'timestamp']

TARGET_GROUPS = {
    'returns': ['future_return_15m', 'future_return_1h', 'future_return_4h', 'future_return_12h'],
    'directions': ['direction_15m', 'direction_1h', 'direction_4h', 'direction_12h'],
    'long_profits': ['long_will_reach_1pct_4h', 'long_will_reach_2pct_4h', 
                     'long_will_reach_3pct_12h', 'long_will_reach_5pct_12h'],
    'short_profits': ['short_will_reach_1pct_4h', 'short_will_reach_2pct_4h',
                      'short_will_reach_3pct_12h', 'short_will_reach_5pct_12h'],
    'risk_metrics': ['max_drawdown_1h', 'max_rally_1h', 'max_drawdown_4h', 'max_rally_4h']
}

# –¶–≤–µ—Ç–Ω–æ–π –≤—ã–≤–æ–¥
class Colors:
    HEADER = '\033[95m'
    OKBLUE = '\033[94m'
    OKGREEN = '\033[92m'
    WARNING = '\033[93m'
    FAIL = '\033[91m'
    ENDC = '\033[0m'
    BOLD = '\033[1m'

def print_header(text):
    print(f"\n{Colors.HEADER}{Colors.BOLD}{'='*60}{Colors.ENDC}")
    print(f"{Colors.HEADER}{Colors.BOLD}{text}{Colors.ENDC}")
    print(f"{Colors.HEADER}{Colors.BOLD}{'='*60}{Colors.ENDC}")

def print_success(text):
    print(f"{Colors.OKGREEN}‚úÖ {text}{Colors.ENDC}")

def print_warning(text):
    print(f"{Colors.WARNING}‚ö†Ô∏è  {text}{Colors.ENDC}")

def print_error(text):
    print(f"{Colors.FAIL}‚ùå {text}{Colors.ENDC}")

def print_info(text):
    print(f"{Colors.OKBLUE}‚ÑπÔ∏è  {text}{Colors.ENDC}")


class DataCorrectnessVerifier:
    """–ö–ª–∞—Å—Å –¥–ª—è –∫–æ–º–ø–ª–µ–∫—Å–Ω–æ–π –ø—Ä–æ–≤–µ—Ä–∫–∏ –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ—Å—Ç–∏ –¥–∞–Ω–Ω—ã—Ö v4.0"""
    
    def __init__(self):
        self.issues = []
        self.warnings = []
        self.train_data = None
        self.val_data = None
        self.test_data = None
        
    def load_data(self):
        """–ó–∞–≥—Ä—É–∑–∫–∞ –≤—Å–µ—Ö –¥–∞—Ç–∞—Å–µ—Ç–æ–≤"""
        files = {
            'train': 'data/processed/train_data.parquet',
            'val': 'data/processed/val_data.parquet', 
            'test': 'data/processed/test_data.parquet'
        }
        
        for name, path in files.items():
            file_path = Path(path)
            if not file_path.exists():
                print_error(f"–§–∞–π–ª {path} –Ω–µ –Ω–∞–π–¥–µ–Ω!")
                self.issues.append(f"{name}_file_missing")
                continue
                
            data = pd.read_parquet(file_path)
            setattr(self, f"{name}_data", data)
            print_success(f"–ó–∞–≥—Ä—É–∂–µ–Ω {name}: {len(data):,} –∑–∞–ø–∏—Å–µ–π")
            
    def check_data_leakage(self):
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ —É—Ç–µ—á–∫–∏ –¥–∞–Ω–Ω—ã—Ö"""
        print_header("üîç –ü–†–û–í–ï–†–ö–ê –ù–ê –£–¢–ï–ß–ö–ò –î–ê–ù–ù–´–• (Data Leakage)")
        
        if self.train_data is None:
            print_error("–î–∞–Ω–Ω—ã–µ –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω—ã!")
            return
            
        # 1. –ü—Ä–æ–≤–µ—Ä–∫–∞ –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —É—Ç–µ—á–µ–∫ (look-ahead bias)
        print_info("\n1. –ü—Ä–æ–≤–µ—Ä–∫–∞ look-ahead bias –≤ –ø—Ä–∏–∑–Ω–∞–∫–∞—Ö...")
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ –≤ –ø—Ä–∏–∑–Ω–∞–∫–∞—Ö –Ω–µ—Ç –±—É–¥—É—â–∏—Ö –¥–∞–Ω–Ω—ã—Ö
        # –ò—Å–∫–ª—é—á–µ–Ω–∏—è –¥–ª—è –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–æ–≤, –∫–æ—Ç–æ—Ä—ã–µ –Ω–µ —è–≤–ª—è—é—Ç—Å—è —É—Ç–µ—á–∫–∞–º–∏
        false_positives = ['williams_r']  # Williams %R - —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–π –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä, –Ω–µ —É—Ç–µ—á–∫–∞
        
        future_cols = [col for col in self.train_data.columns 
                      if any(word in col.lower() for word in ['future', 'forward', 'next', 'will'])
                      and col not in TRADING_TARGET_VARIABLES
                      and col not in false_positives]
        
        if future_cols:
            print_error(f"–ù–∞–π–¥–µ–Ω—ã –ø—Ä–∏–∑–Ω–∞–∫–∏ —Å –≤–æ–∑–º–æ–∂–Ω–æ–π —É—Ç–µ—á–∫–æ–π: {len(future_cols)}")
            for col in future_cols[:10]:
                print_error(f"   - {col}")
            self.issues.append("future_data_in_features")
        else:
            print_success("Look-ahead bias –Ω–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω –≤ –ø—Ä–∏–∑–Ω–∞–∫–∞—Ö")
            
        # 2. –ü—Ä–æ–≤–µ—Ä–∫–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –∞–±—Å–æ–ª—é—Ç–Ω—ã—Ö —Ü–µ–Ω –≤ —Ü–µ–ª–µ–≤—ã—Ö
        print_info("\n2. –ü—Ä–æ–≤–µ—Ä–∫–∞ –∞–±—Å–æ–ª—é—Ç–Ω—ã—Ö —Ü–µ–Ω –≤ —Ü–µ–ª–µ–≤—ã—Ö –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö...")
        
        price_like_targets = []
        for col in TRADING_TARGET_VARIABLES:
            if col in self.train_data.columns:
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ç–∏–ø –¥–∞–Ω–Ω—ã—Ö - –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã–µ –∫–æ–ª–æ–Ω–∫–∏ –ø—Ä–æ–ø—É—Å–∫–∞–µ–º
                if col in MULTICLASS_TARGETS or col in BINARY_TARGETS:
                    continue  # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã–µ –∏ –±–∏–Ω–∞—Ä–Ω—ã–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ
                    
                # –î–ª—è —á–∏—Å–ª–æ–≤—ã—Ö –∫–æ–ª–æ–Ω–æ–∫ –ø—Ä–æ–≤–µ—Ä—è–µ–º –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ
                if self.train_data[col].dtype in ['float32', 'float64', 'int32', 'int64']:
                    max_val = self.train_data[col].abs().max()
                    if max_val > 1000:  # –ü–æ–¥–æ–∑—Ä–µ–Ω–∏–µ –Ω–∞ –∞–±—Å–æ–ª—é—Ç–Ω—ã–µ —Ü–µ–Ω—ã
                        price_like_targets.append((col, max_val))
                    
        if price_like_targets:
            print_error(f"–¶–µ–ª–µ–≤—ã–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ —Å –ø–æ–¥–æ–∑—Ä–µ–Ω–∏–µ–º –Ω–∞ –∞–±—Å–æ–ª—é—Ç–Ω—ã–µ —Ü–µ–Ω—ã: {len(price_like_targets)}")
            for col, val in price_like_targets:
                print_error(f"   - {col}: max={val:.2f}")
            self.issues.append("absolute_prices_in_targets")
        else:
            print_success("–¶–µ–ª–µ–≤—ã–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –Ω–µ —Å–æ–¥–µ—Ä–∂–∞—Ç –∞–±—Å–æ–ª—é—Ç–Ω—ã—Ö —Ü–µ–Ω")
            
        # 3. –ü—Ä–æ–≤–µ—Ä–∫–∞ –≤—Ä–µ–º–µ–Ω–Ω–æ–≥–æ gap –º–µ–∂–¥—É –≤—ã–±–æ—Ä–∫–∞–º–∏
        print_info("\n3. –ü—Ä–æ–≤–µ—Ä–∫–∞ –≤—Ä–µ–º–µ–Ω–Ω–æ–≥–æ —Ä–∞–∑–¥–µ–ª–µ–Ω–∏—è –≤—ã–±–æ—Ä–æ–∫...")
        
        if self.train_data is not None and self.val_data is not None:
            train_end = pd.to_datetime(self.train_data['datetime']).max()
            val_start = pd.to_datetime(self.val_data['datetime']).min()
            gap_days = (val_start - train_end).days
            
            if gap_days < 1:
                print_error(f"–ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω—ã–π gap –º–µ–∂–¥—É train –∏ val: {gap_days} –¥–Ω–µ–π")
                self.issues.append("insufficient_temporal_gap")
            else:
                print_success(f"–í—Ä–µ–º–µ–Ω–Ω–æ–π gap train‚Üíval: {gap_days} –¥–Ω–µ–π ‚úì")
                
        if self.val_data is not None and self.test_data is not None:
            val_end = pd.to_datetime(self.val_data['datetime']).max()
            test_start = pd.to_datetime(self.test_data['datetime']).min()
            gap_days = (test_start - val_end).days
            
            if gap_days < 1:
                print_error(f"–ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω—ã–π gap –º–µ–∂–¥—É val –∏ test: {gap_days} –¥–Ω–µ–π")
                self.issues.append("insufficient_temporal_gap")
            else:
                print_success(f"–í—Ä–µ–º–µ–Ω–Ω–æ–π gap val‚Üítest: {gap_days} –¥–Ω–µ–π ‚úì")
                
        # 4. –ü—Ä–æ–≤–µ—Ä–∫–∞ –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–π –º–µ–∂–¥—É –ø—Ä–∏–∑–Ω–∞–∫–∞–º–∏ –∏ –±—É–¥—É—â–∏–º–∏ –¥–∞–Ω–Ω—ã–º–∏
        print_info("\n4. –ü—Ä–æ–≤–µ—Ä–∫–∞ –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–π —Å —Ü–µ–ª–µ–≤—ã–º–∏ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–º–∏...")
        
        # –ë–µ—Ä–µ–º —Å–ª—É—á–∞–π–Ω—É—é –≤—ã–±–æ—Ä–∫—É –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è
        sample_size = min(10000, len(self.train_data))
        sample_data = self.train_data.sample(n=sample_size)
        
        # –ò—Å–∫–ª—é—á–∞–µ–º —Å–ª—É–∂–µ–±–Ω—ã–µ –∏ —Ü–µ–ª–µ–≤—ã–µ –∫–æ–ª–æ–Ω–∫–∏
        feature_cols = [col for col in sample_data.columns 
                       if col not in SERVICE_COLUMNS 
                       and col not in TRADING_TARGET_VARIABLES]
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–æ—Ä—Ä–µ–ª—è—Ü–∏—é —Å future_return_4h
        if 'future_return_4h' in sample_data.columns:
            suspicious_features = []
            
            for col in feature_cols[:50]:  # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø–µ—Ä–≤—ã–µ 50 –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
                if sample_data[col].dtype in ['float32', 'float64']:
                    corr = sample_data[col].corr(sample_data['future_return_4h'])
                    if abs(corr) > 0.5:  # –ü–æ–¥–æ–∑—Ä–∏—Ç–µ–ª—å–Ω–æ –≤—ã—Å–æ–∫–∞—è –∫–æ—Ä—Ä–µ–ª—è—Ü–∏—è
                        suspicious_features.append((col, corr))
                        
            if suspicious_features:
                print_warning(f"–ü—Ä–∏–∑–Ω–∞–∫–∏ —Å –≤—ã—Å–æ–∫–æ–π –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–µ–π —Å –±—É–¥—É—â–∏–º–∏ –¥–∞–Ω–Ω—ã–º–∏:")
                for col, corr in sorted(suspicious_features, key=lambda x: abs(x[1]), reverse=True)[:5]:
                    print_warning(f"   - {col}: {corr:.3f}")
                self.warnings.append("high_correlation_with_targets")
            else:
                print_success("–ù–µ—Ç –ø–æ–¥–æ–∑—Ä–∏—Ç–µ–ª—å–Ω—ã—Ö –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–π —Å —Ü–µ–ª–µ–≤—ã–º–∏")
                
    def check_overfitting_signs(self):
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –ø—Ä–∏–∑–Ω–∞–∫–∏ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è"""
        print_header("üîç –ü–†–û–í–ï–†–ö–ê –ù–ê –ü–ï–†–ï–û–ë–£–ß–ï–ù–ò–ï (Overfitting)")
        
        if not all([self.train_data is not None, self.val_data is not None, self.test_data is not None]):
            print_error("–ù–µ –≤—Å–µ –¥–∞—Ç–∞—Å–µ—Ç—ã –∑–∞–≥—Ä—É–∂–µ–Ω—ã!")
            return
            
        # 1. –°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–π –º–µ–∂–¥—É –≤—ã–±–æ—Ä–∫–∞–º–∏
        print_info("\n1. –°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–π train/val/test...")
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–ª—é—á–µ–≤—ã–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
        feature_cols = [col for col in self.train_data.columns 
                       if col not in SERVICE_COLUMNS 
                       and col not in TRADING_TARGET_VARIABLES][:20]
        
        distribution_issues = []
        
        for col in feature_cols:
            if self.train_data[col].dtype in ['float32', 'float64']:
                train_mean = self.train_data[col].mean()
                val_mean = self.val_data[col].mean()
                test_mean = self.test_data[col].mean()
                
                train_std = self.train_data[col].std()
                val_std = self.val_data[col].std()
                test_std = self.test_data[col].std()
                
                # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å–¥–≤–∏–≥–∞ —Å—Ä–µ–¥–Ω–µ–≥–æ
                mean_shift_val = abs(val_mean - train_mean) / (train_std + 1e-8)
                mean_shift_test = abs(test_mean - train_mean) / (train_std + 1e-8)
                
                if mean_shift_val > 2 or mean_shift_test > 2:
                    distribution_issues.append(col)
                    
        if distribution_issues:
            print_warning(f"–û–±–Ω–∞—Ä—É–∂–µ–Ω —Å–¥–≤–∏–≥ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –≤ {len(distribution_issues)} –ø—Ä–∏–∑–Ω–∞–∫–∞—Ö")
            for col in distribution_issues[:5]:
                print_warning(f"   - {col}")
            self.warnings.append("distribution_shift")
        else:
            print_success("–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω—ã –º–µ–∂–¥—É –≤—ã–±–æ—Ä–∫–∞–º–∏")
            
        # 2. –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–∏—Å–±–∞–ª–∞–Ω—Å–∞ –∫–ª–∞—Å—Å–æ–≤ –≤ —Ü–µ–ª–µ–≤—ã—Ö
        print_info("\n2. –ü—Ä–æ–≤–µ—Ä–∫–∞ –±–∞–ª–∞–Ω—Å–∞ —Ü–µ–ª–µ–≤—ã—Ö –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö...")
        
        imbalance_issues = []
        
        for target in BINARY_TARGETS:
            if target in self.train_data.columns:
                train_pos = self.train_data[target].mean()
                val_pos = self.val_data[target].mean() if target in self.val_data.columns else 0
                test_pos = self.test_data[target].mean() if target in self.test_data.columns else 0
                
                if train_pos < 0.05 or train_pos > 0.95:
                    imbalance_issues.append((target, train_pos))
                    
                # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç–∏ –º–µ–∂–¥—É –≤—ã–±–æ—Ä–∫–∞–º–∏
                # –ò–°–ü–†–ê–í–õ–ï–ù–û: –£–≤–µ–ª–∏—á–µ–Ω –ø–æ—Ä–æ–≥ –¥–ª—è –∫—Ä–∏–ø—Ç–æ–≤–∞–ª—é—Ç (20% –≤–º–µ—Å—Ç–æ 10%)
                # –ö—Ä–∏–ø—Ç–æ–≤–∞–ª—é—Ç–Ω—ã–µ —Ä—ã–Ω–∫–∏ –Ω–µ—Å—Ç–∞—Ü–∏–æ–Ω–∞—Ä–Ω—ã –∏ –º–æ–≥—É—Ç –∏–º–µ—Ç—å —Ä–∞–∑–Ω—ã–µ —Ä–µ–∂–∏–º—ã –≤ —Ä–∞–∑–Ω—ã–µ –ø–µ—Ä–∏–æ–¥—ã
                if abs(train_pos - val_pos) > 0.2 or abs(train_pos - test_pos) > 0.2:
                    self.warnings.append(f"target_distribution_shift_{target}")
                    
        if imbalance_issues:
            print_error(f"–°–∏–ª—å–Ω—ã–π –¥–∏—Å–±–∞–ª–∞–Ω—Å –≤ {len(imbalance_issues)} —Ü–µ–ª–µ–≤—ã—Ö –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö:")
            for target, pos_rate in imbalance_issues[:5]:
                print_error(f"   - {target}: {pos_rate*100:.1f}%")
            self.issues.append("severe_class_imbalance")
        else:
            print_success("–ë–∞–ª–∞–Ω—Å —Ü–µ–ª–µ–≤—ã—Ö –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –≤ –Ω–æ—Ä–º–µ")
            
        # 3. –ü—Ä–æ–≤–µ—Ä–∫–∞ —É–Ω–∏–∫–∞–ª—å–Ω–æ—Å—Ç–∏ –¥–∞–Ω–Ω—ã—Ö
        print_info("\n3. –ü—Ä–æ–≤–µ—Ä–∫–∞ —É–Ω–∏–∫–∞–ª—å–Ω–æ—Å—Ç–∏ –∏ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤...")
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤ –ø–æ –≤—Ä–µ–º–µ–Ω–∏ –∏ —Å–∏–º–≤–æ–ª—É
        duplicates = self.train_data.duplicated(subset=['datetime', 'symbol']).sum()
        if duplicates > 0:
            print_error(f"–ù–∞–π–¥–µ–Ω–æ {duplicates} –¥—É–±–ª–∏–∫–∞—Ç–æ–≤ –ø–æ (datetime, symbol)")
            self.issues.append("duplicate_records")
        else:
            print_success("–î—É–±–ª–∏–∫–∞—Ç—ã –Ω–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω—ã")
            
        # 4. –ê–Ω–∞–ª–∏–∑ –≤—Ä–µ–º–µ–Ω–Ω–æ–π —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç–∏
        print_info("\n4. –ü—Ä–æ–≤–µ—Ä–∫–∞ –≤—Ä–µ–º–µ–Ω–Ω–æ–π —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç–∏...")
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–µ—Ç –ª–∏ –ø—Ä–æ–ø—É—Å–∫–æ–≤ –≤–æ –≤—Ä–µ–º–µ–Ω–∏
        for symbol in self.train_data['symbol'].unique()[:5]:  # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø–µ—Ä–≤—ã–µ 5 —Å–∏–º–≤–æ–ª–æ–≤
            symbol_data = self.train_data[self.train_data['symbol'] == symbol].sort_values('datetime')
            time_diffs = pd.to_datetime(symbol_data['datetime']).diff()
            
            # –û–∂–∏–¥–∞–µ–º 15-–º–∏–Ω—É—Ç–Ω—ã–µ –∏–Ω—Ç–µ—Ä–≤–∞–ª—ã
            expected_diff = pd.Timedelta(minutes=15)
            gaps = time_diffs[time_diffs > expected_diff * 2]
            
            if len(gaps) > 10:
                print_warning(f"–°–∏–º–≤–æ–ª {symbol}: {len(gaps)} –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –ø—Ä–æ–ø—É—Å–∫–æ–≤")
                
    def check_target_variables_v4(self):
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ—Å—Ç–∏ —Ü–µ–ª–µ–≤—ã—Ö –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –≤–µ—Ä—Å–∏–∏ 4.0 –ë–ï–ó –£–¢–ï–ß–ï–ö"""
        print_header("üéØ –ü–†–û–í–ï–†–ö–ê –¶–ï–õ–ï–í–´–• –ü–ï–†–ï–ú–ï–ù–ù–´–• v4.0 (–ë–ï–ó –£–¢–ï–ß–ï–ö)")
        
        if self.train_data is None:
            print_error("–î–∞–Ω–Ω—ã–µ –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω—ã!")
            return
            
        print_info(f"–û–∂–∏–¥–∞–µ—Ç—Å—è {len(TRADING_TARGET_VARIABLES)} —Ü–µ–ª–µ–≤—ã—Ö –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö")
        
        # 1. –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞–ª–∏—á–∏—è –≤—Å–µ—Ö —Ü–µ–ª–µ–≤—ã—Ö
        missing_targets = [t for t in TRADING_TARGET_VARIABLES if t not in self.train_data.columns]
        if missing_targets:
            print_error(f"–û—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç —Ü–µ–ª–µ–≤—ã–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ: {len(missing_targets)}")
            for target in missing_targets:
                print_error(f"   - {target}")
            self.issues.append("missing_target_variables")
        else:
            print_success("–í—Å–µ 20 —Ü–µ–ª–µ–≤—ã—Ö –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –ø—Ä–∏—Å—É—Ç—Å—Ç–≤—É—é—Ç (–±–µ–∑ —É—Ç–µ—á–µ–∫)")
            
        # 2. –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–∏–∞–ø–∞–∑–æ–Ω–æ–≤ –∑–Ω–∞—á–µ–Ω–∏–π
        print_info("\n2. –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–∏–∞–ø–∞–∑–æ–Ω–æ–≤ —Ü–µ–ª–µ–≤—ã—Ö –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö...")
        
        for group_name, targets in TARGET_GROUPS.items():
            print_info(f"\n{group_name}:")
            
            for target in targets:
                if target in self.train_data.columns:
                    if target in REGRESSION_TARGETS:
                        # –î–ª—è —Ä–µ–≥—Ä–µ—Å—Å–∏–æ–Ω–Ω—ã—Ö —Ü–µ–ª–µ–≤—ã—Ö
                        stats = self.train_data[target].describe()
                        
                        if 'return' in target:
                            # –í–æ–∑–≤—Ä–∞—Ç—ã –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å –≤ —Ä–∞–∑—É–º–Ω—ã—Ö –ø—Ä–µ–¥–µ–ª–∞—Ö
                            if abs(stats['mean']) > 0.1 or stats['max'] > 1.0:
                                print_warning(f"   {target}: mean={stats['mean']:.4f}, max={stats['max']:.4f}")
                            else:
                                print_success(f"   {target}: mean={stats['mean']:.4f}, std={stats['std']:.4f} ‚úì")
                                
                    elif target in BINARY_TARGETS:
                        # –î–ª—è –±–∏–Ω–∞—Ä–Ω—ã—Ö —Ü–µ–ª–µ–≤—ã—Ö
                        pos_rate = self.train_data[target].mean() * 100
                        print_info(f"   {target}: {pos_rate:.1f}% –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã—Ö")
                        
                    elif target in MULTICLASS_TARGETS:
                        # –î–ª—è –º–Ω–æ–≥–æ–∫–ª–∞—Å—Å–æ–≤—ã—Ö
                        dist = self.train_data[target].value_counts(normalize=True) * 100
                        print_info(f"   {target}: {dict(dist.head(3))}")
                        
        # 3. –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å–∏–º–º–µ—Ç—Ä–∏—á–Ω–æ—Å—Ç–∏ LONG/SHORT
        print_info("\n3. –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å–∏–º–º–µ—Ç—Ä–∏—á–Ω–æ—Å—Ç–∏ LONG/SHORT —Ü–µ–ª–µ–≤—ã—Ö...")
        
        for level in ['1pct_4h', '2pct_4h', '3pct_12h', '5pct_12h']:
            long_target = f'long_will_reach_{level}'
            short_target = f'short_will_reach_{level}'
            
            if long_target in self.train_data.columns and short_target in self.train_data.columns:
                long_rate = self.train_data[long_target].mean() * 100
                short_rate = self.train_data[short_target].mean() * 100
                
                diff = abs(long_rate - short_rate)
                if diff > 20:
                    print_warning(f"   {level}: LONG={long_rate:.1f}%, SHORT={short_rate:.1f}% (—Ä–∞–∑–Ω–∏—Ü–∞ {diff:.1f}%)")
                else:
                    print_success(f"   {level}: LONG={long_rate:.1f}%, SHORT={short_rate:.1f}% ‚úì")
                    
    def check_data_quality(self):
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞ –¥–∞–Ω–Ω—ã—Ö"""
        print_header("üìä –ü–†–û–í–ï–†–ö–ê –ö–ê–ß–ï–°–¢–í–ê –î–ê–ù–ù–´–•")
        
        if self.train_data is None:
            print_error("–î–∞–Ω–Ω—ã–µ –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω—ã!")
            return
            
        # 1. –ü—Ä–æ–ø—É—â–µ–Ω–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è
        print_info("\n1. –ê–Ω–∞–ª–∏–∑ –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π...")
        
        nan_stats = self.train_data.isnull().sum()
        nan_cols = nan_stats[nan_stats > 0].sort_values(ascending=False)
        
        if len(nan_cols) > 0:
            total_nan = nan_cols.sum()
            total_cells = len(self.train_data) * len(self.train_data.columns)
            nan_pct = total_nan / total_cells * 100
            
            print_warning(f"–ü—Ä–æ–ø—É—â–µ–Ω–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è –≤ {len(nan_cols)} –∫–æ–ª–æ–Ω–∫–∞—Ö ({nan_pct:.2f}% –æ—Ç –≤—Å–µ—Ö –¥–∞–Ω–Ω—ã—Ö)")
            
            for col, count in nan_cols.head(10).items():
                pct = count / len(self.train_data) * 100
                print_warning(f"   - {col}: {count:,} ({pct:.1f}%)")
                
            if nan_pct > 5:
                self.issues.append("high_missing_values")
        else:
            print_success("–ü—Ä–æ–ø—É—â–µ–Ω–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç")
            
        # 2. –í—ã–±—Ä–æ—Å—ã
        print_info("\n2. –ê–Ω–∞–ª–∏–∑ –≤—ã–±—Ä–æ—Å–æ–≤...")
        
        feature_cols = [col for col in self.train_data.columns 
                       if col not in SERVICE_COLUMNS 
                       and col not in TRADING_TARGET_VARIABLES][:30]
        
        outlier_cols = []
        for col in feature_cols:
            if self.train_data[col].dtype in ['float32', 'float64']:
                # Z-score –º–µ—Ç–æ–¥
                z_scores = np.abs(stats.zscore(self.train_data[col].dropna()))
                outliers = (z_scores > 5).sum()
                
                if outliers > len(self.train_data) * 0.01:  # –ë–æ–ª–µ–µ 1% –≤—ã–±—Ä–æ—Å–æ–≤
                    outlier_cols.append((col, outliers))
                    
        if outlier_cols:
            print_warning(f"–û–±–Ω–∞—Ä—É–∂–µ–Ω—ã –≤—ã–±—Ä–æ—Å—ã –≤ {len(outlier_cols)} –ø—Ä–∏–∑–Ω–∞–∫–∞—Ö:")
            for col, count in outlier_cols[:5]:
                pct = count / len(self.train_data) * 100
                print_warning(f"   - {col}: {count} –≤—ã–±—Ä–æ—Å–æ–≤ ({pct:.1f}%)")
        else:
            print_success("–í—ã–±—Ä–æ—Å—ã –≤ –¥–æ–ø—É—Å—Ç–∏–º—ã—Ö –ø—Ä–µ–¥–µ–ª–∞—Ö")
            
        # 3. –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–∏–∞–ø–∞–∑–æ–Ω–æ–≤ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏—Ö –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–æ–≤
        print_info("\n3. –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–∏–∞–ø–∞–∑–æ–Ω–æ–≤ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏—Ö –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–æ–≤...")
        
        # RSI –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –≤ [0, 100]
        if 'rsi' in self.train_data.columns:
            rsi_stats = self.train_data['rsi'].describe()
            if rsi_stats['min'] < -1 or rsi_stats['max'] > 101:
                print_error(f"RSI –≤–Ω–µ –¥–∏–∞–ø–∞–∑–æ–Ω–∞: [{rsi_stats['min']:.1f}, {rsi_stats['max']:.1f}]")
                self.issues.append("rsi_out_of_range")
            else:
                print_success(f"RSI –≤ –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ–º –¥–∏–∞–ø–∞–∑–æ–Ω–µ: [{rsi_stats['min']:.1f}, {rsi_stats['max']:.1f}]")
                
        # Stochastic –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –≤ [0, 100]
        for indicator in ['stoch_k', 'stoch_d']:
            if indicator in self.train_data.columns:
                stats_ind = self.train_data[indicator].describe()
                if stats_ind['min'] < -1 or stats_ind['max'] > 101:
                    print_error(f"{indicator} –≤–Ω–µ –¥–∏–∞–ø–∞–∑–æ–Ω–∞: [{stats_ind['min']:.1f}, {stats_ind['max']:.1f}]")
                    self.issues.append(f"{indicator}_out_of_range")
                    
    def generate_report(self):
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∏—Ç–æ–≥–æ–≤–æ–≥–æ –æ—Ç—á–µ—Ç–∞"""
        print_header("üìã –ò–¢–û–ì–û–í–´–ô –û–¢–ß–ï–¢")
        
        # –ü–æ–¥—Å—á–µ—Ç –ø—Ä–æ–±–ª–µ–º
        total_issues = len(self.issues)
        total_warnings = len(self.warnings)
        
        if total_issues == 0 and total_warnings == 0:
            print_success("‚úÖ –î–ê–ù–ù–´–ï –ü–û–õ–ù–û–°–¢–¨–Æ –ö–û–†–†–ï–ö–¢–ù–´!")
            print_success("üöÄ –°–∏—Å—Ç–µ–º–∞ –≥–æ—Ç–æ–≤–∞ –∫ –æ–±—É—á–µ–Ω–∏—é")
            print_info("\n–ó–∞–ø—É—Å—Ç–∏—Ç–µ –æ–±—É—á–µ–Ω–∏–µ:")
            print(f"   {Colors.OKBLUE}python main.py --mode train{Colors.ENDC}")
            
        else:
            if total_issues > 0:
                print_error(f"‚ùå –û–±–Ω–∞—Ä—É–∂–µ–Ω–æ –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏—Ö –ø—Ä–æ–±–ª–µ–º: {total_issues}")
                print("\n–ö—Ä–∏—Ç–∏—á–µ—Å–∫–∏–µ –ø—Ä–æ–±–ª–µ–º—ã:")
                for issue in set(self.issues):
                    print_error(f"   - {issue}")
                    
            if total_warnings > 0:
                print_warning(f"‚ö†Ô∏è  –û–±–Ω–∞—Ä—É–∂–µ–Ω–æ –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–π: {total_warnings}")
                print("\n–ü—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏—è:")
                for warning in list(set(self.warnings))[:10]:
                    print_warning(f"   - {warning}")
                    
            # –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏
            print(f"\n{Colors.WARNING}{Colors.BOLD}üîß –†–ï–ö–û–ú–ï–ù–î–ê–¶–ò–ò:{Colors.ENDC}")
            
            if 'future_data_in_features' in self.issues:
                print_info("1. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ feature_engineering.py –Ω–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –±—É–¥—É—â–∏—Ö –¥–∞–Ω–Ω—ã—Ö")
                
            if 'absolute_prices_in_targets' in self.issues:
                print_info("2. –£–±–µ–¥–∏—Ç–µ—Å—å —á—Ç–æ —Ü–µ–ª–µ–≤—ã–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ - —ç—Ç–æ –ø—Ä–æ—Ü–µ–Ω—Ç—ã, –∞ –Ω–µ —Ü–µ–Ω—ã")
                
            if 'insufficient_temporal_gap' in self.issues:
                print_info("3. –£–≤–µ–ª–∏—á—å—Ç–µ gap –º–µ–∂–¥—É –≤—ã–±–æ—Ä–∫–∞–º–∏ –≤ prepare_trading_data.py")
                
            if 'severe_class_imbalance' in self.issues:
                print_info("4. –†–∞—Å—Å–º–æ—Ç—Ä–∏—Ç–µ –±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫—É –∫–ª–∞—Å—Å–æ–≤ –∏–ª–∏ –∏–∑–º–µ–Ω–µ–Ω–∏–µ –ø–æ—Ä–æ–≥–æ–≤")
                
            print_info("\n–î–ª—è –ø–µ—Ä–µ—Å–æ–∑–¥–∞–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö:")
            print(f"   {Colors.OKBLUE}rm -rf cache/features/*{Colors.ENDC}")
            print(f"   {Colors.OKBLUE}python prepare_trading_data.py --force-recreate{Colors.ENDC}")
            
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –¥–µ—Ç–∞–ª—å–Ω–æ–≥–æ –æ—Ç—á–µ—Ç–∞
        self._save_detailed_report()
        
    def _save_detailed_report(self):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –¥–µ—Ç–∞–ª—å–Ω–æ–≥–æ –æ—Ç—á–µ—Ç–∞ –≤ —Ñ–∞–π–ª"""
        report_path = Path('logs/data_validation_v4_report.txt')
        report_path.parent.mkdir(exist_ok=True)
        
        with open(report_path, 'w', encoding='utf-8') as f:
            f.write("="*80 + "\n")
            f.write("–î–ï–¢–ê–õ–¨–ù–´–ô –û–¢–ß–ï–¢ –í–ê–õ–ò–î–ê–¶–ò–ò –î–ê–ù–ù–´–• v4.0 (–ë–ï–ó –£–¢–ï–ß–ï–ö)\n")
            f.write("="*80 + "\n")
            f.write(f"–î–∞—Ç–∞: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")
            
            # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –¥–∞—Ç–∞—Å–µ—Ç–∞–º
            for name in ['train', 'val', 'test']:
                data = getattr(self, f"{name}_data")
                if data is not None:
                    f.write(f"\n{name.upper()} DATASET:\n")
                    f.write(f"  –†–∞–∑–º–µ—Ä: {len(data):,} –∑–∞–ø–∏—Å–µ–π\n")
                    f.write(f"  –ü–µ—Ä–∏–æ–¥: {data['datetime'].min()} - {data['datetime'].max()}\n")
                    f.write(f"  –°–∏–º–≤–æ–ª–æ–≤: {data['symbol'].nunique()}\n")
                    f.write(f"  –ü—Ä–∏–∑–Ω–∞–∫–æ–≤: {len(data.columns)}\n")
                    
            # –ü—Ä–æ–±–ª–µ–º—ã
            f.write(f"\n–û–ë–ù–ê–†–£–ñ–ï–ù–ù–´–ï –ü–†–û–ë–õ–ï–ú–´:\n")
            f.write(f"  –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∏—Ö: {len(self.issues)}\n")
            f.write(f"  –ü—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–π: {len(self.warnings)}\n")
            
            if self.issues:
                f.write("\n–ö–†–ò–¢–ò–ß–ï–°–ö–ò–ï –ü–†–û–ë–õ–ï–ú–´:\n")
                for issue in set(self.issues):
                    f.write(f"  - {issue}\n")
                    
            if self.warnings:
                f.write("\n–ü–†–ï–î–£–ü–†–ï–ñ–î–ï–ù–ò–Ø:\n")
                for warning in set(self.warnings):
                    f.write(f"  - {warning}\n")
                    
        print_info(f"\nüìÑ –î–µ—Ç–∞–ª—å–Ω—ã–π –æ—Ç—á–µ—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {report_path}")


def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è"""
    verifier = DataCorrectnessVerifier()
    
    # –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö
    verifier.load_data()
    
    # –í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –≤—Å–µ—Ö –ø—Ä–æ–≤–µ—Ä–æ–∫
    verifier.check_data_leakage()
    verifier.check_overfitting_signs()
    verifier.check_target_variables_v4()
    verifier.check_data_quality()
    
    # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç—á–µ—Ç–∞
    verifier.generate_report()


if __name__ == "__main__":
    main()