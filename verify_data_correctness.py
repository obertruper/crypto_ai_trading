#!/usr/bin/env python3
"""
–ö–æ–º–ø–∞–∫—Ç–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ—Å—Ç–∏ –¥–∞–Ω–Ω—ã—Ö - —Ç–æ–ª—å–∫–æ —Å–∞–º–æ–µ –≤–∞–∂–Ω–æ–µ
"""

import pandas as pd
import numpy as np
from pathlib import Path
from datetime import datetime
import warnings
warnings.filterwarnings('ignore')

# –¶–≤–µ—Ç–Ω–æ–π –≤—ã–≤–æ–¥
class Colors:
    HEADER = '\033[95m'
    OKBLUE = '\033[94m'
    OKGREEN = '\033[92m'
    WARNING = '\033[93m'
    FAIL = '\033[91m'
    ENDC = '\033[0m'
    BOLD = '\033[1m'

def print_header(text):
    print(f"\n{Colors.HEADER}{Colors.BOLD}{'='*60}{Colors.ENDC}")
    print(f"{Colors.HEADER}{Colors.BOLD}{text}{Colors.ENDC}")
    print(f"{Colors.HEADER}{Colors.BOLD}{'='*60}{Colors.ENDC}")

def print_success(text):
    print(f"{Colors.OKGREEN}‚úÖ {text}{Colors.ENDC}")

def print_warning(text):
    print(f"{Colors.WARNING}‚ö†Ô∏è  {text}{Colors.ENDC}")

def print_error(text):
    print(f"{Colors.FAIL}‚ùå {text}{Colors.ENDC}")

def print_info(text):
    print(f"{Colors.OKBLUE}‚ÑπÔ∏è  {text}{Colors.ENDC}")

def check_critical_indicators(df, name):
    """–ü—Ä–æ–≤–µ—Ä–∫–∞ —Ç–æ–ª—å–∫–æ –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏—Ö –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–æ–≤"""
    print(f"\n{Colors.BOLD}üîç –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∏–µ –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã {name}:{Colors.ENDC}")
    
    issues = []
    
    # 1. TOXICITY - —Å–∞–º—ã–π –≤–∞–∂–Ω—ã–π –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä
    if 'toxicity' in df.columns:
        stats = df['toxicity'].describe()
        mean = stats['mean']
        std = stats['std']
        
        if mean > 0.99 and std < 0.02:
            print_error(f"toxicity: mean={mean:.6f}, std={std:.6f}")
            print_error("   üö® –ö–†–ò–¢–ò–ß–ù–û: toxicity –≤—Å–µ–≥–¥–∞ ‚âà1.0 (–±–µ—Å–ø–æ–ª–µ–∑–Ω—ã–π –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä!)")
            issues.append("toxicity_broken")
        elif 0.4 <= mean <= 0.7 and std > 0.15:
            print_success(f"toxicity: mean={mean:.4f}, std={std:.4f} ‚úì (–∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ)")
        else:
            print_warning(f"toxicity: mean={mean:.4f}, std={std:.4f}")
            if std < 0.1:
                print_warning("   ‚ö†Ô∏è –ù–∏–∑–∫–∞—è –≤–∞—Ä–∏–∞—Ç–∏–≤–Ω–æ—Å—Ç—å toxicity")
    
    # 2. PRICE_IMPACT
    if 'price_impact' in df.columns:
        stats = df['price_impact'].describe()
        mean = stats['mean']
        
        if mean < 0.0001:
            print_error(f"price_impact: mean={mean:.6f} (—Å–ª–∏—à–∫–æ–º –º–∞–ª!)")
            issues.append("price_impact_too_small")
        elif 0.001 <= mean <= 0.1:
            print_success(f"price_impact: mean={mean:.4f} ‚úì")
        else:
            print_warning(f"price_impact: mean={mean:.4f} (–Ω–µ–æ–±—ã—á–Ω–æ –≤—ã—Å–æ–∫–∏–π)")
    
    # 3. RSI - –ø—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—é
    if 'rsi' in df.columns:
        stats = df['rsi'].describe()
        mean = stats['mean']
        std = stats['std']
        
        # –ü—Ä–∏–∑–Ω–∞–∫–∏ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏
        if abs(mean) < 1.0 and 0.8 < std < 1.2:
            print_error(f"rsi: –ù–û–†–ú–ê–õ–ò–ó–û–í–ê–ù! mean={mean:.3f}, std={std:.3f}")
            issues.append("rsi_normalized")
        elif 40 <= mean <= 60 and std > 10:
            print_success(f"rsi: mean={mean:.1f}, std={std:.1f} ‚úì")
        else:
            print_warning(f"rsi: mean={mean:.1f}, std={std:.1f}")
    
    # 4. Stochastic
    for indicator in ['stoch_k', 'stoch_d']:
        if indicator in df.columns:
            stats = df[indicator].describe()
            mean = stats['mean']
            std = stats['std']
            
            if abs(mean) < 1.0 and 0.8 < std < 1.2:
                print_error(f"{indicator}: –ù–û–†–ú–ê–õ–ò–ó–û–í–ê–ù! mean={mean:.3f}, std={std:.3f}")
                issues.append(f"{indicator}_normalized")
            elif 30 <= mean <= 70 and std > 15:
                print_success(f"{indicator}: mean={mean:.1f}, std={std:.1f} ‚úì")
    
    return issues

def check_target_distribution(df, name):
    """–ü—Ä–æ–≤–µ—Ä–∫–∞ —Ü–µ–ª–µ–≤—ã—Ö –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö"""
    print(f"\n{Colors.BOLD}üéØ –¶–µ–ª–µ–≤—ã–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ {name}:{Colors.ENDC}")
    
    # TP/SL –¥–æ—Å—Ç–∏–∂–µ–Ω–∏—è
    for target in ['long_tp1_reached', 'short_tp1_reached']:
        if target in df.columns:
            pct = df[target].mean() * 100
            if 30 <= pct <= 70:
                print_info(f"{target}: {pct:.1f}% ‚úì")
            else:
                print_warning(f"{target}: {pct:.1f}%")
    
    # –ù–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–µ
    if 'best_direction' in df.columns:
        dist = df['best_direction'].value_counts(normalize=True) * 100
        print_info(f"best_direction: LONG={dist.get('LONG', 0):.1f}%, SHORT={dist.get('SHORT', 0):.1f}%, NEUTRAL={dist.get('NEUTRAL', 0):.1f}%")

def check_data_quality(df, name):
    """–ë–∞–∑–æ–≤–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞"""
    issues = []
    
    # NaN –ø—Ä–æ–≤–µ—Ä–∫–∞
    nan_cols = df.isna().sum()
    nan_cols = nan_cols[nan_cols > 0]
    if len(nan_cols) > 0:
        worst_col = nan_cols.idxmax()
        worst_pct = nan_cols.max() / len(df) * 100
        print_warning(f"NaN –≤ {len(nan_cols)} –∫–æ–ª–æ–Ω–∫–∞—Ö (—Ö—É–¥—à–∞—è: {worst_col} = {worst_pct:.1f}%)")
        if worst_pct > 5:
            issues.append("high_nan")
    
    return issues

def check_ml_readiness(df, name):
    """–ü—Ä–æ–≤–µ—Ä–∫–∞ –≥–æ—Ç–æ–≤–Ω–æ—Å—Ç–∏ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è ML"""
    print(f"\n{Colors.BOLD}ü§ñ –ü—Ä–æ–≤–µ—Ä–∫–∏ –¥–ª—è ML {name}:{Colors.ENDC}")
    
    issues = []
    
    # –ò—Å–∫–ª—é—á–∞–µ–º —Å–ª—É–∂–µ–±–Ω—ã–µ –∏ —Ü–µ–ª–µ–≤—ã–µ –∫–æ–ª–æ–Ω–∫–∏
    exclude_cols = ['id', 'symbol', 'datetime', 'timestamp', 'sector']
    target_prefixes = ('target_', 'future_', 'long_', 'short_', 'best_')
    
    # –ò–°–ü–†–ê–í–õ–ï–ù–û: –ò—Å–∫–ª—é—á–∞–µ–º –∏—Å—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ (—Ü–µ–Ω—ã, –æ–±—ä–µ–º—ã) –∏–∑ –ø—Ä–æ–≤–µ—Ä–∫–∏ —ç–∫—Å—Ç—Ä–µ–º–∞–ª—å–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π
    # –≠—Ç–∏ –∫–æ–ª–æ–Ω–∫–∏ –º–æ–≥—É—Ç –∏–º–µ—Ç—å –±–æ–ª—å—à–∏–µ –∑–Ω–∞—á–µ–Ω–∏—è –∏ —ç—Ç–æ –Ω–æ—Ä–º–∞–ª—å–Ω–æ
    raw_data_cols = [
        'open', 'high', 'low', 'close', 'volume', 'turnover',
        'vwap', 'btc_close', 'dollar_volume', 'directed_volume',
        # –°–∫–æ–ª—å–∑—è—â–∏–µ —Å—Ä–µ–¥–Ω–∏–µ –∏ –¥—Ä—É–≥–∏–µ —Ü–µ–Ω–æ–≤—ã–µ –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã
        'sma_10', 'sma_20', 'sma_50', 'ema_10', 'ema_20', 'ema_50',
        'bb_high', 'bb_low', 'bb_middle', 'psar',
        # EMA –¥–ª—è MACD (–º–æ–≥—É—Ç –±—ã—Ç—å –±–æ–ª—å—à–∏–µ –¥–ª—è –¥–æ—Ä–æ–≥–∏—Ö –∞–∫—Ç–∏–≤–æ–≤)
        'ema_12', 'ema_26',
        # –õ–æ–∫–∞–ª—å–Ω—ã–µ —ç–∫—Å—Ç—Ä–µ–º—É–º—ã
        'local_high_20', 'local_high_50', 'local_high_100',
        'local_low_20', 'local_low_50', 'local_low_100',
        'daily_high', 'daily_low',
        # –õ–∏–∫–≤–∏–¥–∞—Ü–∏–æ–Ω–Ω—ã–µ —Ü–µ–Ω—ã
        'long_liquidation_price', 'short_liquidation_price',
        # ATR –≤ –∞–±—Å–æ–ª—é—Ç–Ω—ã—Ö —Ü–µ–Ω–∞—Ö (–Ω–µ –≤ –ø—Ä–æ—Ü–µ–Ω—Ç–∞—Ö)
        'atr',
        # –¢—Ä–µ–Ω–¥—ã –≤ –∞–±—Å–æ–ª—é—Ç–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏—è—Ö
        'trend_1h', 'trend_4h'
    ]
    
    feature_cols = [col for col in df.columns 
                   if col not in exclude_cols
                   and not col.startswith(target_prefixes)
                   and col not in raw_data_cols]
    
    # 1. –ü—Ä–æ–≤–µ—Ä–∫–∞ —ç–∫—Å—Ç—Ä–µ–º–∞–ª—å–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π (–∏—Å–∫–ª—é—á–∞—è —Ü–µ–Ω—ã –∏ –æ–±—ä–µ–º—ã)
    extreme_cols = []
    for col in feature_cols:
        if df[col].dtype in ['float32', 'float64', 'int32', 'int64']:
            max_val = df[col].abs().max()
            if max_val > 1000:
                extreme_cols.append((col, max_val))
    
    if extreme_cols:
        print_error(f"–≠–∫—Å—Ç—Ä–µ–º–∞–ª—å–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è (>1000) –≤ {len(extreme_cols)} –ø—Ä–∏–∑–Ω–∞–∫–∞—Ö!")
        for col, val in sorted(extreme_cols, key=lambda x: x[1], reverse=True)[:5]:
            print_error(f"   - {col}: {val:.2e}")
        issues.append("extreme_values")
    else:
        print_success("–ù–µ—Ç —ç–∫—Å—Ç—Ä–µ–º–∞–ª—å–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π –≤ –ø—Ä–∏–∑–Ω–∞–∫–∞—Ö (–∏—Å–∫–ª—é—á–∞—è —Ü–µ–Ω—ã/–æ–±—ä–µ–º—ã)")
    
    # 2. –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
    zero_variance_cols = []
    for col in feature_cols[:50]:  # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø–µ—Ä–≤—ã–µ 50
        std = df[col].std()
        if std < 1e-6:
            zero_variance_cols.append(col)
    
    if zero_variance_cols:
        print_warning(f"–ù—É–ª–µ–≤–∞—è –¥–∏—Å–ø–µ—Ä—Å–∏—è –≤ {len(zero_variance_cols)} –∫–æ–ª–æ–Ω–∫–∞—Ö")
        issues.append("zero_variance")
    else:
        print_success("–í—Å–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ –∏–º–µ—é—Ç –¥–æ—Å—Ç–∞—Ç–æ—á–Ω—É—é –¥–∏—Å–ø–µ—Ä—Å–∏—é")
    
    # 3. –ü—Ä–æ–≤–µ—Ä–∫–∞ –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–π –º–µ–∂–¥—É –ø—Ä–∏–∑–Ω–∞–∫–∞–º–∏
    if len(feature_cols) > 10:
        corr_matrix = df[feature_cols[:20]].corr().abs()
        high_corr_pairs = []
        for i in range(len(corr_matrix)):
            for j in range(i+1, len(corr_matrix)):
                if corr_matrix.iloc[i, j] > 0.95:
                    high_corr_pairs.append((corr_matrix.index[i], corr_matrix.columns[j], corr_matrix.iloc[i, j]))
        
        if high_corr_pairs:
            print_warning(f"–í—ã—Å–æ–∫–∞—è –∫–æ—Ä—Ä–µ–ª—è—Ü–∏—è (>0.95) –º–µ–∂–¥—É {len(high_corr_pairs)} –ø–∞—Ä–∞–º–∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤")
        else:
            print_success("–ù–µ—Ç —Å–∏–ª—å–Ω–æ –∫–æ—Ä—Ä–µ–ª–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤")
    
    # 4. –ü—Ä–æ–≤–µ—Ä–∫–∞ –±–∞–ª–∞–Ω—Å–∞ —Ü–µ–ª–µ–≤—ã—Ö –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö
    for direction in ['long', 'short']:
        tp1_col = f'{direction}_tp1_reached'
        if tp1_col in df.columns:
            positive_pct = df[tp1_col].mean() * 100
            if positive_pct < 5 or positive_pct > 95:
                print_error(f"{tp1_col}: {positive_pct:.1f}% - —Å–∏–ª—å–Ω—ã–π –¥–∏—Å–±–∞–ª–∞–Ω—Å!")
                issues.append("target_imbalance")
    
    # 5. –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ç–∏–ø–æ–≤ –¥–∞–Ω–Ω—ã—Ö
    non_numeric = []
    for col in feature_cols:
        if df[col].dtype == 'object':
            non_numeric.append(col)
    
    if non_numeric:
        print_error(f"–ù–µ —á–∏—Å–ª–æ–≤—ã–µ —Ç–∏–ø—ã –≤ {len(non_numeric)} –∫–æ–ª–æ–Ω–∫–∞—Ö: {non_numeric[:5]}")
        issues.append("non_numeric_features")
    else:
        print_success("–í—Å–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ –∏–º–µ—é—Ç —á–∏—Å–ª–æ–≤–æ–π —Ç–∏–ø")
    
    # 6. –ü—Ä–æ–≤–µ—Ä–∫–∞ inf –∑–Ω–∞—á–µ–Ω–∏–π
    inf_cols = []
    for col in feature_cols:
        if np.isinf(df[col]).any():
            inf_cols.append(col)
    
    if inf_cols:
        print_error(f"–ë–µ—Å–∫–æ–Ω–µ—á–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è –≤ {len(inf_cols)} –∫–æ–ª–æ–Ω–∫–∞—Ö: {inf_cols[:5]}")
        issues.append("inf_values")
    else:
        print_success("–ù–µ—Ç –±–µ—Å–∫–æ–Ω–µ—á–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π")
    
    return issues

def main():
    """–ö–æ–º–ø–∞–∫—Ç–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –¥–∞–Ω–Ω—ã—Ö"""
    print_header("üìä –ü–†–û–í–ï–†–ö–ê –ö–û–†–†–ï–ö–¢–ù–û–°–¢–ò –î–ê–ù–ù–´–•")
    print(f"‚è∞ {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    
    files = ['train_data.parquet', 'val_data.parquet', 'test_data.parquet']
    all_issues = []
    
    for file in files:
        path = Path(f'data/processed/{file}')
        if not path.exists():
            print_error(f"\n‚ùå {file} –ù–ï –ù–ê–ô–î–ï–ù!")
            all_issues.append("file_missing")
            continue
            
        print(f"\n{Colors.OKBLUE}{Colors.BOLD}{'='*50}{Colors.ENDC}")
        print(f"{Colors.OKBLUE}{Colors.BOLD}üìÅ {file.upper()}{Colors.ENDC}")
        print(f"{Colors.OKBLUE}{Colors.BOLD}{'='*50}{Colors.ENDC}")
        
        df = pd.read_parquet(path)
        print_info(f"–†–∞–∑–º–µ—Ä: {len(df):,} –∑–∞–ø–∏—Å–µ–π, {len(df.columns)} –∫–æ–ª–æ–Ω–æ–∫")
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∏
        dataset_name = file.split('_')[0].upper()
        issues = []
        
        issues.extend(check_critical_indicators(df, dataset_name))
        issues.extend(check_data_quality(df, dataset_name))
        check_target_distribution(df, dataset_name)
        issues.extend(check_ml_readiness(df, dataset_name))
        
        # –ò—Ç–æ–≥ –ø–æ —Ñ–∞–π–ª—É
        if not issues:
            print_success(f"\n‚úÖ {dataset_name} - –¥–∞–Ω–Ω—ã–µ –∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã!")
        else:
            print_error(f"\n‚ùå {dataset_name} - –æ–±–Ω–∞—Ä—É–∂–µ–Ω–æ {len(issues)} –ø—Ä–æ–±–ª–µ–º!")
            all_issues.extend(issues)
    
    # –§–ò–ù–ê–õ–¨–ù–´–ô –ò–¢–û–ì
    print_header("üìã –ò–¢–û–ì–û–í–´–ô –†–ï–ó–£–õ–¨–¢–ê–¢")
    
    if not all_issues:
        print_success("‚úÖ –í–°–ï –î–ê–ù–ù–´–ï –ì–û–¢–û–í–´ –ö –û–ë–£–ß–ï–ù–ò–Æ!")
        print_success("\nüöÄ –ó–∞–ø—É—Å–∫–∞–π—Ç–µ: python main.py --mode train")
    else:
        unique_issues = set(all_issues)
        print_error(f"‚ùå –û–ë–ù–ê–†–£–ñ–ï–ù–û –ü–†–û–ë–õ–ï–ú: {len(unique_issues)}")
        
        # –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∏–µ –ø—Ä–æ–±–ª–µ–º—ã
        if 'toxicity_broken' in unique_issues:
            print_error("\nüî• –ö–†–ò–¢–ò–ß–ù–û: –ò–Ω–¥–∏–∫–∞—Ç–æ—Ä toxicity –Ω–µ —Ä–∞–±–æ—Ç–∞–µ—Ç!")
            print_warning("   –§–æ—Ä–º—É–ª–∞ –≤ feature_engineering.py —É–∂–µ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∞")
            print_warning("   –ù—É–∂–Ω–æ –ø–µ—Ä–µ—Å–æ–∑–¥–∞—Ç—å –¥–∞–Ω–Ω—ã–µ")
        
        if any('normalized' in issue for issue in unique_issues):
            print_error("\nüî• –ö–†–ò–¢–ò–ß–ù–û: –û–±–Ω–∞—Ä—É–∂–µ–Ω–∞ –Ω–µ–∂–µ–ª–∞—Ç–µ–ª—å–Ω–∞—è –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è!")
            print_warning("   –¢–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã –Ω–µ –¥–æ–ª–∂–Ω—ã –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞—Ç—å—Å—è")
        
        if 'extreme_values' in unique_issues:
            print_error("\nüî• –ö–†–ò–¢–ò–ß–ù–û: –≠–∫—Å—Ç—Ä–µ–º–∞–ª—å–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è –≤ –ø—Ä–∏–∑–Ω–∞–∫–∞—Ö!")
            print_warning("   –¢—Ä–µ–±—É–µ—Ç—Å—è –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö")
        
        if 'target_imbalance' in unique_issues:
            print_warning("\n‚ö†Ô∏è –í–ù–ò–ú–ê–ù–ò–ï: –î–∏—Å–±–∞–ª–∞–Ω—Å –≤ —Ü–µ–ª–µ–≤—ã—Ö –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö!")
            print_info("   –ú–æ–∂–µ—Ç –ø–æ—Ç—Ä–µ–±–æ–≤–∞—Ç—å—Å—è –±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∞ –∫–ª–∞—Å—Å–æ–≤")
        
        # –ò–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏
        print(f"\n{Colors.WARNING}{Colors.BOLD}üîß –ù–ï–û–ë–•–û–î–ò–ú–´–ï –î–ï–ô–°–¢–í–ò–Ø:{Colors.ENDC}")
        print_info("1. –û—á–∏—Å—Ç–∏—Ç—å –∫—ç—à:")
        print(f"   {Colors.OKBLUE}rm -rf cache/features/*{Colors.ENDC}")
        print_info("2. –ü–µ—Ä–µ—Å–æ–∑–¥–∞—Ç—å –¥–∞–Ω–Ω—ã–µ:")
        print(f"   {Colors.OKBLUE}python prepare_trading_data.py --force-recreate{Colors.ENDC}")
        print_info("3. –ü—Ä–æ–≤–µ—Ä–∏—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç:")
        print(f"   {Colors.OKBLUE}python verify_data_correctness.py{Colors.ENDC}")
    
    # –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ —Å–æ—Ö—Ä–∞–Ω—è–µ–º –æ—Ç—á–µ—Ç –±–µ–∑ –∑–∞–ø—Ä–æ—Å–∞
    create_detailed_report(files)

def create_detailed_report(files):
    """–°–æ–∑–¥–∞–Ω–∏–µ –¥–µ—Ç–∞–ª—å–Ω–æ–≥–æ –æ—Ç—á–µ—Ç–∞ –¥–ª—è –ª–æ–≥–æ–≤ —Å –ø–æ–ª–Ω—ã–º –æ–ø–∏—Å–∞–Ω–∏–µ–º –≤—Å–µ—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤"""
    report_lines = []
    report_lines.append("="*80)
    report_lines.append("üìä –î–ï–¢–ê–õ–¨–ù–´–ô –û–¢–ß–ï–¢ –í–ê–õ–ò–î–ê–¶–ò–ò –î–ê–ù–ù–´–•")
    report_lines.append("="*80)
    report_lines.append(f"–î–∞—Ç–∞: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    
    # –î–ª—è —Å–±–æ—Ä–∞ –ø–æ–ª–Ω–æ–π —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
    all_features = set()
    
    for file in files:
        path = Path(f'data/processed/{file}')
        if path.exists():
            df = pd.read_parquet(path)
            report_lines.append(f"\n{'='*80}")
            report_lines.append(f"üìÅ {file.upper()}")
            report_lines.append(f"{'='*80}")
            report_lines.append(f"–†–∞–∑–º–µ—Ä: {len(df):,} –∑–∞–ø–∏—Å–µ–π, {len(df.columns)} –∫–æ–ª–æ–Ω–æ–∫")
            report_lines.append(f"–ü–µ—Ä–∏–æ–¥: {df['datetime'].min()} - {df['datetime'].max()}")
            report_lines.append(f"–°–∏–º–≤–æ–ª–æ–≤: {df['symbol'].nunique()}")
            
            # 1. –ú–ò–ö–†–û–°–¢–†–£–ö–¢–£–†–ù–´–ï –ü–†–ò–ó–ù–ê–ö–ò
            report_lines.append("\nüìà –ú–ò–ö–†–û–°–¢–†–£–ö–¢–£–†–ù–´–ï –ü–†–ò–ó–ù–ê–ö–ò:")
            microstructure_features = ['toxicity', 'price_impact', 'price_impact_log', 'amihud_illiquidity', 
                                     'kyle_lambda', 'realized_vol', 'hl_spread', 'volume_imbalance']
            for feature in microstructure_features:
                if feature in df.columns:
                    stats = df[feature].describe()
                    report_lines.append(f"\n  {feature}:")
                    report_lines.append(f"    Min: {stats['min']:.6f}, Max: {stats['max']:.6f}")
                    report_lines.append(f"    Mean: {stats['mean']:.6f}, Std: {stats['std']:.6f}")
                    report_lines.append(f"    25%: {stats['25%']:.6f}, 50%: {stats['50%']:.6f}, 75%: {stats['75%']:.6f}")
                    
                    # –°–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ –ø—Ä–æ–≤–µ—Ä–∫–∏
                    if feature == 'toxicity':
                        if stats['mean'] > 0.99:
                            report_lines.append("    ‚ö†Ô∏è –ü–†–û–ë–õ–ï–ú–ê: toxicity —Å–ª–∏—à–∫–æ–º –≤—ã—Å–æ–∫–∏–π!")
                        else:
                            report_lines.append("    ‚úÖ –ö–æ—Ä—Ä–µ–∫—Ç–Ω–æ–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ")
            
            # 2. –¢–ï–•–ù–ò–ß–ï–°–ö–ò–ï –ò–ù–î–ò–ö–ê–¢–û–†–´
            report_lines.append("\nüìä –¢–ï–•–ù–ò–ß–ï–°–ö–ò–ï –ò–ù–î–ò–ö–ê–¢–û–†–´:")
            technical_indicators = ['rsi', 'stoch_k', 'stoch_d', 'adx', 'adx_pos', 'adx_neg',
                                  'macd', 'macd_signal', 'macd_diff', 'bb_width', 'bb_position',
                                  'atr', 'atr_pct', 'psar_trend', 'close_position']
            for indicator in technical_indicators:
                if indicator in df.columns:
                    stats = df[indicator].describe()
                    report_lines.append(f"\n  {indicator}:")
                    report_lines.append(f"    Min: {stats['min']:.4f}, Max: {stats['max']:.4f}")
                    report_lines.append(f"    Mean: {stats['mean']:.4f}, Std: {stats['std']:.4f}")
                    
                    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—é
                    if indicator in ['rsi', 'stoch_k', 'stoch_d', 'adx'] and abs(stats['mean']) < 1.0 and 0.8 < stats['std'] < 1.2:
                        report_lines.append("    ‚ö†Ô∏è –ü–†–û–ë–õ–ï–ú–ê: –ü–æ–¥–æ–∑—Ä–µ–Ω–∏–µ –Ω–∞ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—é!")
            
            # 3. RALLY DETECTION –ü–†–ò–ó–ù–ê–ö–ò
            report_lines.append("\nüöÄ RALLY DETECTION –ü–†–ò–ó–ù–ê–ö–ò:")
            rally_features = ['volume_cumsum_4h', 'volume_cumsum_24h', 'volume_spike', 'spring_pattern',
                            'momentum_1h', 'momentum_4h', 'momentum_24h', 'momentum_acceleration']
            for feature in rally_features:
                if feature in df.columns:
                    stats = df[feature].describe()
                    report_lines.append(f"\n  {feature}:")
                    report_lines.append(f"    Mean: {stats['mean']:.4f}, Std: {stats['std']:.4f}")
                    if 'spike' in feature or 'pattern' in feature:
                        positive_pct = (df[feature] > 0).mean() * 100
                        report_lines.append(f"    –ü–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã—Ö: {positive_pct:.1f}%")
            
            # 4. SIGNAL QUALITY –ü–†–ò–ó–ù–ê–ö–ò
            report_lines.append("\nüì° SIGNAL QUALITY –ü–†–ò–ó–ù–ê–ö–ò:")
            signal_features = ['indicators_consensus_long', 'indicators_consensus_short',
                             'trend_1h_strength', 'trend_4h_strength', 'liquidity_score']
            for feature in signal_features:
                if feature in df.columns:
                    stats = df[feature].describe()
                    report_lines.append(f"\n  {feature}:")
                    report_lines.append(f"    Mean: {stats['mean']:.4f}, Std: {stats['std']:.4f}")
            
            # 5. FUTURES SPECIFIC –ü–†–ò–ó–ù–ê–ö–ò
            report_lines.append("\nüí∞ FUTURES SPECIFIC –ü–†–ò–ó–ù–ê–ö–ò:")
            futures_features = ['long_liquidation_distance_pct', 'short_liquidation_distance_pct',
                              'optimal_leverage', 'safe_leverage', 'var_95']
            for feature in futures_features:
                if feature in df.columns:
                    stats = df[feature].describe()
                    report_lines.append(f"\n  {feature}:")
                    report_lines.append(f"    Mean: {stats['mean']:.4f}, Std: {stats['std']:.4f}")
            
            # 6. –¶–ï–õ–ï–í–´–ï –ü–ï–†–ï–ú–ï–ù–ù–´–ï
            report_lines.append("\nüéØ –¶–ï–õ–ï–í–´–ï –ü–ï–†–ï–ú–ï–ù–ù–´–ï:")
            
            # –ë–∏–Ω–∞—Ä–Ω—ã–µ —Ü–µ–ª–µ–≤—ã–µ
            for direction in ['long', 'short']:
                for level in ['tp1', 'tp2', 'tp3', 'sl']:
                    target = f'{direction}_{level}_reached'
                    if target in df.columns:
                        pct = df[target].mean() * 100
                        report_lines.append(f"  {target}: {pct:.2f}%")
            
            # Expected values
            for ev in ['long_expected_value', 'short_expected_value']:
                if ev in df.columns:
                    stats = df[ev].describe()
                    report_lines.append(f"\n  {ev}:")
                    report_lines.append(f"    Mean: {stats['mean']:.4f}, Std: {stats['std']:.4f}")
                    report_lines.append(f"    –ü–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã—Ö: {(df[ev] > 0).mean() * 100:.1f}%")
            
            # Best direction
            if 'best_direction' in df.columns:
                report_lines.append(f"\n  best_direction —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ:")
                dist = df['best_direction'].value_counts()
                for direction, count in dist.items():
                    pct = count / len(df) * 100
                    report_lines.append(f"    {direction}: {count:,} ({pct:.1f}%)")
            
            # Signal strength
            if 'signal_strength' in df.columns:
                stats = df['signal_strength'].describe()
                report_lines.append(f"\n  signal_strength:")
                report_lines.append(f"    Mean: {stats['mean']:.4f}, Std: {stats['std']:.4f}")
                report_lines.append(f"    Max: {stats['max']:.4f}")
            
            # 7. –ü–†–û–í–ï–†–ö–ê NAN –ó–ù–ê–ß–ï–ù–ò–ô
            report_lines.append("\n‚ö†Ô∏è –ü–†–û–ü–£–©–ï–ù–ù–´–ï –ó–ù–ê–ß–ï–ù–ò–Ø:")
            nan_cols = df.isna().sum()
            nan_cols = nan_cols[nan_cols > 0].sort_values(ascending=False)
            if len(nan_cols) > 0:
                for col, count in nan_cols.head(10).items():
                    pct = count / len(df) * 100
                    report_lines.append(f"  {col}: {count:,} ({pct:.2f}%)")
            else:
                report_lines.append("  ‚úÖ –ù–µ—Ç –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π")
            
            # 8. –ò–¢–û–ì–û–í–ê–Ø –°–¢–ê–¢–ò–°–¢–ò–ö–ê
            report_lines.append(f"\nüìä –ò–¢–û–ì–û–í–ê–Ø –°–¢–ê–¢–ò–°–¢–ò–ö–ê {file.split('_')[0].upper()}:")
            
            # –ü–æ–¥—Å—á–µ—Ç –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º
            feature_categories = {
                '–ë–∞–∑–æ–≤—ã–µ': ['returns', 'volume_ratio', 'high_low_ratio', 'close_open_ratio'],
                '–¢–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ': [col for col in df.columns if any(ind in col for ind in ['sma', 'ema', 'rsi', 'macd', 'bb_', 'stoch'])],
                '–ú–∏–∫—Ä–æ—Å—Ç—Ä—É–∫—Ç—É—Ä–∞': [col for col in df.columns if any(ms in col for ms in ['toxicity', 'impact', 'illiquidity', 'spread'])],
                'Rally detection': [col for col in df.columns if any(rd in col for rd in ['volume_cumsum', 'momentum', 'spring', 'divergence'])],
                'Signal quality': [col for col in df.columns if any(sq in col for sq in ['consensus', 'trend_strength', 'liquidity_score'])],
                'Futures': [col for col in df.columns if any(f in col for f in ['liquidation', 'leverage', 'var_', 'funding'])],
                '–í—Ä–µ–º–µ–Ω–Ω—ã–µ': [col for col in df.columns if any(t in col for t in ['hour', 'day', 'month', 'session'])],
                'Cross-asset': [col for col in df.columns if any(ca in col for ca in ['btc_', 'sector', 'relative_'])],
                '–¶–µ–ª–µ–≤—ã–µ': [col for col in df.columns if any(tgt in col for tgt in ['target_', 'future_', '_reached', 'expected_value', 'best_direction'])]
            }
            
            for category, features in feature_categories.items():
                count = len([f for f in features if f in df.columns])
                if count > 0:
                    report_lines.append(f"  {category}: {count} –ø—Ä–∏–∑–Ω–∞–∫–æ–≤")
            
            report_lines.append(f"  –í–°–ï–ì–û: {len(df.columns)} –∫–æ–ª–æ–Ω–æ–∫")
    
    report_lines.append("\n" + "="*80)
    report_lines.append("‚úÖ –û–¢–ß–ï–¢ –ó–ê–í–ï–†–®–ï–ù")
    report_lines.append("="*80)
    
    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ
    report_path = Path('logs/data_validation_report.txt')
    report_path.parent.mkdir(exist_ok=True)
    with open(report_path, 'w', encoding='utf-8') as f:
        f.write('\n'.join(report_lines))
    
    print_info(f"\nüìÑ –î–µ—Ç–∞–ª—å–Ω—ã–π –æ—Ç—á–µ—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {report_path}")

if __name__ == "__main__":
    main()