#!/usr/bin/env python3
"""
–ü–æ–ª–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ—Å—Ç–∏ —Å—Ñ–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö —Å –≤–∞–ª–∏–¥–∞—Ü–∏–µ–π –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–æ–≤
"""

import pandas as pd
import numpy as np
import torch
from pathlib import Path
import yaml
from datetime import datetime
import warnings
warnings.filterwarnings('ignore')

# –ò–º–ø–æ—Ä—Ç –Ω–∞—à–µ–≥–æ –≤–∞–ª–∏–¥–∞—Ç–æ—Ä–∞ –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–æ–≤
from utils.indicator_validator import IndicatorValidator

# –¶–≤–µ—Ç–Ω–æ–π –≤—ã–≤–æ–¥
class Colors:
    HEADER = '\033[95m'
    OKBLUE = '\033[94m'
    OKGREEN = '\033[92m'
    WARNING = '\033[93m'
    FAIL = '\033[91m'
    ENDC = '\033[0m'
    BOLD = '\033[1m'

def print_header(text):
    print(f"\n{Colors.HEADER}{Colors.BOLD}{'='*80}{Colors.ENDC}")
    print(f"{Colors.HEADER}{Colors.BOLD}{text}{Colors.ENDC}")
    print(f"{Colors.HEADER}{Colors.BOLD}{'='*80}{Colors.ENDC}")

def print_success(text):
    print(f"{Colors.OKGREEN}‚úÖ {text}{Colors.ENDC}")

def print_warning(text):
    print(f"{Colors.WARNING}‚ö†Ô∏è  {text}{Colors.ENDC}")

def print_error(text):
    print(f"{Colors.FAIL}‚ùå {text}{Colors.ENDC}")

def print_info(text):
    print(f"{Colors.OKBLUE}‚ÑπÔ∏è  {text}{Colors.ENDC}")

def check_file_existence():
    """–ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞–ª–∏—á–∏—è —Ñ–∞–π–ª–æ–≤"""
    print_header("1. –ü–†–û–í–ï–†–ö–ê –ù–ê–õ–ò–ß–ò–Ø –§–ê–ô–õ–û–í")
    
    cache_dir = Path("data/processed")
    required_files = ["train_data.parquet", "val_data.parquet", "test_data.parquet"]
    
    all_exist = True
    file_sizes = {}
    
    for file_name in required_files:
        file_path = cache_dir / file_name
        if file_path.exists():
            size_mb = file_path.stat().st_size / (1024 * 1024)
            file_sizes[file_name] = size_mb
            print_success(f"{file_name}: {size_mb:.2f} MB")
        else:
            print_error(f"{file_name}: –ù–ï –ù–ê–ô–î–ï–ù")
            all_exist = False
    
    return all_exist, file_sizes

def check_data_structure(df, name):
    """–ü—Ä–æ–≤–µ—Ä–∫–∞ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –¥–∞–Ω–Ω—ã—Ö"""
    print(f"\n{Colors.BOLD}–ü—Ä–æ–≤–µ—Ä–∫–∞ {name}:{Colors.ENDC}")
    
    # –ë–∞–∑–æ–≤–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è
    print_info(f"–†–∞–∑–º–µ—Ä: {len(df):,} –∑–∞–ø–∏—Å–µ–π, {len(df.columns)} –∫–æ–ª–æ–Ω–æ–∫")
    print_info(f"–ü–∞–º—è—Ç—å: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB")
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã—Ö –∫–æ–ª–æ–Ω–æ–∫
    required_cols = ['datetime', 'symbol', 'open', 'high', 'low', 'close', 'volume', 'turnover']
    missing_required = [col for col in required_cols if col not in df.columns]
    if missing_required:
        print_error(f"–û—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã–µ –∫–æ–ª–æ–Ω–∫–∏: {missing_required}")
    else:
        print_success("–í—Å–µ –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã–µ –∫–æ–ª–æ–Ω–∫–∏ –ø—Ä–∏—Å—É—Ç—Å—Ç–≤—É—é—Ç")
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ü–µ–ª–µ–≤—ã—Ö –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö
    from data.constants import TRADING_TARGET_VARIABLES, ALL_TARGET_VARIABLES
    
    missing_targets = [col for col in TRADING_TARGET_VARIABLES if col not in df.columns]
    if missing_targets:
        print_error(f"–û—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç —Ü–µ–ª–µ–≤—ã–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ: {len(missing_targets)} –∏–∑ {len(TRADING_TARGET_VARIABLES)}")
    else:
        print_success(f"–í—Å–µ {len(TRADING_TARGET_VARIABLES)} –æ—Å–Ω–æ–≤–Ω—ã—Ö —Ü–µ–ª–µ–≤—ã—Ö –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –ø—Ä–∏—Å—É—Ç—Å—Ç–≤—É—é—Ç")
    
    # –ü–æ–¥—Å—á–µ—Ç —Ç–∏–ø–æ–≤ –∫–æ–ª–æ–Ω–æ–∫
    feature_cols = [col for col in df.columns 
                   if col not in ALL_TARGET_VARIABLES 
                   and col not in ['id', 'symbol', 'datetime', 'timestamp']]
    
    print_info(f"–ü—Ä–∏–∑–Ω–∞–∫–æ–≤: {len(feature_cols)}")
    print_info(f"–¶–µ–ª–µ–≤—ã—Ö –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö: {len([col for col in df.columns if col in ALL_TARGET_VARIABLES])}")
    
    return len(missing_required) == 0 and len(missing_targets) == 0

def check_data_quality(df, name):
    """–ü—Ä–æ–≤–µ—Ä–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞ –¥–∞–Ω–Ω—ã—Ö"""
    print(f"\n{Colors.BOLD}–ö–∞—á–µ—Å—Ç–≤–æ –¥–∞–Ω–Ω—ã—Ö {name}:{Colors.ENDC}")
    
    issues = []
    
    # 1. –ü—Ä–æ–≤–µ—Ä–∫–∞ NaN
    nan_counts = df.isna().sum()
    nan_cols = nan_counts[nan_counts > 0]
    if len(nan_cols) > 0:
        print_warning(f"–ù–∞–π–¥–µ–Ω–æ {len(nan_cols)} –∫–æ–ª–æ–Ω–æ–∫ —Å NaN –∑–Ω–∞—á–µ–Ω–∏—è–º–∏")
        worst_cols = nan_cols.nlargest(5)
        for col, count in worst_cols.items():
            pct = count / len(df) * 100
            print(f"   - {col}: {count:,} NaN ({pct:.1f}%)")
        issues.append("nan_values")
    else:
        print_success("–ù–µ—Ç –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π")
    
    # 2. –ü—Ä–æ–≤–µ—Ä–∫–∞ inf
    numeric_cols = df.select_dtypes(include=[np.number]).columns
    inf_counts = {}
    for col in numeric_cols:
        inf_count = np.isinf(df[col]).sum()
        if inf_count > 0:
            inf_counts[col] = inf_count
    
    if inf_counts:
        print_warning(f"–ù–∞–π–¥–µ–Ω–æ {len(inf_counts)} –∫–æ–ª–æ–Ω–æ–∫ —Å inf –∑–Ω–∞—á–µ–Ω–∏—è–º–∏")
        for col, count in list(inf_counts.items())[:5]:
            print(f"   - {col}: {count:,} inf")
        issues.append("inf_values")
    else:
        print_success("–ù–µ—Ç –±–µ—Å–∫–æ–Ω–µ—á–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π")
    
    # 3. –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤
    if 'datetime' in df.columns and 'symbol' in df.columns:
        duplicates = df.duplicated(subset=['datetime', 'symbol']).sum()
        if duplicates > 0:
            print_warning(f"–ù–∞–π–¥–µ–Ω–æ {duplicates:,} –¥—É–±–ª–∏–∫–∞—Ç–æ–≤ –ø–æ datetime+symbol")
            issues.append("duplicates")
        else:
            print_success("–ù–µ—Ç –¥—É–±–ª–∏–∫–∞—Ç–æ–≤")
    
    return issues


def check_target_distribution(df, name):
    """–ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —Ü–µ–ª–µ–≤—ã—Ö –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö"""
    print(f"\n{Colors.BOLD}–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ü–µ–ª–µ–≤—ã—Ö –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö {name}:{Colors.ENDC}")
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –±–∏–Ω–∞—Ä–Ω—ã—Ö —Ü–µ–ª–µ–≤—ã—Ö
    binary_targets = []
    for col in df.columns:
        if any(pattern in col for pattern in ['_hit', '_reached']):
            binary_targets.append(col)
    
    issues = []
    for target in binary_targets[:10]:  # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø–µ—Ä–≤—ã–µ 10
        if target in df.columns:
            unique_vals = df[target].unique()
            if not set(unique_vals).issubset({0, 1, 0.0, 1.0}):
                print_error(f"{target}: –Ω–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è {unique_vals}")
                issues.append(f"{target}_invalid")
            else:
                pos_rate = df[target].mean() * 100
                print_info(f"{target}: {pos_rate:.1f}% –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã—Ö")
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ best_direction
    if 'best_direction' in df.columns:
        direction_counts = df['best_direction'].value_counts()
        print("\n–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ best_direction:")
        for direction, count in direction_counts.items():
            pct = count / len(df) * 100
            print(f"   {direction}: {count:,} ({pct:.1f}%)")
    
    return issues

def check_temporal_consistency(df, name):
    """–ü—Ä–æ–≤–µ—Ä–∫–∞ –≤—Ä–µ–º–µ–Ω–Ω–æ–π –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏"""
    print(f"\n{Colors.BOLD}–í—Ä–µ–º–µ–Ω–Ω–∞—è –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å {name}:{Colors.ENDC}")
    
    if 'datetime' not in df.columns:
        print_error("–ö–æ–ª–æ–Ω–∫–∞ datetime –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç")
        return ["no_datetime"]
    
    issues = []
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å–æ—Ä—Ç–∏—Ä–æ–≤–∫–∏
    if not df['datetime'].is_monotonic_increasing:
        print_warning("–î–∞–Ω–Ω—ã–µ –Ω–µ –æ—Ç—Å–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω—ã –ø–æ –≤—Ä–µ–º–µ–Ω–∏")
        issues.append("not_sorted")
    else:
        print_success("–î–∞–Ω–Ω—ã–µ –æ—Ç—Å–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω—ã –ø–æ –≤—Ä–µ–º–µ–Ω–∏")
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–∏–∞–ø–∞–∑–æ–Ω–∞ –¥–∞—Ç
    date_min = df['datetime'].min()
    date_max = df['datetime'].max()
    print_info(f"–ü–µ—Ä–∏–æ–¥: {date_min} - {date_max}")
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –≥—ç–ø–æ–≤ –ø–æ —Å–∏–º–≤–æ–ª–∞–º
    if 'symbol' in df.columns:
        symbols = df['symbol'].unique()
        print(f"\n–ü—Ä–æ–≤–µ—Ä–∫–∞ –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –≥—ç–ø–æ–≤ –¥–ª—è {len(symbols)} —Å–∏–º–≤–æ–ª–æ–≤:")
        
        large_gaps = 0
        for symbol in symbols[:5]:  # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø–µ—Ä–≤—ã–µ 5 —Å–∏–º–≤–æ–ª–æ–≤
            symbol_data = df[df['symbol'] == symbol].sort_values('datetime')
            time_diff = symbol_data['datetime'].diff()
            expected_diff = pd.Timedelta('15 minutes')
            gaps = time_diff[time_diff > expected_diff * 2]  # –ì—ç–ø—ã –±–æ–ª—å—à–µ 30 –º–∏–Ω—É—Ç
            if len(gaps) > 0:
                large_gaps += len(gaps)
        
        if large_gaps > 0:
            print_warning(f"–û–±–Ω–∞—Ä—É–∂–µ–Ω–æ {large_gaps} –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –≥—ç–ø–æ–≤ > 30 –º–∏–Ω—É—Ç")
            issues.append("time_gaps")
        else:
            print_success("–ù–µ—Ç –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω—ã—Ö –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –≥—ç–ø–æ–≤")
    
    return issues

def check_critical_indicators(df, name):
    """–ü—Ä–æ–≤–µ—Ä–∫–∞ –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏—Ö –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–æ–≤ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω–æ–≥–æ –≤–∞–ª–∏–¥–∞—Ç–æ—Ä–∞"""
    print(f"\n{Colors.BOLD}–ü—Ä–æ–≤–µ—Ä–∫–∞ –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–æ–≤ {name} (Enhanced):{Colors.ENDC}")
    
    # –ò—Å–ø–æ–ª—å–∑—É–µ–º –Ω–∞—à –ø—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–π –≤–∞–ª–∏–¥–∞—Ç–æ—Ä
    validator = IndicatorValidator()
    
    try:
        validation_results = validator.validate_dataframe(df, strict=False)
        
        # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤ –Ω–∞—à —Ñ–æ—Ä–º–∞—Ç
        issues = []
        
        # –û—à–∏–±–∫–∏ - –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏–µ –ø—Ä–æ–±–ª–µ–º—ã
        for error in validation_results['errors']:
            print_error(error)
            if "extreme" in error.lower():
                issues.append("extreme_values")
            elif "–Ω–æ—Ä–º–∞–ª–∏–∑" in error.lower():
                issues.append("normalization_error")
            else:
                issues.append("indicator_error")
        
        # –ü—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏—è
        for warning in validation_results['warnings']:
            print_warning(warning)
            if "vwap" in warning.lower():
                issues.append("vwap_warning")
            else:
                issues.append("indicator_warning")
        
        # –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏–æ–Ω–Ω—ã–µ —Å–æ–æ–±—â–µ–Ω–∏—è (–ø–µ—Ä–≤—ã–µ 3)
        for info in validation_results['info'][:3]:
            print_success(info)
        
        if len(validation_results['info']) > 3:
            print_info(f"... –∏ –µ—â–µ {len(validation_results['info']) - 3} –∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã—Ö –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–æ–≤")
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –ø—Ä–æ–≤–µ—Ä–µ–Ω–Ω—ã–º –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–∞–º
        total_checked = len(validation_results['statistics'])
        total_errors = len(validation_results['errors'])
        total_warnings = len(validation_results['warnings'])
        
        print(f"\n{Colors.BOLD}üìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–∏:{Colors.ENDC}")
        print_info(f"–ü—Ä–æ–≤–µ—Ä–µ–Ω–æ –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–æ–≤: {total_checked}")
        if total_errors > 0:
            print_error(f"–ö—Ä–∏—Ç–∏—á–µ—Å–∫–∏–µ –æ—à–∏–±–∫–∏: {total_errors}")
        if total_warnings > 0:
            print_warning(f"–ü—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏—è: {total_warnings}")
        if total_errors == 0 and total_warnings == 0:
            print_success("–í—Å–µ –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã –ø—Ä–æ—à–ª–∏ –≤–∞–ª–∏–¥–∞—Ü–∏—é!")
            
        return issues
        
    except Exception as e:
        print_error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –≤–∞–ª–∏–¥–∞—Ü–∏–∏ –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–æ–≤: {e}")
        # Fallback –∫ –±–∞–∑–æ–≤–æ–π –ø—Ä–æ–≤–µ—Ä–∫–µ
        return check_basic_indicators(df)

def check_basic_indicators(df):
    """–ë–∞–∑–æ–≤–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–æ–≤ (fallback)"""
    issues = []
    
    # –û—Å–Ω–æ–≤–Ω—ã–µ –ø—Ä–æ–≤–µ—Ä–∫–∏
    critical_ranges = {
        'rsi': (0, 100),
        'stoch_k': (0, 100), 
        'stoch_d': (0, 100),
        'adx': (0, 100),
        'toxicity': (0, 1),
        'bb_position': (0, 1)
    }
    
    for indicator, (min_val, max_val) in critical_ranges.items():
        if indicator in df.columns:
            stats = df[indicator].describe()
            
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—é (–ø–æ–¥–æ–∑—Ä–∏—Ç–µ–ª—å–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è)
            if abs(stats['mean']) < 1.0 and stats['std'] < 2.0 and indicator in ['rsi', 'stoch_k', 'stoch_d', 'adx']:
                print_error(f"‚ùå {indicator}: –ü–û–î–û–ó–†–ï–ù–ò–ï –ù–ê –ù–û–†–ú–ê–õ–ò–ó–ê–¶–ò–Æ! Mean={stats['mean']:.3f}, Std={stats['std']:.3f}")
                issues.append(f"{indicator}_normalized")
            elif stats['min'] < min_val or stats['max'] > max_val:
                print_warning(f"‚ö†Ô∏è {indicator}: [{stats['min']:.3f}, {stats['max']:.3f}] –≤—ã—Ö–æ–¥–∏—Ç –∑–∞ [{min_val}, {max_val}]")
                issues.append(f"{indicator}_range")
            else:
                print_success(f"‚úÖ {indicator}: –∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–π –¥–∏–∞–ø–∞–∑–æ–Ω [{stats['min']:.3f}, {stats['max']:.3f}]")
    
    return issues

def check_technical_indicators_enhanced(df, name):
    """–ò–°–ü–†–ê–í–õ–ï–ù–ù–ê–Ø –ø—Ä–æ–≤–µ—Ä–∫–∞ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏—Ö –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–æ–≤"""
    print(f"\n{Colors.BOLD}–†–∞—Å—à–∏—Ä–µ–Ω–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏—Ö –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–æ–≤ {name}:{Colors.ENDC}")
    
    issues = []
    indicators_checked = 0
    indicators_passed = 0
    
    # –ò–°–ü–†–ê–í–õ–ï–ù–û: –ü—Ä–∞–≤–∏–ª—å–Ω—ã–µ –¥–∏–∞–ø–∞–∑–æ–Ω—ã –¥–ª—è —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏—Ö –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–æ–≤
    technical_ranges = {
        'rsi': (0, 100, "RSI –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –æ—Ç 0 –¥–æ 100"),
        'stoch_k': (0, 100, "Stochastic %K –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –æ—Ç 0 –¥–æ 100"),
        'stoch_d': (0, 100, "Stochastic %D –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –æ—Ç 0 –¥–æ 100"),
        'adx': (0, 100, "ADX –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –æ—Ç 0 –¥–æ 100"),
        'adx_pos': (0, 100, "ADX+ –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –æ—Ç 0 –¥–æ 100"),
        'adx_neg': (0, 100, "ADX- –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –æ—Ç 0 –¥–æ 100"),
        'bb_position': (0, 1, "Bollinger Bands Position –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –æ—Ç 0 –¥–æ 1"),
        'close_position': (0, 1, "Close Position –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –æ—Ç 0 –¥–æ 1"),
        'psar_trend': (0, 1, "PSAR Trend –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å 0 –∏–ª–∏ 1"),
        'rsi_oversold': (0, 1, "RSI Oversold –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å 0 –∏–ª–∏ 1"),
        'rsi_overbought': (0, 1, "RSI Overbought –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å 0 –∏–ª–∏ 1"),
    }
    
    # –°–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ –ø—Ä–æ–≤–µ—Ä–∫–∏
    special_checks = {
        'close_vwap_ratio': (0.5, 1.5, "Close/VWAP ratio –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –æ—Ç 0.5 –¥–æ 1.5"),
        'close_open_ratio': (0.8, 1.25, "Close/Open ratio –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –æ—Ç 0.8 –¥–æ 1.25"),
        'high_low_ratio': (1.0, 2.0, "High/Low ratio –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –æ—Ç 1.0 –¥–æ 2.0"),
        'toxicity': (0.5, 1.0, "Toxicity –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –æ—Ç 0.5 –¥–æ 1.0"),
    }
    
    # –û–±—ä–µ–¥–∏–Ω—è–µ–º –≤—Å–µ –ø—Ä–æ–≤–µ—Ä–∫–∏
    all_checks = {**technical_ranges, **special_checks}
    
    for indicator, (min_val, max_val, description) in all_checks.items():
        if indicator in df.columns:
            indicators_checked += 1
            stats = df[indicator].describe()
            
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–∏–∞–ø–∞–∑–æ–Ω–∞
            if stats['min'] < min_val or stats['max'] > max_val:
                print_error(f"{indicator}: –≤—ã—Ö–æ–¥ –∑–∞ –¥–∏–∞–ø–∞–∑–æ–Ω [{min_val}, {max_val}] -> [{stats['min']:.4f}, {stats['max']:.4f}]")
                issues.append(f"{indicator}_range_error")
            else:
                print_success(f"{indicator}: –∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–π –¥–∏–∞–ø–∞–∑–æ–Ω [{stats['min']:.4f}, {stats['max']:.4f}]")
                indicators_passed += 1
            
            # –ò–°–ü–†–ê–í–õ–ï–ù–û: –°–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ –ø—Ä–æ–≤–µ—Ä–∫–∏ –¥–ª—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã—Ö –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–æ–≤
            if indicator == 'toxicity':
                # Toxicity –Ω–µ –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –∫–æ–Ω—Å—Ç–∞–Ω—Ç–æ–π –æ–∫–æ–ª–æ 1.0
                if stats['std'] < 0.01 and stats['mean'] > 0.99:
                    print_error(f"TOXICITY: –ø–æ–¥–æ–∑—Ä–µ–Ω–∏–µ –Ω–∞ –æ—à–∏–±–∫—É –≤ —Ñ–æ—Ä–º—É–ª–µ! Mean={stats['mean']:.6f}, Std={stats['std']:.6f}")
                    issues.append("toxicity_formula_error")
                elif 0.5 <= stats['mean'] <= 1.0 and stats['std'] > 0.01:
                    print_success(f"TOXICITY: –∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è Mean={stats['mean']:.4f}, Std={stats['std']:.4f}")
            
            elif indicator == 'rsi':
                # RSI –¥–æ–ª–∂–µ–Ω –∏–º–µ—Ç—å —Ä–∞–∑—É–º–Ω–æ–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ
                if 20 <= stats['mean'] <= 80 and stats['std'] > 5:
                    print_success(f"RSI: –∑–¥–æ—Ä–æ–≤–æ–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ Mean={stats['mean']:.2f}, Std={stats['std']:.2f}")
                else:
                    print_warning(f"RSI: –Ω–µ–æ–±—ã—á–Ω–æ–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ Mean={stats['mean']:.2f}, Std={stats['std']:.2f}")
            
            elif indicator in ['stoch_k', 'stoch_d']:
                # Stochastic –¥–æ–ª–∂–µ–Ω –∏–º–µ—Ç—å —Ö–æ—Ä–æ—à—É—é –≤–∞—Ä–∏–∞—Ü–∏—é
                if stats['std'] > 15:
                    print_success(f"{indicator}: —Ö–æ—Ä–æ—à–∞—è –≤–∞—Ä–∏–∞—Ü–∏—è Std={stats['std']:.2f}")
                else:
                    print_warning(f"{indicator}: –Ω–∏–∑–∫–∞—è –≤–∞—Ä–∏–∞—Ü–∏—è Std={stats['std']:.2f}")
    
    # –ò–°–ü–†–ê–í–õ–ï–ù–û: –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø—Ä–æ–±–ª–µ–º–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ —Å –±–æ–ª—å—à–∏–º–∏ –∑–Ω–∞—á–µ–Ω–∏—è–º–∏
    problematic_features = ['bb_width', 'daily_range']
    for feature in problematic_features:
        if feature in df.columns:
            indicators_checked += 1
            stats = df[feature].describe()
            
            # bb_width –∏ daily_range –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å –∫–∞–∫ –ø—Ä–æ—Ü–µ–Ω—Ç –æ—Ç —Ü–µ–Ω—ã (–æ–±—ã—á–Ω–æ < 0.5)
            if stats['max'] > 1.0:  # –ë–æ–ª—å—à–µ 100%
                print_error(f"{feature}: —ç–∫—Å—Ç—Ä–µ–º–∞–ª—å–Ω–æ –±–æ–ª—å—à–∏–µ –∑–Ω–∞—á–µ–Ω–∏—è! Max={stats['max']:.2e}")
                print_info(f"   –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è: –ø–µ—Ä–µ—Å—á–∏—Ç–∞—Ç—å –∫–∞–∫ –ø—Ä–æ—Ü–µ–Ω—Ç –æ—Ç —Ü–µ–Ω—ã")
                issues.append(f"{feature}_extreme_values")
            elif stats['max'] > 0.5:  # –ë–æ–ª—å—à–µ 50%
                print_warning(f"{feature}: –±–æ–ª—å—à–∏–µ –∑–Ω–∞—á–µ–Ω–∏—è Max={stats['max']:.4f}, –≤–æ–∑–º–æ–∂–Ω–æ –Ω—É–∂–Ω–æ –æ–≥—Ä–∞–Ω–∏—á–∏—Ç—å")
                issues.append(f"{feature}_large_values")
            else:
                print_success(f"{feature}: —Ä–∞–∑—É–º–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è Max={stats['max']:.4f}")
                indicators_passed += 1
    
    # –ò–°–ü–†–ê–í–õ–ï–ù–û: –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø–æ–¥–æ–∑—Ä–∏—Ç–µ–ª—å–Ω–æ–π –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏
    normalization_suspects = [
        'price_direction', 'volume_zscore', 'momentum_1h', 'trend_1h_strength',
        'future_return_3', 'future_return_4', 'target_return_1h'
    ]
    allowed_zscore = {
        'price_direction', 'volume_zscore', 'momentum_1h',
        'trend_1h_strength', 'future_return_3', 'future_return_4',
        'target_return_1h'
    }
    
    print(f"\n{Colors.BOLD}–ü—Ä–æ–≤–µ—Ä–∫–∞ –ø–æ–¥–æ–∑—Ä–∏—Ç–µ–ª—å–Ω–æ–π –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏:{Colors.ENDC}")
    normalization_issues = 0
    
    for col in normalization_suspects:
        if col in df.columns:
            stats = df[col].describe()
            
            # –ü—Ä–∏–∑–Ω–∞–∫–∏ –Ω–µ–ø—Ä–∞–≤–∏–ª—å–Ω–æ–π –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏:
            # 1. Mean –æ—á–µ–Ω—å –±–ª–∏–∑–∫–æ –∫ 0
            # 2. Std –æ—á–µ–Ω—å –±–ª–∏–∑–∫–æ –∫ 1
            # 3. –ù–æ —ç—Ç–æ –Ω–µ –¥–æ–ª–∂–Ω–æ –±—ã—Ç—å —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–π –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä
            is_normalized = (abs(stats['mean']) < 0.1 and 0.8 < stats['std'] < 1.2)
            
            if is_normalized and col not in allowed_zscore:
                print_error(f"{col}: –ü–û–î–û–ó–†–ï–ù–ò–ï –ù–ê –ù–û–†–ú–ê–õ–ò–ó–ê–¶–ò–Æ! Mean={stats['mean']:.3f}, Std={stats['std']:.3f}")
                issues.append(f"{col}_normalized")
                normalization_issues += 1
            elif col in allowed_zscore and is_normalized:
                print_success(f"{col}: –∫–æ—Ä—Ä–µ–∫—Ç–Ω–∞—è Z-score –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è Mean={stats['mean']:.3f}, Std={stats['std']:.3f}")
            else:
                print_success(f"{col}: –Ω–æ—Ä–º–∞–ª—å–Ω–æ–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ Mean={stats['mean']:.3f}, Std={stats['std']:.3f}")
    
    # –ò—Ç–æ–≥–æ–≤–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
    print(f"\n{Colors.BOLD}üìä –ò—Ç–æ–≥–æ–≤–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–∏ –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–æ–≤:{Colors.ENDC}")
    print_info(f"–ü—Ä–æ–≤–µ—Ä–µ–Ω–æ –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–æ–≤: {indicators_checked}")
    print_info(f"–ü—Ä–æ—à–ª–∏ –ø—Ä–æ–≤–µ—Ä–∫—É: {indicators_passed}")
    
    if len(issues) == 0:
        print_success("üéâ –í–°–ï –ò–ù–î–ò–ö–ê–¢–û–†–´ –ö–û–†–†–ï–ö–¢–ù–´!")
    else:
        critical_issues = len([i for i in issues if 'error' in i])
        warning_issues = len(issues) - critical_issues
        
        if critical_issues > 0:
            print_error(f"–ö—Ä–∏—Ç–∏—á–µ—Å–∫–∏–µ –æ—à–∏–±–∫–∏: {critical_issues}")
        if warning_issues > 0:
            print_warning(f"–ü—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏—è: {warning_issues}")
        if normalization_issues > 0:
            print_error(f"–ü—Ä–æ–±–ª–µ–º—ã –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏: {normalization_issues}")
    
    return issues

def check_advanced_issues(df, name):
    """–†–∞—Å—à–∏—Ä–µ–Ω–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –ø—Ä–æ–±–ª–µ–º –¥–∞–Ω–Ω—ã—Ö"""
    print(f"\n{Colors.BOLD}–†–∞—Å—à–∏—Ä–µ–Ω–Ω–∞—è –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞ {name}:{Colors.ENDC}")
    
    issues = []
    
    # 1. –ü—Ä–æ–≤–µ—Ä–∫–∞ –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–π (–ø—Ä–∏–∑–Ω–∞–∫ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è)
    numeric_cols = df.select_dtypes(include=[np.number]).columns[:20]  # –ü–µ—Ä–≤—ã–µ 20 –¥–ª—è —Å–∫–æ—Ä–æ—Å—Ç–∏
    if len(numeric_cols) > 1:
        corr_matrix = df[numeric_cols].corr().abs()
        # –ò—Å–∫–ª—é—á–∞–µ–º –¥–∏–∞–≥–æ–Ω–∞–ª—å
        corr_matrix = corr_matrix.where(~np.eye(corr_matrix.shape[0], dtype=bool))
        high_corr = (corr_matrix > 0.95).sum().sum()
        if high_corr > 0:
            print_warning(f"–ù–∞–π–¥–µ–Ω–æ {high_corr} –ø–∞—Ä –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ —Å –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–µ–π > 0.95")
            issues.append("high_correlation")
        else:
            print_success("–ö–æ—Ä—Ä–µ–ª—è—Ü–∏–∏ –≤ –Ω–æ—Ä–º–µ")
    
    # 2. –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–π (–≤—ã–±—Ä–æ—Å—ã)
    extreme_features = []
    for col in numeric_cols:
        if col not in ['timestamp', 'volume', 'turnover']:
            q99 = df[col].quantile(0.99)
            q01 = df[col].quantile(0.01)
            if abs(q99) > 100 or abs(q01) > 100:
                extreme_features.append(f"{col} (Q99={q99:.2e}, Q01={q01:.2e})")
    
    if extreme_features:
        print_warning(f"–ü—Ä–∏–∑–Ω–∞–∫–∏ —Å —ç–∫—Å—Ç—Ä–µ–º–∞–ª—å–Ω—ã–º–∏ –∑–Ω–∞—á–µ–Ω–∏—è–º–∏:")
        for feature in extreme_features[:5]:
            print(f"   - {feature}")
        issues.append("extreme_distributions")
    else:
        print_success("–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –≤ —Ä–∞–∑—É–º–Ω—ã—Ö –ø—Ä–µ–¥–µ–ª–∞—Ö")
    
    # 3. –ü—Ä–æ–≤–µ—Ä–∫–∞ –≤—Ä–µ–º–µ–Ω–Ω–æ–π —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏
    if 'datetime' in df.columns and len(df) > 1000:
        # –†–∞–∑–¥–µ–ª–∏–º –Ω–∞ —á–∞—Å—Ç–∏ –∏ –ø—Ä–æ–≤–µ—Ä–∏–º —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç—å —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫
        mid_point = len(df) // 2
        first_half = df.iloc[:mid_point]
        second_half = df.iloc[mid_point:]
        
        unstable_features = []
        for col in numeric_cols[:10]:  # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø–µ—Ä–≤—ã–µ 10
            if col in ['timestamp', 'datetime']:
                continue
            
            mean1 = first_half[col].mean()
            mean2 = second_half[col].mean()
            
            if abs(mean1) > 1e-6 and abs(mean2) > 1e-6:  # –ò–∑–±–µ–≥–∞–µ–º –¥–µ–ª–µ–Ω–∏—è –Ω–∞ –Ω–æ–ª—å
                ratio = abs(mean1 / mean2) if mean2 != 0 else float('inf')
                if ratio > 2 or ratio < 0.5:
                    unstable_features.append(f"{col} (ratio={ratio:.2f})")
        
        if unstable_features:
            print_warning(f"–ù–µ—Å—Ç–∞–±–∏–ª—å–Ω—ã–µ –≤–æ –≤—Ä–µ–º–µ–Ω–∏ –ø—Ä–∏–∑–Ω–∞–∫–∏:")
            for feature in unstable_features[:3]:
                print(f"   - {feature}")
            issues.append("temporal_instability")
        else:
            print_success("–í—Ä–µ–º–µ–Ω–Ω–∞—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç—å –≤ –Ω–æ—Ä–º–µ")
    
    return issues

def check_gpu_readiness(df, name):
    """–ü—Ä–æ–≤–µ—Ä–∫–∞ –≥–æ—Ç–æ–≤–Ω–æ—Å—Ç–∏ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è GPU –æ–±—É—á–µ–Ω–∏—è"""
    print(f"\n{Colors.BOLD}–ü—Ä–æ–≤–µ—Ä–∫–∞ GPU –≥–æ—Ç–æ–≤–Ω–æ—Å—Ç–∏ {name}:{Colors.ENDC}")
    
    issues = []
    
    # 1. –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏ CUDA
    if torch.cuda.is_available():
        gpu_name = torch.cuda.get_device_name(0)
        gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1e9
        print_success(f"GPU –¥–æ—Å—Ç—É–ø–Ω–æ: {gpu_name} ({gpu_memory:.1f} GB)")
    else:
        print_error("CUDA –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–æ!")
        issues.append("no_cuda")
    
    # 2. –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–∞–∑–º–µ—Ä–∞ –¥–∞–Ω–Ω—ã—Ö
    memory_mb = df.memory_usage(deep=True).sum() / 1024**2
    if memory_mb > 1000:  # > 1 GB
        print_warning(f"–ë–æ–ª—å—à–æ–π —Ä–∞–∑–º–µ—Ä –¥–∞–Ω–Ω—ã—Ö: {memory_mb:.1f} MB")
        print_info("   –†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è batch_size <= 64")
        issues.append("large_dataset")
    else:
        print_success(f"–†–∞–∑–º–µ—Ä –¥–∞–Ω–Ω—ã—Ö: {memory_mb:.1f} MB (–æ–ø—Ç–∏–º–∞–ª–µ–Ω –¥–ª—è GPU)")
    
    # 3. –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ç–∏–ø–æ–≤ –¥–∞–Ω–Ω—ã—Ö
    float64_cols = df.select_dtypes(include=['float64']).columns
    if len(float64_cols) > 0:
        print_warning(f"–ù–∞–π–¥–µ–Ω–æ {len(float64_cols)} –∫–æ–ª–æ–Ω–æ–∫ —Å float64")
        print_info("   –†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞—Ç—å –≤ float32 –¥–ª—è GPU")
        issues.append("float64_types")
    else:
        print_success("–¢–∏–ø—ã –¥–∞–Ω–Ω—ã—Ö –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω—ã –¥–ª—è GPU")
    
    # 4. –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –≥–æ—Ç–æ–≤–Ω–æ—Å—Ç—å –∫ –±–∞—Ç—á–∞–º
    numeric_cols = df.select_dtypes(include=[np.number]).columns
    batch_ready = True
    for col in numeric_cols:
        if df[col].isna().any():
            print_error(f"–ù–∞–π–¥–µ–Ω—ã NaN –≤ {col} - –Ω–µ–¥–æ–ø—É—Å—Ç–∏–º–æ –¥–ª—è GPU")
            batch_ready = False
            break
        if np.isinf(df[col]).any():
            print_error(f"–ù–∞–π–¥–µ–Ω—ã Inf –≤ {col} - –Ω–µ–¥–æ–ø—É—Å—Ç–∏–º–æ –¥–ª—è GPU")
            batch_ready = False
            break
    
    if batch_ready:
        print_success("–î–∞–Ω–Ω—ã–µ –≥–æ—Ç–æ–≤—ã –¥–ª—è GPU –±–∞—Ç—á–µ–π")
    else:
        print_error("–î–∞–Ω–Ω—ã–µ –ù–ï –≥–æ—Ç–æ–≤—ã –¥–ª—è GPU –æ–±—É—á–µ–Ω–∏—è")
        issues.append("not_batch_ready")
    
    return issues

def generate_detailed_report(all_issues, file_sizes):
    """–ò–°–ü–†–ê–í–õ–ï–ù–ù–ê–Ø –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –¥–µ—Ç–∞–ª—å–Ω–æ–≥–æ –æ—Ç—á–µ—Ç–∞ —Å —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è–º–∏"""
    print_header("üìä –î–ï–¢–ê–õ–¨–ù–´–ô –û–¢–ß–ï–¢ –î–ò–ê–ì–ù–û–°–¢–ò–ö–ò")
    
    total_issues = sum(len(issues) for issues in all_issues.values())
    
    # –ê–Ω–∞–ª–∏–∑ —Ç–∏–ø–æ–≤ –ø—Ä–æ–±–ª–µ–º
    critical_issues = []
    normalization_issues = []
    formula_errors = []
    extreme_value_issues = []
    gpu_issues = []
    warnings = []
    
    for dataset, issues in all_issues.items():
        for issue in issues:
            if 'error' in issue or issue in ['not_batch_ready', 'no_cuda']:
                critical_issues.append(f"{dataset}: {issue}")
            elif 'normalized' in issue or 'normalization' in issue:
                normalization_issues.append(f"{dataset}: {issue}")
            elif 'formula' in issue or 'toxicity' in issue:
                formula_errors.append(f"{dataset}: {issue}")
            elif 'extreme' in issue or 'range_error' in issue:
                extreme_value_issues.append(f"{dataset}: {issue}")
            elif issue in ['large_dataset', 'float64_types']:
                gpu_issues.append(f"{dataset}: {issue}")
            else:
                warnings.append(f"{dataset}: {issue}")
    
    # –ö–†–ò–¢–ò–ß–ï–°–ö–ê–Ø –î–ò–ê–ì–ù–û–°–¢–ò–ö–ê
    if total_issues == 0:
        print_success("üéâ –í–°–ï –ü–†–û–í–ï–†–ö–ò –ü–†–û–ô–î–ï–ù–´ –£–°–ü–ï–®–ù–û!")
        print_success("‚úÖ –î–∞–Ω–Ω—ã–µ –ø–æ–ª–Ω–æ—Å—Ç—å—é –≥–æ—Ç–æ–≤—ã –∫ –æ–±—É—á–µ–Ω–∏—é –º–æ–¥–µ–ª–∏")
        print_info("üöÄ –ú–æ–∂–Ω–æ –∑–∞–ø—É—Å–∫–∞—Ç—å: python main.py --mode train")
    else:
        print_warning(f"‚ö†Ô∏è –û–±–Ω–∞—Ä—É–∂–µ–Ω–æ {total_issues} –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω—ã—Ö –ø—Ä–æ–±–ª–µ–º")
        
        # –î–µ—Ç–∞–ª—å–Ω—ã–π –≤—ã–≤–æ–¥ –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º
        if critical_issues:
            print(f"\nüî¥ –ö–†–ò–¢–ò–ß–ï–°–ö–ò–ï –û–®–ò–ë–ö–ò ({len(critical_issues)}):")
            for issue in critical_issues:
                print_error(f"   {issue}")
        
        if formula_errors:
            print(f"\nüî• –û–®–ò–ë–ö–ò –í –§–û–†–ú–£–õ–ê–• ({len(formula_errors)}):")
            for issue in formula_errors:
                print_error(f"   {issue}")
            print_error("   ‚ö†Ô∏è –ù–ï–û–ë–•–û–î–ò–ú–û –ò–°–ü–†–ê–í–ò–¢–¨ feature_engineering.py!")
        
        if normalization_issues:
            print(f"\nüü† –ü–†–û–ë–õ–ï–ú–´ –ù–û–†–ú–ê–õ–ò–ó–ê–¶–ò–ò ({len(normalization_issues)}):")
            for issue in normalization_issues:
                print_error(f"   {issue}")
            print_error("   ‚ö†Ô∏è –¢–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã –Ω–µ –¥–æ–ª–∂–Ω—ã –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞—Ç—å—Å—è!")
        
        if extreme_value_issues:
            print(f"\nüü° –≠–ö–°–¢–†–ï–ú–ê–õ–¨–ù–´–ï –ó–ù–ê–ß–ï–ù–ò–Ø ({len(extreme_value_issues)}):")
            for issue in extreme_value_issues:
                print_warning(f"   {issue}")
        
        if gpu_issues:
            print(f"\nüîµ GPU –û–ü–¢–ò–ú–ò–ó–ê–¶–ò–Ø ({len(gpu_issues)}):")
            for issue in gpu_issues:
                print_warning(f"   {issue}")
        
        if warnings:
            print(f"\n‚ö™ –ü–†–ï–î–£–ü–†–ï–ñ–î–ï–ù–ò–Ø ({len(warnings)}):")
            for warning in warnings:
                print_warning(f"   {warning}")
    
    # –î–ï–¢–ê–õ–¨–ù–´–ï –†–ï–ö–û–ú–ï–ù–î–ê–¶–ò–ò –ü–û –ò–°–ü–†–ê–í–õ–ï–ù–ò–Æ
    print(f"\nüìã –ü–û–®–ê–ì–û–í–´–ï –†–ï–ö–û–ú–ï–ù–î–ê–¶–ò–ò:")
    
    if formula_errors:
        print_error("   üî• –ü–†–ò–û–†–ò–¢–ï–¢ 1: –ò—Å–ø—Ä–∞–≤–∏—Ç—å —Ñ–æ—Ä–º—É–ª—ã –≤ feature_engineering.py")
        print_error("      - Toxicity: –∏—Å–ø—Ä–∞–≤–∏—Ç—å —Ñ–æ—Ä–º—É–ª–∞ –¥–æ–ª–∂–Ω–∞ –¥–∞–≤–∞—Ç—å –¥–∏–∞–ø–∞–∑–æ–Ω 0.5-1.0")
        print_error("      - bb_width: —Å—á–∏—Ç–∞—Ç—å –∫–∞–∫ –ø—Ä–æ—Ü–µ–Ω—Ç –æ—Ç —Ü–µ–Ω—ã, –∞ –Ω–µ –∞–±—Å–æ–ª—é—Ç–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è") 
        print_error("      - daily_range: –∞–Ω–∞–ª–æ–≥–∏—á–Ω–æ –∫–∞–∫ –ø—Ä–æ—Ü–µ–Ω—Ç –æ—Ç —Ü–µ–Ω—ã")
        print_error("      - –ü–æ—Å–ª–µ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏—è: python main.py --mode data")
        
    elif normalization_issues:
        print_error("   üü† –ü–†–ò–û–†–ò–¢–ï–¢ 1: –ò—Å–ø—Ä–∞–≤–∏—Ç—å –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—é")
        print_error("      - –î–æ–±–∞–≤–∏—Ç—å —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã –≤ exclude_cols")
        print_error("      - RSI, Stochastic, ADX –Ω–µ –¥–æ–ª–∂–Ω—ã –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞—Ç—å—Å—è")
        print_error("      - –ü–æ—Å–ª–µ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏—è: python main.py --mode data")
        
    elif critical_issues:
        print_error("   üî¥ –ü–†–ò–û–†–ò–¢–ï–¢ 1: –£—Å—Ç—Ä–∞–Ω–∏—Ç—å –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏–µ –æ—à–∏–±–∫–∏")
        print_error("      - –ü—Ä–æ–≤–µ—Ä–∏—Ç—å –¥–∏–∞–ø–∞–∑–æ–Ω—ã –∑–Ω–∞—á–µ–Ω–∏–π")
        print_error("      - –ò—Å–ø—Ä–∞–≤–∏—Ç—å NaN/Inf –∑–Ω–∞—á–µ–Ω–∏—è")
        print_error("      - –ü–æ—Å–ª–µ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏—è –ø–æ–≤—Ç–æ—Ä–∏—Ç—å –ø—Ä–æ–≤–µ—Ä–∫—É")
        
    elif extreme_value_issues or gpu_issues:
        print_warning("   üü° –ü–†–ò–û–†–ò–¢–ï–¢ 2: –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è")
        if extreme_value_issues:
            print_info("      - –î–æ–±–∞–≤–∏—Ç—å –∫–ª–∏–ø–ø–∏–Ω–≥ —ç–∫—Å—Ç—Ä–µ–º–∞–ª—å–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π")
            print_info("      - –ü—Ä–æ–≤–µ—Ä–∏—Ç—å —Ñ–æ—Ä–º—É–ª—ã —Ä–∞—Å—á–µ—Ç–∞ –±–æ–ª—å—à–∏—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤")
        if gpu_issues:
            print_info("      - –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞—Ç—å float64 ‚Üí float32")
            print_info("      - –£–º–µ–Ω—å—à–∏—Ç—å batch_size –≤ config.yaml")
            
    elif warnings:
        print_info("   üîµ –ü–†–ò–û–†–ò–¢–ï–¢ 3: –ú–µ–ª–∫–∏–µ —É–ª—É—á—à–µ–Ω–∏—è")
        print_info("      - –ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏—è")
        print_info("      - –ü—Ä–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏ –≤–Ω–µ—Å—Ç–∏ –∫–æ—Ä—Ä–µ–∫—Ç–∏—Ä–æ–≤–∫–∏")
        
    else:
        print_success("   ‚úÖ –í—Å—ë –≥–æ—Ç–æ–≤–æ! –ú–æ–∂–Ω–æ –∑–∞–ø—É—Å–∫–∞—Ç—å –æ–±—É—á–µ–Ω–∏–µ")
    
    # –°–õ–ï–î–£–Æ–©–ò–ï –®–ê–ì–ò
    print(f"\nüöÄ –°–õ–ï–î–£–Æ–©–ò–ï –®–ê–ì–ò:")
    if formula_errors or normalization_issues or critical_issues:
        print_info("   1. üîß –ò—Å–ø—Ä–∞–≤–∏—Ç—å feature_engineering.py —Å–æ–≥–ª–∞—Å–Ω–æ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è–º")
        print_info("   2. üîÑ python main.py --mode data  # –ü–µ—Ä–µ—Å–æ–∑–¥–∞—Ç—å –∫—ç—à")
        print_info("   3. üîç python verify_data_correctness.py  # –ü–æ–≤—Ç–æ—Ä–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞")
        print_info("   4. üöÄ python main.py --mode train  # –ó–∞–ø—É—Å–∫ –æ–±—É—á–µ–Ω–∏—è")
    elif extreme_value_issues or gpu_issues:
        print_info("   1. üéõÔ∏è –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞—Ç—å —Å–æ–≥–ª–∞—Å–Ω–æ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è–º")
        print_info("   2. üîç –ü–æ–≤—Ç–æ—Ä–∏—Ç—å –ø—Ä–æ–≤–µ—Ä–∫—É (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)")
        print_info("   3. üöÄ python main.py --mode train")
    else:
        print_success("   üöÄ python main.py --mode train")
    
    # –°–¢–ê–¢–ò–°–¢–ò–ö–ê –ü–û –§–ê–ô–õ–ê–ú
    print(f"\nüìÅ –ò–ù–§–û–†–ú–ê–¶–ò–Ø –û –§–ê–ô–õ–ê–•:")
    total_size = sum(file_sizes.values())
    for filename, size_mb in file_sizes.items():
        print_info(f"   {filename}: {size_mb:.2f} MB")
    print_info(f"   –û–±—â–∏–π —Ä–∞–∑–º–µ—Ä: {total_size:.2f} MB")
    
    return total_issues


def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω–æ–π –ø—Ä–æ–≤–µ—Ä–∫–∏ –¥–∞–Ω–Ω—ã—Ö"""
    print_header("üîç –ü–û–õ–ù–ê–Ø –î–ò–ê–ì–ù–û–°–¢–ò–ö–ê –ö–û–†–†–ï–ö–¢–ù–û–°–¢–ò –î–ê–ù–ù–´–•")
    print(f"‚è∞ –í—Ä–µ–º—è –∑–∞–ø—É—Å–∫–∞: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"üß† –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è Enhanced Indicator Validator")
    print(f"üöÄ GPU –ø—Ä–æ–≤–µ—Ä–∫–∏ –≤–∫–ª—é—á–µ–Ω—ã")
    
    # 1. –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ñ–∞–π–ª–æ–≤
    files_exist, file_sizes = check_file_existence()
    if not files_exist:
        print_error("\n‚ùå –ù–µ –≤—Å–µ —Ñ–∞–π–ª—ã –Ω–∞–π–¥–µ–Ω—ã! –ó–∞–ø—É—Å—Ç–∏—Ç–µ: python main.py --mode data")
        return
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–∞–∑–º–µ—Ä—ã —Ñ–∞–π–ª–æ–≤
    total_size = sum(file_sizes.values())
    print_info(f"üìä –û–±—â–∏–π —Ä–∞–∑–º–µ—Ä –¥–∞–Ω–Ω—ã—Ö: {total_size:.2f} MB")
    
    # 2. –ó–∞–≥—Ä—É–∑–∫–∞ –∏ –∞–Ω–∞–ª–∏–∑ –¥–∞–Ω–Ω—ã—Ö
    print_header("üìã –ó–ê–ì–†–£–ó–ö–ê –ò –î–ï–¢–ê–õ–¨–ù–´–ô –ê–ù–ê–õ–ò–ó –î–ê–ù–ù–´–•")
    
    all_issues = {}
    
    for file_name in ['train_data.parquet', 'val_data.parquet', 'test_data.parquet']:
        print(f"\n{Colors.BOLD}{'='*70}{Colors.ENDC}")
        print(f"{Colors.BOLD}üî¨ –ê–ù–ê–õ–ò–ó {file_name.upper()}{Colors.ENDC}")
        print(f"{Colors.BOLD}{'='*70}{Colors.ENDC}")
        
        file_path = Path('data/processed') / file_name
        try:
            df = pd.read_parquet(file_path)
            print_success(f"–ó–∞–≥—Ä—É–∂–µ–Ω–æ: {len(df):,} –∑–∞–ø–∏—Å–µ–π, {len(df.columns)} –∫–æ–ª–æ–Ω–æ–∫")
        except Exception as e:
            print_error(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ {file_name}: {e}")
            continue
        
        dataset_name = file_name.split('_')[0].upper()
        issues = []
        
        # –û—Å–Ω–æ–≤–Ω—ã–µ –ø—Ä–æ–≤–µ—Ä–∫–∏
        if not check_data_structure(df, dataset_name):
            issues.append("structure_error")
        
        issues.extend(check_data_quality(df, dataset_name))
        issues.extend(check_critical_indicators(df, dataset_name))
        issues.extend(check_target_distribution(df, dataset_name))
        issues.extend(check_temporal_consistency(df, dataset_name))
        
        # üÜï –†–ê–°–®–ò–†–ï–ù–ù–´–ï –ü–†–û–í–ï–†–ö–ò
        issues.extend(check_advanced_issues(df, dataset_name))
        issues.extend(check_gpu_readiness(df, dataset_name))
        
        all_issues[dataset_name] = issues
        
        # –ö—Ä–∞—Ç–∫–∞—è —Å–≤–æ–¥–∫–∞ –ø–æ –¥–∞—Ç–∞—Å–µ—Ç—É
        if issues:
            print_warning(f"‚ö†Ô∏è {dataset_name}: {len(issues)} –ø—Ä–æ–±–ª–µ–º –æ–±–Ω–∞—Ä—É–∂–µ–Ω–æ")
        else:
            print_success(f"‚úÖ {dataset_name}: –í—Å–µ –ø—Ä–æ–≤–µ—Ä–∫–∏ –ø—Ä–æ–π–¥–µ–Ω—ã!")
    
    # 3. –§–∏–Ω–∞–ª—å–Ω—ã–π –æ—Ç—á–µ—Ç —Å –¥–µ—Ç–∞–ª—å–Ω—ã–º–∏ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è–º–∏
    total_issues = generate_detailed_report(all_issues, file_sizes)
    
    # 4. –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –¥–µ—Ç–∞–ª—å–Ω–æ–≥–æ –æ—Ç—á–µ—Ç–∞
    try:
        # –°–æ–∑–¥–∞–µ–º –¥–µ—Ç–∞–ª—å–Ω—ã–π –æ—Ç—á–µ—Ç —á–µ—Ä–µ–∑ –≤–∞–ª–∏–¥–∞—Ç–æ—Ä
        validator = IndicatorValidator()
        sample_df = pd.read_parquet(Path('data/processed/train_data.parquet'))
        detailed_report = validator.create_validation_report(sample_df)
        
        report_path = Path('logs/data_validation_report.txt')
        report_path.parent.mkdir(exist_ok=True)
        with open(report_path, 'w', encoding='utf-8') as f:
            f.write(detailed_report)
        
        print_info(f"\nüìÑ –î–µ—Ç–∞–ª—å–Ω—ã–π –æ—Ç—á–µ—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {report_path}")
        
    except Exception as e:
        print_warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ–∑–¥–∞—Ç—å –¥–µ—Ç–∞–ª—å–Ω—ã–π –æ—Ç—á–µ—Ç: {e}")
    
    print(f"\n‚è∞ –ü—Ä–æ–≤–µ—Ä–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞: {datetime.now().strftime('%H:%M:%S')}")

def create_validation_report(df, filename):
    """–°–æ–∑–¥–∞–Ω–∏–µ –¥–µ—Ç–∞–ª—å–Ω–æ–≥–æ –æ—Ç—á–µ—Ç–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–∏ –≤ —Ñ–∞–π–ª"""
    report_lines = []
    report_lines.append("=" * 80)
    report_lines.append("üìä –û–¢–ß–ï–¢ –í–ê–õ–ò–î–ê–¶–ò–ò –¢–ï–•–ù–ò–ß–ï–°–ö–ò–• –ò–ù–î–ò–ö–ê–¢–û–†–û–í")
    report_lines.append("=" * 80)
    report_lines.append(f"–î–∞—Ç–∞: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    report_lines.append(f"–†–∞–∑–º–µ—Ä –¥–∞–Ω–Ω—ã—Ö: {len(df):,} –∑–∞–ø–∏—Å–µ–π, {len(df.columns)} –∫–æ–ª–æ–Ω–æ–∫")
    report_lines.append("")
    
    # –û–±—â–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
    numeric_cols = df.select_dtypes(include=[np.number]).columns
    report_lines.append("üìà –û–ë–©–ê–Ø –°–¢–ê–¢–ò–°–¢–ò–ö–ê:")
    report_lines.append(f"   - –ß–∏—Å–ª–æ–≤—ã—Ö –∫–æ–ª–æ–Ω–æ–∫: {len(numeric_cols)}")
    report_lines.append(f"   - NaN –∑–Ω–∞—á–µ–Ω–∏–π: {df.isna().sum().sum()}")
    report_lines.append(f"   - Inf –∑–Ω–∞—á–µ–Ω–∏–π: {sum(np.isinf(df[col]).sum() for col in numeric_cols)}")
    report_lines.append("")
    
    # –î–µ—Ç–∞–ª—å–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–∞–º
    indicators = ['rsi', 'stoch_k', 'stoch_d', 'adx', 'adx_pos', 'adx_neg', 
                 'toxicity', 'bb_position', 'close_position', 'psar_trend',
                 'rsi_oversold', 'rsi_overbought', 'close_vwap_ratio', 
                 'close_open_ratio', 'high_low_ratio', 'bb_width', 'daily_range']
    
    report_lines.append("üìä –î–ï–¢–ê–õ–¨–ù–ê–Ø –°–¢–ê–¢–ò–°–¢–ò–ö–ê –ü–û –ò–ù–î–ò–ö–ê–¢–û–†–ê–ú:")
    for indicator in indicators:
        if indicator in df.columns:
            stats = df[indicator].describe()
            report_lines.append(f"   {indicator}:")
            report_lines.append(f"      Min: {stats['min']:.4f}, Max: {stats['max']:.4f}")
            report_lines.append(f"      Mean: {stats['mean']:.4f}, Std: {stats['std']:.4f}")
            report_lines.append(f"      –ó–∞–ø–∏—Å–µ–π: {len(df[indicator]):,}")
    
    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –æ—Ç—á–µ—Ç–∞
    report_path = Path('logs') / filename
    report_path.parent.mkdir(exist_ok=True)
    
    with open(report_path, 'w', encoding='utf-8') as f:
        f.write('\n'.join(report_lines))
    
    print_info(f"üìÑ –î–µ—Ç–∞–ª—å–Ω—ã–π –æ—Ç—á–µ—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {report_path}")

if __name__ == "__main__":
    main()