"""
–°–∫—Ä–∏–ø—Ç –¥–ª—è –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∏ –¥–∞—Ç–∞—Å–µ—Ç–∞ —Å —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–º–∏ –ø—Ä–∏–∑–Ω–∞–∫–∞–º–∏
–ü—Ä–∏–º–µ–Ω—è–µ—Ç enhanced features –∫ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–º –¥–∞–Ω–Ω—ã–º –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è direction prediction
"""

import pandas as pd
import numpy as np
from pathlib import Path
import argparse
from datetime import datetime
import json
from typing import Dict, List, Optional
from tqdm import tqdm
import warnings
warnings.filterwarnings('ignore')

from data.data_loader import CryptoDataLoader
from data.feature_engineering import FeatureEngineer
from data.enhanced_features import EnhancedFeatureEngineer
from utils.logger import get_logger, setup_logging
from utils.config import load_config
from utils.db_utils import DatabaseManager


class EnhancedDatasetPreparer:
    """–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞ —Å —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–º–∏ –ø—Ä–∏–∑–Ω–∞–∫–∞–º–∏"""
    
    def __init__(self, config: Dict):
        self.config = config
        self.logger = get_logger("EnhancedDatasetPreparer")
        self.db_manager = DatabaseManager(config)
        self.feature_engineer = FeatureEngineer(config)
        self.enhanced_engineer = EnhancedFeatureEngineer()
        
    def prepare_enhanced_dataset(self, 
                               symbols: Optional[List[str]] = None,
                               start_date: Optional[str] = None,
                               end_date: Optional[str] = None,
                               save_to_db: bool = True) -> pd.DataFrame:
        """
        –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞ —Å enhanced features
        
        Args:
            symbols: —Å–ø–∏—Å–æ–∫ —Å–∏–º–≤–æ–ª–æ–≤ (None = –≤—Å–µ –∏–∑ –∫–æ–Ω—Ñ–∏–≥–∞)
            start_date: –Ω–∞—á–∞–ª—å–Ω–∞—è –¥–∞—Ç–∞
            end_date: –∫–æ–Ω–µ—á–Ω–∞—è –¥–∞—Ç–∞
            save_to_db: —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å –≤ –ë–î
            
        Returns:
            DataFrame —Å —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–º–∏ –ø—Ä–∏–∑–Ω–∞–∫–∞–º–∏
        """
        self.logger.info("üöÄ –ù–∞—á–∏–Ω–∞–µ–º –ø–æ–¥–≥–æ—Ç–æ–≤–∫—É enhanced –¥–∞—Ç–∞—Å–µ—Ç–∞...")
        
        # –ó–∞–≥—Ä—É–∑–∫–∞ –±–∞–∑–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö
        data_loader = CryptoDataLoader(self.config)
        
        if symbols is None:
            symbols = self.config['data']['symbols']
        
        self.logger.info(f"üìä –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è {len(symbols)} —Å–∏–º–≤–æ–ª–æ–≤...")
        raw_data = data_loader.load_raw_data(
            symbols=symbols,
            start_date=start_date,
            end_date=end_date
        )
        
        if raw_data.empty:
            self.logger.error("‚ùå –ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏!")
            return pd.DataFrame()
        
        self.logger.info(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(raw_data)} —Å—Ç—Ä–æ–∫ –¥–∞–Ω–Ω—ã—Ö")
        
        # –ë–∞–∑–æ–≤—ã–µ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã
        self.logger.info("üìà –°–æ–∑–¥–∞–Ω–∏–µ –±–∞–∑–æ–≤—ã—Ö —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏—Ö –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–æ–≤...")
        featured_data = self.feature_engineer.create_features(raw_data)
        
        # –ì—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –ø–æ —Å–∏–º–≤–æ–ª–∞–º –¥–ª—è cross-asset features
        all_symbols_data = {}
        for symbol in symbols:
            symbol_data = featured_data[featured_data['symbol'] == symbol].copy()
            all_symbols_data[symbol] = symbol_data
        
        # Enhanced features –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Å–∏–º–≤–æ–ª–∞
        enhanced_dfs = []
        
        for symbol in tqdm(symbols, desc="–î–æ–±–∞–≤–ª–µ–Ω–∏–µ enhanced features"):
            symbol_data = all_symbols_data[symbol].copy()
            
            # –ü—Ä–∏–º–µ–Ω—è–µ–º enhanced features
            enhanced_data = self.enhanced_engineer.create_enhanced_features(
                symbol_data,
                all_symbols_data  # –ü–µ—Ä–µ–¥–∞–µ–º –≤—Å–µ —Å–∏–º–≤–æ–ª—ã –¥–ª—è cross-asset
            )
            
            enhanced_dfs.append(enhanced_data)
        
        # –û–±—ä–µ–¥–∏–Ω—è–µ–º –≤—Å–µ —Å–∏–º–≤–æ–ª—ã
        final_dataset = pd.concat(enhanced_dfs, ignore_index=True)
        
        # –§–∏–Ω–∞–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞
        final_dataset = self._post_process_features(final_dataset)
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –ø—Ä–∏–∑–Ω–∞–∫–∞–º
        self._log_feature_statistics(final_dataset)
        
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ –ë–î
        if save_to_db:
            self._save_to_database(final_dataset)
        
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ —Ñ–∞–π–ª
        self._save_to_file(final_dataset)
        
        self.logger.info(f"‚úÖ Enhanced –¥–∞—Ç–∞—Å–µ—Ç –≥–æ—Ç–æ–≤: {final_dataset.shape}")
        
        return final_dataset
    
    def _post_process_features(self, df: pd.DataFrame) -> pd.DataFrame:
        """–ü–æ—Å—Ç–æ–±—Ä–∞–±–æ—Ç–∫–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤"""
        self.logger.info("üîß –ü–æ—Å—Ç–æ–±—Ä–∞–±–æ—Ç–∫–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤...")
        
        # –£–¥–∞–ª—è–µ–º –≤—Ä–µ–º–µ–Ω–Ω—ã–µ –∫–æ–ª–æ–Ω–∫–∏
        temp_columns = [col for col in df.columns if col.startswith('_temp_')]
        if temp_columns:
            df = df.drop(columns=temp_columns)
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ inf –∑–Ω–∞—á–µ–Ω–∏—è
        numeric_columns = df.select_dtypes(include=[np.number]).columns
        inf_columns = []
        
        for col in numeric_columns:
            if np.isinf(df[col]).any():
                inf_columns.append(col)
                df[col] = df[col].replace([np.inf, -np.inf], np.nan)
        
        if inf_columns:
            self.logger.warning(f"‚ö†Ô∏è –ó–∞–º–µ–Ω–µ–Ω—ã inf –∑–Ω–∞—á–µ–Ω–∏—è –≤ –∫–æ–ª–æ–Ω–∫–∞—Ö: {inf_columns[:5]}...")
        
        # –ó–∞–ø–æ–ª–Ω–µ–Ω–∏–µ –ø—Ä–æ–ø—É—Å–∫–æ–≤
        # –î–ª—è –±–æ–ª—å—à–∏–Ω—Å—Ç–≤–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –∏—Å–ø–æ–ª—å–∑—É–µ–º forward fill
        feature_columns = [col for col in df.columns 
                          if col not in ['id', 'symbol', 'datetime', 'timestamp']
                          and not col.startswith(('target_', 'future_', 'direction_'))]
        
        df[feature_columns] = df.groupby('symbol')[feature_columns].fillna(method='ffill')
        
        # –î–ª—è –æ—Å—Ç–∞–≤—à–∏—Ö—Å—è NaN –∏—Å–ø–æ–ª—å–∑—É–µ–º 0
        df[feature_columns] = df[feature_columns].fillna(0)
        
        # –°–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞
        df = df.sort_values(['symbol', 'datetime']).reset_index(drop=True)
        
        return df
    
    def _log_feature_statistics(self, df: pd.DataFrame):
        """–õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –ø–æ –ø—Ä–∏–∑–Ω–∞–∫–∞–º"""
        self.logger.info("\nüìä –°–¢–ê–¢–ò–°–¢–ò–ö–ê ENHANCED FEATURES:")
        
        # –ü–æ–¥—Å—á–µ—Ç –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º
        feature_categories = {
            'price_based': [],
            'volume_based': [],
            'technical': [],
            'market_regime': [],
            'microstructure': [],
            'cross_asset': [],
            'sentiment': [],
            'other': []
        }
        
        feature_columns = [col for col in df.columns 
                          if col not in ['id', 'symbol', 'datetime', 'timestamp']
                          and not col.startswith(('target_', 'future_', 'direction_'))]
        
        for col in feature_columns:
            if any(x in col for x in ['open', 'high', 'low', 'close', 'vwap', 'price']):
                feature_categories['price_based'].append(col)
            elif any(x in col for x in ['volume', 'turnover', 'trade']):
                feature_categories['volume_based'].append(col)
            elif any(x in col for x in ['rsi', 'macd', 'ema', 'sma', 'bb_', 'atr']):
                feature_categories['technical'].append(col)
            elif any(x in col for x in ['regime', 'wyckoff', 'trend_strength']):
                feature_categories['market_regime'].append(col)
            elif any(x in col for x in ['ofi', 'tick', 'aggressive', 'imbalance']):
                feature_categories['microstructure'].append(col)
            elif any(x in col for x in ['btc_', 'sector_', 'beta_', 'correlation']):
                feature_categories['cross_asset'].append(col)
            elif any(x in col for x in ['fear_greed', 'panic', 'euphoria', 'sentiment']):
                feature_categories['sentiment'].append(col)
            else:
                feature_categories['other'].append(col)
        
        # –í—ã–≤–æ–¥ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
        self.logger.info(f"–í—Å–µ–≥–æ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤: {len(feature_columns)}")
        for category, features in feature_categories.items():
            if features:
                self.logger.info(f"  - {category}: {len(features)} –ø—Ä–∏–∑–Ω–∞–∫–æ–≤")
                # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø—Ä–∏–º–µ—Ä—ã
                examples = features[:3]
                self.logger.info(f"    –ü—Ä–∏–º–µ—Ä—ã: {', '.join(examples)}")
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ü–µ–ª–µ–≤—ã—Ö –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö
        target_columns = [col for col in df.columns if col.startswith(('direction_', 'future_return_'))]
        self.logger.info(f"\n–¶–µ–ª–µ–≤—ã–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ: {len(target_columns)}")
        for target in target_columns[:5]:
            if target.startswith('direction_'):
                value_counts = df[target].value_counts()
                self.logger.info(f"  - {target}: UP={value_counts.get(0, 0)}, "
                               f"DOWN={value_counts.get(1, 0)}, FLAT={value_counts.get(2, 0)}")
    
    def _save_to_database(self, df: pd.DataFrame):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ –±–∞–∑—É –¥–∞–Ω–Ω—ã—Ö"""
        self.logger.info("üíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ –±–∞–∑—É –¥–∞–Ω–Ω—ã—Ö...")
        
        try:
            # –°–æ–∑–¥–∞–µ–º —Ç–∞–±–ª–∏—Ü—É –µ—Å–ª–∏ –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç
            create_table_query = """
            CREATE TABLE IF NOT EXISTS enhanced_market_data (
                id SERIAL PRIMARY KEY,
                symbol VARCHAR(20) NOT NULL,
                datetime TIMESTAMP NOT NULL,
                data JSONB NOT NULL,
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                UNIQUE(symbol, datetime)
            );
            
            CREATE INDEX IF NOT EXISTS idx_enhanced_symbol_datetime 
            ON enhanced_market_data(symbol, datetime);
            """
            
            with self.db_manager.get_connection() as conn:
                with conn.cursor() as cur:
                    cur.execute(create_table_query)
                    conn.commit()
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –¥–∞–Ω–Ω—ã–µ –ø–æ—Ä—Ü–∏—è–º–∏
            batch_size = 1000
            total_saved = 0
            
            for i in range(0, len(df), batch_size):
                batch = df.iloc[i:i+batch_size]
                
                # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ JSON –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è
                records = []
                for _, row in batch.iterrows():
                    record_data = row.to_dict()
                    # –£–±–∏—Ä–∞–µ–º —Å–ª—É–∂–µ–±–Ω—ã–µ –ø–æ–ª—è
                    for field in ['id', 'symbol', 'datetime']:
                        record_data.pop(field, None)
                    
                    records.append((
                        row['symbol'],
                        row['datetime'],
                        json.dumps(record_data, default=str)
                    ))
                
                # –í—Å—Ç–∞–≤–∫–∞ —Å –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ–º –ø—Ä–∏ –∫–æ–Ω—Ñ–ª–∏–∫—Ç–µ
                insert_query = """
                INSERT INTO enhanced_market_data (symbol, datetime, data)
                VALUES (%s, %s, %s)
                ON CONFLICT (symbol, datetime) 
                DO UPDATE SET data = EXCLUDED.data;
                """
                
                with self.db_manager.get_connection() as conn:
                    with conn.cursor() as cur:
                        cur.executemany(insert_query, records)
                        conn.commit()
                
                total_saved += len(records)
                
            self.logger.info(f"‚úÖ –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ {total_saved} –∑–∞–ø–∏—Å–µ–π –≤ –ë–î")
            
        except Exception as e:
            self.logger.error(f"‚ùå –û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –≤ –ë–î: {e}")
    
    def _save_to_file(self, df: pd.DataFrame):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ —Ñ–∞–π–ª –¥–ª—è –±—ã—Å—Ç—Ä–æ–π –∑–∞–≥—Ä—É–∑–∫–∏"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # –°–æ–∑–¥–∞–µ–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –µ—Å–ª–∏ –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç
        data_dir = Path("data/enhanced_datasets")
        data_dir.mkdir(parents=True, exist_ok=True)
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ —Ä–∞–∑–Ω—ã—Ö —Ñ–æ—Ä–º–∞—Ç–∞—Ö
        # Parquet –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ —Ö—Ä–∞–Ω–µ–Ω–∏—è
        parquet_path = data_dir / f"enhanced_dataset_{timestamp}.parquet"
        df.to_parquet(parquet_path, index=False, compression='snappy')
        self.logger.info(f"üíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ –≤ Parquet: {parquet_path}")
        
        # Pickle –¥–ª—è –±—ã—Å—Ç—Ä–æ–π –∑–∞–≥—Ä—É–∑–∫–∏ —Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º —Ç–∏–ø–æ–≤
        pickle_path = data_dir / f"enhanced_dataset_{timestamp}.pkl"
        df.to_pickle(pickle_path)
        self.logger.info(f"üíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ –≤ Pickle: {pickle_path}")
        
        # –ú–µ—Ç–∞–∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è
        meta_info = {
            'timestamp': timestamp,
            'shape': df.shape,
            'symbols': df['symbol'].unique().tolist(),
            'date_range': {
                'start': str(df['datetime'].min()),
                'end': str(df['datetime'].max())
            },
            'features': {
                'total': len([col for col in df.columns 
                            if col not in ['id', 'symbol', 'datetime', 'timestamp']]),
                'enhanced': len([col for col in df.columns 
                               if any(x in col for x in ['regime', 'ofi', 'btc_', 'sentiment'])])
            }
        }
        
        meta_path = data_dir / f"enhanced_dataset_{timestamp}_meta.json"
        with open(meta_path, 'w') as f:
            json.dump(meta_info, f, indent=2)
        self.logger.info(f"üìù –ú–µ—Ç–∞–∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: {meta_path}")
    
    def load_latest_enhanced_dataset(self) -> Optional[pd.DataFrame]:
        """–ó–∞–≥—Ä—É–∑–∫–∞ –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ enhanced –¥–∞—Ç–∞—Å–µ—Ç–∞"""
        data_dir = Path("data/enhanced_datasets")
        
        if not data_dir.exists():
            return None
        
        # –ò—â–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–π pickle —Ñ–∞–π–ª
        pickle_files = list(data_dir.glob("enhanced_dataset_*.pkl"))
        
        if not pickle_files:
            return None
        
        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –≤—Ä–µ–º–µ–Ω–∏ —Å–æ–∑–¥–∞–Ω–∏—è
        latest_file = max(pickle_files, key=lambda x: x.stat().st_mtime)
        
        self.logger.info(f"üìÇ –ó–∞–≥—Ä—É–∑–∫–∞ enhanced –¥–∞—Ç–∞—Å–µ—Ç–∞: {latest_file}")
        
        try:
            df = pd.read_pickle(latest_file)
            self.logger.info(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ {df.shape[0]} —Å—Ç—Ä–æ–∫ —Å {df.shape[1]} –∫–æ–ª–æ–Ω–∫–∞–º–∏")
            return df
        except Exception as e:
            self.logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏: {e}")
            return None


def main():
    parser = argparse.ArgumentParser(description='Prepare Enhanced Dataset')
    parser.add_argument('--config', type=str, default='config/config.yaml',
                       help='Path to config file')
    parser.add_argument('--symbols', nargs='+', default=None,
                       help='Symbols to process (default: all from config)')
    parser.add_argument('--start-date', type=str, default=None,
                       help='Start date (YYYY-MM-DD)')
    parser.add_argument('--end-date', type=str, default=None,
                       help='End date (YYYY-MM-DD)')
    parser.add_argument('--no-save-db', action='store_true',
                       help='Do not save to database')
    parser.add_argument('--load-latest', action='store_true',
                       help='Load latest enhanced dataset instead of creating new')
    
    args = parser.parse_args()
    
    # Setup
    config = load_config(args.config)
    
    # –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    log_dir = Path(f"logs/enhanced_dataset_{timestamp}")
    log_dir.mkdir(parents=True, exist_ok=True)
    
    setup_logging(log_dir, "enhanced_dataset")
    logger = get_logger("PrepareEnhancedDataset")
    
    preparer = EnhancedDatasetPreparer(config)
    
    if args.load_latest:
        # –ó–∞–≥—Ä—É–∑–∫–∞ —Å—É—â–µ—Å—Ç–≤—É—é—â–µ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞
        logger.info("üìÇ –ó–∞–≥—Ä—É–∑–∫–∞ –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ enhanced –¥–∞—Ç–∞—Å–µ—Ç–∞...")
        df = preparer.load_latest_enhanced_dataset()
        
        if df is not None:
            logger.info("‚úÖ –î–∞—Ç–∞—Å–µ—Ç —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω!")
            # –ú–æ–∂–Ω–æ –¥–æ–±–∞–≤–∏—Ç—å –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—É—é –æ–±—Ä–∞–±–æ—Ç–∫—É
        else:
            logger.error("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –¥–∞—Ç–∞—Å–µ—Ç")
    else:
        # –°–æ–∑–¥–∞–Ω–∏–µ –Ω–æ–≤–æ–≥–æ enhanced –¥–∞—Ç–∞—Å–µ—Ç–∞
        logger.info("üöÄ –°–æ–∑–¥–∞–Ω–∏–µ –Ω–æ–≤–æ–≥–æ enhanced –¥–∞—Ç–∞—Å–µ—Ç–∞...")
        
        # –ï—Å–ª–∏ –Ω–µ —É–∫–∞–∑–∞–Ω—ã —Å–∏–º–≤–æ–ª—ã, –∏—Å–ø–æ–ª—å–∑—É–µ–º —Ç–æ–ø –∏–∑ –∫–æ–Ω—Ñ–∏–≥–∞
        if args.symbols is None and config.get('data', {}).get('max_symbols'):
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω—ã–π –Ω–∞–±–æ—Ä –¥–ª—è —Ç–µ—Å—Ç–∞
            args.symbols = config['data']['symbols'][:config['data']['max_symbols']]
        
        df = preparer.prepare_enhanced_dataset(
            symbols=args.symbols,
            start_date=args.start_date,
            end_date=args.end_date,
            save_to_db=not args.no_save_db
        )
        
        if not df.empty:
            logger.info("‚úÖ Enhanced –¥–∞—Ç–∞—Å–µ—Ç —É—Å–ø–µ—à–Ω–æ —Å–æ–∑–¥–∞–Ω!")
            logger.info(f"üìä –†–∞–∑–º–µ—Ä: {df.shape}")
            logger.info(f"üìÖ –ü–µ—Ä–∏–æ–¥: {df['datetime'].min()} - {df['datetime'].max()}")
        else:
            logger.error("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ–∑–¥–∞—Ç—å –¥–∞—Ç–∞—Å–µ—Ç")


if __name__ == "__main__":
    main()