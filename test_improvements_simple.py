"""
–£–ø—Ä–æ—â–µ–Ω–Ω—ã–π —Ç–µ—Å—Ç —É–ª—É—á—à–µ–Ω–∏–π –º–æ–¥–µ–ª–∏
"""

import torch
import numpy as np
import pandas as pd
import yaml
from pathlib import Path

# –ò–º–ø–æ—Ä—Ç—ã –∏–∑ –ø—Ä–æ–µ–∫—Ç–∞
from models.patchtst_improved import ImprovedPatchTST
from data.target_scaler import TargetScaler
from utils.logger import get_logger


def test_model_improvements():
    """–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —É–ª—É—á—à–µ–Ω–∏–π –º–æ–¥–µ–ª–∏"""
    logger = get_logger("TestImproved")
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é
    with open('config/config.yaml', 'r') as f:
        config = yaml.safe_load(f)
    
    logger.info("üöÄ –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —É–ª—É—á—à–µ–Ω–∏–π –º–æ–¥–µ–ª–∏")
    
    # 1. –°–æ–∑–¥–∞–µ–º —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏–µ –¥–∞–Ω–Ω—ã–µ –¥–ª—è —Ç–µ—Å—Ç–∞
    logger.info("üìä –°–æ–∑–¥–∞–Ω–∏–µ —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö...")
    
    batch_size = 32
    context_window = config['model']['context_window']
    n_features = 86  # –¢–∏–ø–∏—á–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
    target_window = config['model']['target_window']
    
    # –°–æ–∑–¥–∞–µ–º –≤—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
    inputs = torch.randn(batch_size, context_window, n_features)
    targets = torch.randn(batch_size, target_window, 1)
    
    logger.info(f"Input shape: {inputs.shape}")
    logger.info(f"Target shape: {targets.shape}")
    
    # 2. –°–æ–∑–¥–∞–Ω–∏–µ —É–ª—É—á—à–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏
    logger.info("ü§ñ –°–æ–∑–¥–∞–Ω–∏–µ —É–ª—É—á—à–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏ PatchTST...")
    
    model_params = {
        'c_in': n_features,
        'c_out': 1,
        'context_window': context_window,
        'target_window': target_window,
        'patch_len': config['model']['patch_len'],
        'stride': config['model']['stride'],
        'd_model': config['model']['d_model'],
        'n_heads': config['model']['n_heads'],
        'd_ff': config['model']['d_ff'],
        'n_layers': config['model']['e_layers'],
        'dropout': config['model']['dropout'],
        'activation': config['model']['activation']
    }
    
    model = ImprovedPatchTST(**model_params)
    logger.info(f"–ü–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –º–æ–¥–µ–ª–∏: {sum(p.numel() for p in model.parameters()):,}")
    
    # 3. –¢–µ—Å—Ç forward pass
    logger.info("üîç –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ forward pass...")
    
    model.eval()
    with torch.no_grad():
        outputs = model(inputs)
        
    logger.info(f"Output shape: {outputs.shape}")
    logger.info(f"Output statistics: mean={outputs.mean():.4f}, std={outputs.std():.4f}")
    
    # 4. –¢–µ—Å—Ç loss
    criterion = torch.nn.MSELoss()
    loss = criterion(outputs, targets)
    logger.info(f"Initial loss: {loss.item():.4f}")
    
    # 5. –¢–µ—Å—Ç –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏—è —Ü–µ–ª–µ–≤–æ–π –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π
    logger.info("\nüéØ –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏—è —Ü–µ–ª–µ–≤–æ–π –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π...")
    
    # –°–æ–∑–¥–∞–µ–º —Ç–µ—Å—Ç–æ–≤—ã–µ —Ü–µ–ª–µ–≤—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è –ø–æ—Ö–æ–∂–∏–µ –Ω–∞ future_return_4
    test_targets = np.random.normal(0.006, 1.2, size=10000)  # mean=0.006, std=1.2
    
    scaler = TargetScaler(method='robust', clip_quantiles=(0.01, 0.99))
    scaler.fit(test_targets)
    
    scaled_targets = scaler.transform(test_targets)
    logger.info(f"–ü–æ—Å–ª–µ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏—è - mean: {np.mean(scaled_targets):.4f}, std: {np.std(scaled_targets):.4f}")
    
    # 6. –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä
    logger.info("\nüìä –°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Å –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–æ–π PatchTST...")
    
    try:
        from models.patchtst import PatchTST
        original_model = PatchTST(**model_params)
        original_params = sum(p.numel() for p in original_model.parameters())
        improved_params = sum(p.numel() for p in model.parameters())
        
        logger.info(f"–û—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å: {original_params:,} –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤")
        logger.info(f"–£–ª—É—á—à–µ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å: {improved_params:,} –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤")
        logger.info(f"–£–≤–µ–ª–∏—á–µ–Ω–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤: {(improved_params - original_params):,} ({(improved_params/original_params - 1)*100:.1f}%)")
        
        # –¢–µ—Å—Ç –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–æ–π –º–æ–¥–µ–ª–∏
        with torch.no_grad():
            original_outputs = original_model(inputs)
            original_loss = criterion(original_outputs, targets)
        
        logger.info(f"\n–û—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å loss: {original_loss.item():.4f}")
        logger.info(f"–£–ª—É—á—à–µ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å loss: {loss.item():.4f}")
        
    except Exception as e:
        logger.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å —Å—Ä–∞–≤–Ω–∏—Ç—å —Å –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–æ–π –º–æ–¥–µ–ª—å—é: {e}")
    
    # 7. –¢–µ—Å—Ç –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤
    logger.info("\nüîß –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤...")
    
    model.train()
    optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)
    
    # Forward
    outputs = model(inputs)
    loss = criterion(outputs, targets)
    
    # Backward
    loss.backward()
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤
    grad_norms = []
    for name, param in model.named_parameters():
        if param.grad is not None:
            grad_norm = param.grad.data.norm(2).item()
            grad_norms.append(grad_norm)
            if grad_norm > 10:
                logger.warning(f"–ë–æ–ª—å—à–æ–π –≥—Ä–∞–¥–∏–µ–Ω—Ç –≤ {name}: {grad_norm:.4f}")
    
    logger.info(f"–°—Ä–µ–¥–Ω—è—è –Ω–æ—Ä–º–∞ –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤: {np.mean(grad_norms):.4f}")
    logger.info(f"–ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –Ω–æ—Ä–º–∞ –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤: {np.max(grad_norms):.4f}")
    
    logger.info("\n‚úÖ –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ —É—Å–ø–µ—à–Ω–æ!")
    
    return model


if __name__ == "__main__":
    test_model_improvements()