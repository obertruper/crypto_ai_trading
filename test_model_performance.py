#!/usr/bin/env python3
"""
–ò–∑–æ–ª–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Å–∫—Ä–∏–ø—Ç –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –º–æ–¥–µ–ª–∏ PatchTST
–ù–ï –≤–ª–∏—è–µ—Ç –Ω–∞ –ø—Ä–æ—Ü–µ—Å—Å –æ–±—É—á–µ–Ω–∏—è - –ø–æ–ª–Ω–æ—Å—Ç—å—é –Ω–µ–∑–∞–≤–∏—Å–∏–º!
"""

import torch
import numpy as np
import pandas as pd
import yaml
from pathlib import Path
from datetime import datetime
import warnings
warnings.filterwarnings('ignore')

# –ò–º–ø–æ—Ä—Ç—ã –ø—Ä–æ–µ–∫—Ç–∞
from models.patchtst_unified import UnifiedPatchTSTForTrading as UnifiedPatchTST
from trading.model_adapter import ModelOutputAdapter
from trading.backtester import Backtester
from trading.signals import SignalGenerator
from trading.risk_manager import RiskManager
from data.data_loader import CryptoDataLoader

print("üöÄ –ó–∞–ø—É—Å–∫ –∏–∑–æ–ª–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –º–æ–¥–µ–ª–∏...")
print("=" * 80)

# –ó–∞–≥—Ä—É–∑–∫–∞ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
with open('config/config.yaml', 'r') as f:
    config = yaml.safe_load(f)

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f"üì± –£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {device}")

# 1. –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏
print("\nüì• –ó–∞–≥—Ä—É–∑–∫–∞ –æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏...")
checkpoint_path = 'models_saved/best_model.pth'
checkpoint = torch.load(checkpoint_path, map_location=device)

# –°–æ–∑–¥–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏ - UnifiedPatchTSTForTrading –ø—Ä–∏–Ω–∏–º–∞–µ—Ç config —Ü–µ–ª–∏–∫–æ–º
model = UnifiedPatchTST(checkpoint['config'])

# –ó–∞–≥—Ä—É–∑–∫–∞ –≤–µ—Å–æ–≤
model.load_state_dict(checkpoint['model_state_dict'])
model.to(device)
model.eval()

print(f"‚úÖ –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞. –≠–ø–æ—Ö–∞: {checkpoint.get('epoch', 'N/A')}")
print(f"üìä Val Loss: {checkpoint.get('val_loss', 'N/A'):.6f}")

# 2. –ó–∞–≥—Ä—É–∑–∫–∞ —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö
print("\nüìä –ó–∞–≥—Ä—É–∑–∫–∞ —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö...")
data_loader = CryptoDataLoader(config)

# –ò—Å–ø–æ–ª—å–∑—É–µ–º –∫—ç—à–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –µ—Å–ª–∏ –µ—Å—Ç—å
test_data_path = Path('data/processed/test_data.parquet')
if test_data_path.exists():
    test_df = pd.read_parquet(test_data_path)
    print(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(test_df)} –∑–∞–ø–∏—Å–µ–π –∏–∑ –∫—ç—à–∞")
else:
    print("‚ö†Ô∏è –ö—ç—à–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã, –∑–∞–≥—Ä—É–∂–∞–µ–º –∏–∑ –ë–î...")
    test_df = data_loader.load_test_data()

# –û–≥—Ä–∞–Ω–∏—á–∏–º—Å—è –ø–æ—Å–ª–µ–¥–Ω–∏–º–∏ 30 –¥–Ω—è–º–∏ –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ —Ç–µ—Å—Ç–∞
test_df_recent = test_df.sort_values('datetime').tail(30 * 24 * 4)  # 30 –¥–Ω–µ–π * 24 —á–∞—Å–∞ * 4 (15-–º–∏–Ω –∏–Ω—Ç–µ—Ä–≤–∞–ª—ã)
print(f"üìÖ –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –Ω–∞ –ø–æ—Å–ª–µ–¥–Ω–∏—Ö {len(test_df_recent)} –∑–∞–ø–∏—Å—è—Ö")

# 3. –°–æ–∑–¥–∞–Ω–∏–µ –∞–¥–∞–ø—Ç–µ—Ä–∞ –∏ –ø–æ–ª—É—á–µ–Ω–∏–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π
print("\nüîÆ –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π –º–æ–¥–µ–ª–∏...")
adapter = ModelOutputAdapter()

# –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –º–æ–¥–µ–ª–∏
# –ë–µ—Ä–µ–º –ø—Ä–∏–∑–Ω–∞–∫–∏ (–≤—Å–µ –∫–æ–ª–æ–Ω–∫–∏ –∫—Ä–æ–º–µ —Ü–µ–ª–µ–≤—ã—Ö –∏ –º–µ—Ç–∞-–¥–∞–Ω–Ω—ã—Ö)
feature_columns = [col for col in test_df_recent.columns 
                  if not col.startswith(('future_', 'direction_', 'volatility_', 'volume_change_', 'price_range_'))
                  and col not in ['datetime', 'symbol', 'open', 'high', 'low', 'close', 'volume']]

# –°–æ–∑–¥–∞–µ–º –±–∞—Ç—á–∏ –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π
batch_size = 1024
predictions_list = []
symbols_list = []

print(f"üìä –ù–∞–π–¥–µ–Ω–æ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤: {len(feature_columns)}")

# –ú–æ–¥–µ–ª—å –æ–±—É—á–µ–Ω–∞ –Ω–∞ 240 –ø—Ä–∏–∑–Ω–∞–∫–∞—Ö, –±–µ—Ä–µ–º –ø–µ—Ä–≤—ã–µ 240
if len(feature_columns) > 240:
    print(f"‚ö†Ô∏è –ú–æ–¥–µ–ª—å –æ–±—É—á–µ–Ω–∞ –Ω–∞ 240 –ø—Ä–∏–∑–Ω–∞–∫–∞—Ö, –æ–±—Ä–µ–∑–∞–µ–º —Å {len(feature_columns)} –¥–æ 240")
    feature_columns = feature_columns[:240]
    
print(f"üì¶ –û–±—Ä–∞–±–æ—Ç–∫–∞ –±–∞—Ç—á–µ–π (—Ä–∞–∑–º–µ—Ä –±–∞—Ç—á–∞: {batch_size})...")

with torch.no_grad():
    for i in range(0, len(test_df_recent), batch_size):
        batch_df = test_df_recent.iloc[i:i+batch_size]
        
        # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ —Å –æ–±—Ä–∞–±–æ—Ç–∫–æ–π —Ç–∏–ø–æ–≤
        try:
            # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤ float, –∑–∞–º–µ–Ω—è—è –Ω–µ—á–∏—Å–ª–æ–≤—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è –Ω–∞ 0
            features_np = batch_df[feature_columns].values.astype(np.float32)
        except:
            # –ï—Å–ª–∏ –µ—Å—Ç—å –ø—Ä–æ–±–ª–µ–º—ã —Å —Ç–∏–ø–∞–º–∏, –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –ø–æ–∫–æ–ª–æ–Ω–æ—á–Ω–æ
            features_list = []
            for col in feature_columns:
                try:
                    col_values = pd.to_numeric(batch_df[col], errors='coerce').fillna(0).values
                    features_list.append(col_values)
                except:
                    # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –ø—Ä–æ–±–ª–µ–º–Ω—É—é –∫–æ–ª–æ–Ω–∫—É
                    print(f"‚ö†Ô∏è –ü—Ä–æ–ø—É—â–µ–Ω–∞ –∫–æ–ª–æ–Ω–∫–∞ {col}")
                    features_list.append(np.zeros(len(batch_df)))
            features_np = np.column_stack(features_list).astype(np.float32)
            
        features = torch.tensor(features_np, dtype=torch.float32).to(device)
        
        # –î–æ–±–∞–≤–ª—è–µ–º —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –¥–ª—è sequence length –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
        if len(features.shape) == 2:
            features = features.unsqueeze(1)  # [batch, 1, features]
        
        # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ
        outputs = model(features)
        
        predictions_list.append(outputs.cpu())
        symbols_list.extend(batch_df['symbol'].tolist())
        
        if (i // batch_size + 1) % 10 == 0:
            print(f"   –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {min(i + batch_size, len(test_df_recent))}/{len(test_df_recent)}")

# –û–±—ä–µ–¥–∏–Ω—è–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
all_predictions = torch.cat(predictions_list, dim=0)
print(f"‚úÖ –ü–æ–ª—É—á–µ–Ω–æ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π: {all_predictions.shape}")

# 4. –ê–¥–∞–ø—Ç–∞—Ü–∏—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π
print("\nüîÑ –ê–¥–∞–ø—Ç–∞—Ü–∏—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π –¥–ª—è —Ç–æ—Ä–≥–æ–≤—ã—Ö —Å–∏–≥–Ω–∞–ª–æ–≤...")
adapted_predictions = adapter.adapt_model_outputs(all_predictions, symbols_list)

# –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ —Å–∏–º–≤–æ–ª–∞–º
print(f"\nüìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π –ø–æ —Å–∏–º–≤–æ–ª–∞–º:")
for symbol, preds in adapted_predictions.items():
    print(f"   {symbol}: –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–µ={preds['direction']}, —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å={preds['confidence']:.3f}")

# 5. –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ç–æ—Ä–≥–æ–≤—ã—Ö —Å–∏–≥–Ω–∞–ª–æ–≤
print("\nüìà –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ç–æ—Ä–≥–æ–≤—ã—Ö —Å–∏–≥–Ω–∞–ª–æ–≤...")
signal_generator = SignalGenerator(config)

# –ú–æ–¥–∏—Ñ–∏—Ü–∏—Ä—É–µ–º SignalGenerator —á—Ç–æ–±—ã –ø—Ä–∏–Ω–∏–º–∞—Ç—å –Ω–∞—à–∏ –∞–¥–∞–ø—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
# –ü–µ—Ä–µ–æ–ø—Ä–µ–¥–µ–ª—è–µ–º –º–µ—Ç–æ–¥ _extract_symbol_predictions
original_extract = signal_generator._extract_symbol_predictions
signal_generator._extract_symbol_predictions = lambda preds, symbol: preds.get(symbol)

# –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —Å–∏–≥–Ω–∞–ª—ã
signals = signal_generator.generate_signals(
    predictions=adapted_predictions,
    market_data=test_df_recent,
    features=test_df_recent
)

print(f"‚úÖ –°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–æ {len(signals)} —Ç–æ—Ä–≥–æ–≤—ã—Ö —Å–∏–≥–Ω–∞–ª–æ–≤")

# 6. –ó–∞–ø—É—Å–∫ –±—ç–∫—Ç–µ—Å—Ç–∏–Ω–≥–∞
print("\nüí∞ –ó–∞–ø—É—Å–∫ –±—ç–∫—Ç–µ—Å—Ç–∏–Ω–≥–∞...")
backtester = Backtester(config)

# –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –±—ç–∫—Ç–µ—Å—Ç–µ—Ä–∞
market_data = test_df_recent[['datetime', 'symbol', 'open', 'high', 'low', 'close', 'volume']].copy()

# –ó–∞–ø—É—Å–∫–∞–µ–º –±—ç–∫—Ç–µ—Å—Ç–∏–Ω–≥
try:
    results = backtester.run_backtest(
        market_data=market_data,
        features=test_df_recent,
        model_predictions=adapted_predictions
    )
    
    # 7. –û—Ç–æ–±—Ä–∞–∂–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    print("\n" + "=" * 80)
    print("üìä –†–ï–ó–£–õ–¨–¢–ê–¢–´ –¢–ï–°–¢–ò–†–û–í–ê–ù–ò–Ø:")
    print("=" * 80)
    
    print(f"\nüíµ –§–∏–Ω–∞–Ω—Å–æ–≤—ã–µ –ø–æ–∫–∞–∑–∞—Ç–µ–ª–∏:")
    print(f"   –ù–∞—á–∞–ª—å–Ω—ã–π –∫–∞–ø–∏—Ç–∞–ª: ${results['initial_capital']:,.2f}")
    print(f"   –§–∏–Ω–∞–ª—å–Ω—ã–π –∫–∞–ø–∏—Ç–∞–ª: ${results['final_capital']:,.2f}")
    print(f"   –û–±—â–∞—è –¥–æ—Ö–æ–¥–Ω–æ—Å—Ç—å: {results['total_return_pct']:.2f}%")
    
    print(f"\nüìà –ú–µ—Ç—Ä–∏–∫–∏ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏:")
    print(f"   –ö–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç –®–∞—Ä–ø–∞: {results['sharpe_ratio']:.2f}")
    print(f"   –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –ø—Ä–æ—Å–∞–¥–∫–∞: {results['max_drawdown_pct']:.2f}%")
    print(f"   Win Rate: {results['win_rate_pct']:.2f}%")
    print(f"   Profit Factor: {results.get('profit_factor', 0):.2f}")
    
    print(f"\nüî¢ –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ —Ç–æ—Ä–≥–æ–≤–ª–∏:")
    print(f"   –í—Å–µ–≥–æ —Å–¥–µ–ª–æ–∫: {results['total_trades']}")
    print(f"   –ü—Ä–∏–±—ã–ª—å–Ω—ã—Ö: {results.get('winning_trades', 0)}")
    print(f"   –£–±—ã—Ç–æ—á–Ω—ã—Ö: {results.get('losing_trades', 0)}")
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
    output_dir = Path('experiments/backtest_results')
    output_dir.mkdir(parents=True, exist_ok=True)
    
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    results_file = output_dir / f'test_results_{timestamp}.yaml'
    
    with open(results_file, 'w') as f:
        yaml.dump(results, f, default_flow_style=False)
    
    print(f"\nüíæ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤: {results_file}")
    
    # –û—Ü–µ–Ω–∫–∞ –≥–æ—Ç–æ–≤–Ω–æ—Å—Ç–∏ –∫ production
    print("\n" + "=" * 80)
    print("üéØ –û–¶–ï–ù–ö–ê –ì–û–¢–û–í–ù–û–°–¢–ò –ö PRODUCTION:")
    print("=" * 80)
    
    min_sharpe = config['validation']['min_sharpe_ratio']
    min_win_rate = config['validation']['min_win_rate'] * 100
    max_dd = config['validation']['max_drawdown'] * 100
    
    checks = {
        'Sharpe Ratio': (results['sharpe_ratio'] >= min_sharpe, f">= {min_sharpe}"),
        'Win Rate': (results['win_rate_pct'] >= min_win_rate, f">= {min_win_rate}%"),
        'Max Drawdown': (results['max_drawdown_pct'] <= max_dd, f"<= {max_dd}%"),
        '–ü–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω–∞—è –¥–æ—Ö–æ–¥–Ω–æ—Å—Ç—å': (results['total_return_pct'] > 0, "> 0%")
    }
    
    all_passed = True
    for metric, (passed, threshold) in checks.items():
        status = "‚úÖ PASS" if passed else "‚ùå FAIL"
        print(f"   {metric}: {status} (—Ç—Ä–µ–±—É–µ—Ç—Å—è {threshold})")
        if not passed:
            all_passed = False
    
    print("\n" + "=" * 80)
    if all_passed:
        print("‚úÖ –ú–û–î–ï–õ–¨ –ì–û–¢–û–í–ê –ö –ó–ê–ü–£–°–ö–£ –ù–ê –í–°–ï –ú–û–ù–ï–¢–´!")
        print("   –í—Å–µ –∫—Ä–∏—Ç–µ—Ä–∏–∏ –≤–∞–ª–∏–¥–∞—Ü–∏–∏ –ø—Ä–æ–π–¥–µ–Ω—ã —É—Å–ø–µ—à–Ω–æ.")
    else:
        print("‚ö†Ô∏è –ú–û–î–ï–õ–¨ –¢–†–ï–ë–£–ï–¢ –î–û–ü–û–õ–ù–ò–¢–ï–õ–¨–ù–û–ô –û–ü–¢–ò–ú–ò–ó–ê–¶–ò–ò")
        print("   –ù–µ–∫–æ—Ç–æ—Ä—ã–µ –∫—Ä–∏—Ç–µ—Ä–∏–∏ –Ω–µ –≤—ã–ø–æ–ª–Ω–µ–Ω—ã.")
    print("=" * 80)
    
except Exception as e:
    print(f"\n‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –±—ç–∫—Ç–µ—Å—Ç–∏–Ω–≥–µ: {str(e)}")
    print("   –ü—Ä–æ–≤–µ—Ä—å—Ç–µ —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å –∞–¥–∞–ø—Ç–µ—Ä–∞ —Å –±—ç–∫—Ç–µ—Å—Ç–µ—Ä–æ–º")
    
    # –ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π
    print("\nüìä –ê–Ω–∞–ª–∏–∑ –∫–∞—á–µ—Å—Ç–≤–∞ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π:")
    
    # –°—á–∏—Ç–∞–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –ø–æ –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏—è–º
    long_count = sum(1 for p in adapted_predictions.values() if p['direction'] == 'long')
    short_count = len(adapted_predictions) - long_count
    avg_confidence = np.mean([p['confidence'] for p in adapted_predictions.values()])
    
    print(f"   Long —Å–∏–≥–Ω–∞–ª–æ–≤: {long_count} ({long_count/len(adapted_predictions)*100:.1f}%)")
    print(f"   Short —Å–∏–≥–Ω–∞–ª–æ–≤: {short_count} ({short_count/len(adapted_predictions)*100:.1f}%)")
    print(f"   –°—Ä–µ–¥–Ω—è—è —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: {avg_confidence:.3f}")
    
    if avg_confidence > 0.6:
        print("\n‚úÖ –ú–æ–¥–µ–ª—å –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç —Ö–æ—Ä–æ—à—É—é —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å –≤ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è—Ö")
    else:
        print("\n‚ö†Ô∏è –ú–æ–¥–µ–ª—å –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç –Ω–∏–∑–∫—É—é —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å, —Ç—Ä–µ–±—É–µ—Ç—Å—è –¥–æ—Ä–∞–±–æ—Ç–∫–∞")

print("\nüèÅ –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ!")