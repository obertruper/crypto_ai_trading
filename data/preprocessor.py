"""
–ü—Ä–µ–ø—Ä–æ—Ü–µ—Å—Å–∏–Ω–≥ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –º–æ–¥–µ–ª–µ–π
"""

import pandas as pd
import numpy as np
from typing import Dict, List, Optional, Tuple
from sklearn.preprocessing import StandardScaler, RobustScaler, MinMaxScaler
from sklearn.impute import SimpleImputer, KNNImputer
import warnings
warnings.filterwarnings('ignore')

from utils.logger import get_logger


class DataPreprocessor:
    """–ö–ª–∞—Å—Å –¥–ª—è –ø—Ä–µ–ø—Ä–æ—Ü–µ—Å—Å–∏–Ω–≥–∞ –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ä—è–¥–æ–≤"""
    
    def __init__(self, config: Dict):
        """
        Args:
            config: –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –ø—Ä–µ–ø—Ä–æ—Ü–µ—Å—Å–∏–Ω–≥–∞
        """
        self.config = config
        self.logger = get_logger("DataPreprocessor")
        
        # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã
        self.scaling_method = config.get('preprocessing', {}).get('scaling_method', 'robust')
        self.handle_missing = config.get('preprocessing', {}).get('handle_missing', 'interpolate')
        self.remove_outliers = config.get('preprocessing', {}).get('remove_outliers', True)
        self.outlier_threshold = config.get('preprocessing', {}).get('outlier_threshold', 3)
        
        # –°–∫–µ–π–ª–µ—Ä—ã –¥–ª—è —Ä–∞–∑–Ω—ã—Ö —Ç–∏–ø–æ–≤ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
        self.scalers = {}
        self.feature_groups = {}
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        self.preprocessing_stats = {}
        
    def fit(self, data: pd.DataFrame, exclude_targets: bool = True) -> 'DataPreprocessor':
        """
        –û–±—É—á–µ–Ω–∏–µ –ø—Ä–µ–ø—Ä–æ—Ü–µ—Å—Å–æ—Ä–∞ –Ω–∞ –¥–∞–Ω–Ω—ã—Ö (–¢–û–õ–¨–ö–û –Ω–∞ –æ–±—É—á–∞—é—â–µ–π –≤—ã–±–æ—Ä–∫–µ!)
        
        Args:
            data: DataFrame —Å –¥–∞–Ω–Ω—ã–º–∏ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è (–¢–û–õ–¨–ö–û train!)
            exclude_targets: –∏—Å–∫–ª—é—á–∏—Ç—å —Ü–µ–ª–µ–≤—ã–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –∏–∑ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏
            
        Returns:
            self
        """
        self.logger.info("–û–±—É—á–µ–Ω–∏–µ –ø—Ä–µ–ø—Ä–æ—Ü–µ—Å—Å–æ—Ä–∞ –Ω–∞ TRAIN –¥–∞–Ω–Ω—ã—Ö...")
        
        # –í–ê–ñ–ù–û: –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ —ç—Ç–æ train –¥–∞–Ω–Ω—ã–µ
        if hasattr(data, 'attrs') and data.attrs.get('dataset_type') != 'train':
            self.logger.warning("‚ö†Ô∏è –í–ù–ò–ú–ê–ù–ò–ï: fit() –¥–æ–ª–∂–µ–Ω –≤—ã–∑—ã–≤–∞—Ç—å—Å—è –¢–û–õ–¨–ö–û –Ω–∞ train –¥–∞–Ω–Ω—ã—Ö!")
        
        # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –≥—Ä—É–ø–ø –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
        self._identify_feature_groups(data, exclude_targets=exclude_targets)
        
        # –û–±—É—á–µ–Ω–∏–µ —Å–∫–µ–π–ª–µ—Ä–æ–≤ –¥–ª—è –∫–∞–∂–¥–æ–π –≥—Ä—É–ø–ø—ã
        for group_name, features in self.feature_groups.items():
            if features:
                group_data = data[features].select_dtypes(include=[np.number])
                
                if len(group_data.columns) > 0:
                    # –í—ã–±–æ—Ä —Å–∫–µ–π–ª–µ—Ä–∞ –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —Ç–∏–ø–∞ –¥–∞–Ω–Ω—ã—Ö
                    if group_name == 'price':
                        scaler = RobustScaler()  # –£—Å—Ç–æ–π—á–∏–≤ –∫ –≤—ã–±—Ä–æ—Å–∞–º
                    elif group_name == 'volume':
                        scaler = MinMaxScaler()  # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –≤ [0, 1]
                    elif group_name == 'returns':
                        scaler = StandardScaler()  # –°—Ç–∞–Ω–¥–∞—Ä—Ç–∏–∑–∞—Ü–∏—è
                    elif group_name == 'indicators':
                        # –î–ª—è –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–æ–≤ —Å –∏–∑–≤–µ—Å—Ç–Ω—ã–º–∏ –¥–∏–∞–ø–∞–∑–æ–Ω–∞–º–∏ –Ω–µ –Ω–æ—Ä–º–∞–ª–∏–∑—É–µ–º
                        # RSI, Stochastic –∏ —Ç.–¥. —É–∂–µ –≤ –¥–∏–∞–ø–∞–∑–æ–Ω–µ [0, 100]
                        continue
                    elif group_name == 'fixed_range':
                        # –ù–µ –Ω–æ—Ä–º–∞–ª–∏–∑—É–µ–º –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã —Å —Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã–º–∏ –¥–∏–∞–ø–∞–∑–æ–Ω–∞–º–∏
                        # RSI [0, 100], Stochastic [0, 100], ADX [0, 100], 
                        # CCI [-200, 200], Williams %R [-100, 0], signal_strength [0, 1]
                        continue
                    else:
                        scaler = self._create_scaler()
                    
                    # –û–±—É—á–µ–Ω–∏–µ —Å–∫–µ–π–ª–µ—Ä–∞ –¢–û–õ–¨–ö–û –Ω–∞ train –¥–∞–Ω–Ω—ã—Ö
                    clean_data = self._handle_missing_values(group_data)
                    scaler.fit(clean_data)
                    self.scalers[group_name] = scaler
                    
                    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
                    self.preprocessing_stats[group_name] = {
                        'mean': clean_data.mean().to_dict(),
                        'std': clean_data.std().to_dict(),
                        'min': clean_data.min().to_dict(),
                        'max': clean_data.max().to_dict()
                    }
        
        self.logger.info(f"–ü—Ä–µ–ø—Ä–æ—Ü–µ—Å—Å–æ—Ä –æ–±—É—á–µ–Ω –Ω–∞ {len(self.feature_groups)} –≥—Ä—É–ø–ø–∞—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤")
        return self
    
    def transform(self, data: pd.DataFrame) -> pd.DataFrame:
        """
        –¢—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö
        
        Args:
            data: DataFrame –¥–ª—è —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏–∏
            
        Returns:
            –¢—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–Ω—ã–π DataFrame
        """
        self.logger.info(f"–¢—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏—è {len(data)} –∑–∞–ø–∏—Å–µ–π...")
        
        data_transformed = data.copy()
        
        # –û–±—Ä–∞–±–æ—Ç–∫–∞ –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π
        data_transformed = self._handle_missing_values(data_transformed)
        
        # –£–¥–∞–ª–µ–Ω–∏–µ –≤—ã–±—Ä–æ—Å–æ–≤ –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
        if self.remove_outliers:
            data_transformed = self._remove_outliers(data_transformed)
        
        # –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ —Å–∫–µ–π–ª–µ—Ä–æ–≤ –ø–æ –≥—Ä—É–ø–ø–∞–º
        for group_name, features in self.feature_groups.items():
            if group_name in self.scalers and features:
                valid_features = [f for f in features if f in data_transformed.columns]
                if valid_features:
                    numeric_features = data_transformed[valid_features].select_dtypes(include=[np.number]).columns.tolist()
                    if numeric_features:
                        data_transformed[numeric_features] = self.scalers[group_name].transform(
                            data_transformed[numeric_features]
                        )
        
        # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞
        data_transformed = self._additional_preprocessing(data_transformed)
        
        return data_transformed
    
    def fit_transform(self, data: pd.DataFrame) -> pd.DataFrame:
        """
        –û–±—É—á–µ–Ω–∏–µ –∏ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏—è –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ
        
        Args:
            data: DataFrame –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –∏ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏–∏
            
        Returns:
            –¢—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–Ω—ã–π DataFrame
        """
        return self.fit(data).transform(data)
    
    def inverse_transform(self, data: pd.DataFrame, feature_group: str = 'price') -> pd.DataFrame:
        """
        –û–±—Ä–∞—Ç–Ω–∞—è —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö
        
        Args:
            data: —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–Ω—ã–π DataFrame
            feature_group: –≥—Ä—É–ø–ø–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è –æ–±—Ä–∞—Ç–Ω–æ–π —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏–∏
            
        Returns:
            DataFrame —Å –∏—Å—Ö–æ–¥–Ω—ã–º –º–∞—Å—à—Ç–∞–±–æ–º
        """
        if feature_group not in self.scalers:
            self.logger.warning(f"–°–∫–µ–π–ª–µ—Ä –¥–ª—è –≥—Ä—É–ø–ø—ã {feature_group} –Ω–µ –Ω–∞–π–¥–µ–Ω")
            return data
        
        data_inverse = data.copy()
        features = self.feature_groups.get(feature_group, [])
        valid_features = [f for f in features if f in data_inverse.columns]
        
        if valid_features:
            data_inverse[valid_features] = self.scalers[feature_group].inverse_transform(
                data_inverse[valid_features]
            )
        
        return data_inverse
    
    def _identify_feature_groups(self, data: pd.DataFrame, exclude_targets: bool = True):
        """–û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –≥—Ä—É–ø–ø –ø—Ä–∏–∑–Ω–∞–∫–æ–≤"""
        columns = data.columns.tolist()
        
        # –ò—Å–∫–ª—é—á–∞–µ–º —Ü–µ–ª–µ–≤—ã–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –∏–∑ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏ (v4.0 - 20 —Ü–µ–ª–µ–≤—ã—Ö)
        if exclude_targets:
            target_keywords = ['future_', 'direction_', 'will_reach_', 'max_drawdown_', 'max_rally_',
                             'volatility_1h', 'volatility_4h', 'volatility_12h']
            # –£–¥–∞–ª–µ–Ω–æ: 'best_action', 'signal_strength', 'risk_reward', 'optimal_hold' - –±–æ–ª—å—à–µ –Ω–µ —Ü–µ–ª–µ–≤—ã–µ
            columns = [col for col in columns 
                      if not any(keyword in col for keyword in target_keywords)]
        
        # –ì—Ä—É–ø–ø—ã –ø–æ —Ç–∏–ø–∞–º –¥–∞–Ω–Ω—ã—Ö
        self.feature_groups = {
            'price': [col for col in columns if any(x in col.lower() for x in ['open', 'high', 'low', 'close', 'price'])],
            'volume': [col for col in columns if 'volume' in col.lower() or 'turnover' in col.lower()],
            'returns': [col for col in columns if 'return' in col.lower() or 'change' in col.lower()],
            'indicators': [col for col in columns if any(x in col.lower() for x in ['rsi', 'macd', 'ema', 'sma', 'bb_'])],
            'volatility': [col for col in columns if any(x in col.lower() for x in ['atr', 'volatility', 'std'])],
            'microstructure': [col for col in columns if any(x in col.lower() for x in ['spread', 'imbalance', 'pressure'])],
            'temporal': [col for col in columns if any(x in col.lower() for x in ['hour', 'day', 'week', 'month'])],
            'fixed_range': [col for col in columns if any(x in col.lower() for x in ['rsi', 'stoch_k', 'stoch_d', 'adx', 'cci', 'williams_r', 'signal_strength'])]
        }
        
        # –£–¥–∞–ª–µ–Ω–∏–µ –ø—É—Å—Ç—ã—Ö –≥—Ä—É–ø–ø
        self.feature_groups = {k: v for k, v in self.feature_groups.items() if v}
        
        # –í—Å–µ –æ—Å—Ç–∞–ª—å–Ω—ã–µ —á–∏—Å–ª–æ–≤—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
        identified_features = set(sum(self.feature_groups.values(), []))
        numeric_columns = data.select_dtypes(include=[np.number]).columns.tolist()
        other_features = [col for col in numeric_columns if col not in identified_features 
                         and col not in ['id', 'timestamp']]
        
        if other_features:
            self.feature_groups['other'] = other_features
    
    def _create_scaler(self):
        """–°–æ–∑–¥–∞–Ω–∏–µ —Å–∫–µ–π–ª–µ—Ä–∞ –ø–æ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏"""
        if self.scaling_method == 'standard':
            return StandardScaler()
        elif self.scaling_method == 'robust':
            return RobustScaler()
        elif self.scaling_method == 'minmax':
            return MinMaxScaler()
        else:
            return StandardScaler()
    
    def _handle_missing_values(self, data: pd.DataFrame) -> pd.DataFrame:
        """–û–±—Ä–∞–±–æ—Ç–∫–∞ –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π"""
        if self.handle_missing == 'drop':
            return data.dropna()
        
        elif self.handle_missing == 'interpolate':
            # –ò–Ω—Ç–µ—Ä–ø–æ–ª—è—Ü–∏—è –¥–ª—è –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ä—è–¥–æ–≤
            numeric_columns = data.select_dtypes(include=[np.number]).columns
            data[numeric_columns] = data[numeric_columns].interpolate(method='linear', limit_direction='both')
            
        elif self.handle_missing == 'forward_fill':
            data = data.fillna(method='ffill').fillna(method='bfill')
            
        elif self.handle_missing == 'impute':
            # KNN –∏–º–ø—É—Ç–∞—Ü–∏—è –¥–ª—è —Å–ª–æ–∂–Ω—ã—Ö —Å–ª—É—á–∞–µ–≤
            numeric_columns = data.select_dtypes(include=[np.number]).columns
            if len(numeric_columns) > 0:
                imputer = KNNImputer(n_neighbors=5)
                data[numeric_columns] = imputer.fit_transform(data[numeric_columns])
        
        # –ó–∞–ø–æ–ª–Ω–µ–Ω–∏–µ –æ—Å—Ç–∞–≤—à–∏—Ö—Å—è –ø—Ä–æ–ø—É—Å–∫–æ–≤
        data = data.fillna(0)
        
        return data
    
    def _remove_outliers(self, data: pd.DataFrame) -> pd.DataFrame:
        """–£–¥–∞–ª–µ–Ω–∏–µ –≤—ã–±—Ä–æ—Å–æ–≤"""
        numeric_columns = data.select_dtypes(include=[np.number]).columns
        
        for col in numeric_columns:
            if col in ['id', 'timestamp'] or 'target' in col:
                continue
                
            # Z-score –º–µ—Ç–æ–¥
            if self.outlier_threshold > 0:
                z_scores = np.abs((data[col] - data[col].mean()) / data[col].std())
                data = data[z_scores < self.outlier_threshold]
        
        return data
    
    def _additional_preprocessing(self, data: pd.DataFrame) -> pd.DataFrame:
        """–î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö"""
        
        # –ö–ª–∏–ø–ø–∏–Ω–≥ —ç–∫—Å—Ç—Ä–µ–º–∞–ª—å–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π
        for col in data.select_dtypes(include=[np.number]).columns:
            if 'return' in col.lower():
                # –û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ –¥–æ—Ö–æ–¥–Ω–æ—Å—Ç–µ–π —Ä–∞–∑—É–º–Ω—ã–º–∏ –∑–Ω–∞—á–µ–Ω–∏—è–º–∏
                data[col] = data[col].clip(-0.5, 0.5)
            elif any(x in col.lower() for x in ['rsi', 'percent', 'prob']):
                # –û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ –ø—Ä–æ—Ü–µ–Ω—Ç–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π
                data[col] = data[col].clip(0, 100)
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –±–µ—Å–∫–æ–Ω–µ—á–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è
        data = data.replace([np.inf, -np.inf], np.nan).fillna(0)
        
        return data
    
    def get_feature_importance(self, data: pd.DataFrame) -> pd.DataFrame:
        """–û—Ü–µ–Ω–∫–∞ –≤–∞–∂–Ω–æ—Å—Ç–∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤"""
        importance_scores = []
        
        numeric_columns = data.select_dtypes(include=[np.number]).columns
        target_columns = [col for col in numeric_columns if 'target' in col or 'future' in col]
        feature_columns = [col for col in numeric_columns if col not in target_columns + ['id', 'timestamp']]
        
        if target_columns and feature_columns:
            # –ö–æ—Ä—Ä–µ–ª—è—Ü–∏—è —Å —Ü–µ–ª–µ–≤—ã–º–∏ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–º–∏
            for target in target_columns[:1]:  # –ë–µ—Ä–µ–º –ø–µ—Ä–≤—É—é —Ü–µ–ª–µ–≤—É—é –ø–µ—Ä–µ–º–µ–Ω–Ω—É—é
                for feature in feature_columns:
                    correlation = data[feature].corr(data[target])
                    importance_scores.append({
                        'feature': feature,
                        'correlation': abs(correlation),
                        'signed_correlation': correlation
                    })
        
        importance_df = pd.DataFrame(importance_scores)
        if not importance_df.empty:
            importance_df = importance_df.sort_values('correlation', ascending=False)
        
        return importance_df


class SequencePreprocessor:
    """–ü—Ä–µ–ø—Ä–æ—Ü–µ—Å—Å–æ—Ä –¥–ª—è –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π"""
    
    def __init__(self, sequence_length: int, stride: int = 1):
        """
        Args:
            sequence_length: –¥–ª–∏–Ω–∞ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
            stride: —à–∞–≥ –º–µ–∂–¥—É –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—è–º–∏
        """
        self.sequence_length = sequence_length
        self.stride = stride
        self.logger = get_logger("SequencePreprocessor")
    
    def create_sequences(self, 
                        data: pd.DataFrame,
                        target_column: Optional[str] = None,
                        group_column: Optional[str] = 'symbol') -> Tuple[np.ndarray, np.ndarray]:
        """
        –°–æ–∑–¥–∞–Ω–∏–µ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π –∏–∑ –¥–∞–Ω–Ω—ã—Ö
        
        Args:
            data: DataFrame —Å –≤—Ä–µ–º–µ–Ω–Ω—ã–º–∏ —Ä—è–¥–∞–º–∏
            target_column: —Ü–µ–ª–µ–≤–∞—è –ø–µ—Ä–µ–º–µ–Ω–Ω–∞—è
            group_column: —Å—Ç–æ–ª–±–µ—Ü –¥–ª—è –≥—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∏ (–Ω–∞–ø—Ä–∏–º–µ—Ä, symbol)
            
        Returns:
            –ö–æ—Ä—Ç–µ–∂ (sequences, targets)
        """
        sequences = []
        targets = []
        
        if group_column and group_column in data.columns:
            # –°–æ–∑–¥–∞–Ω–∏–µ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π –ø–æ –≥—Ä—É–ø–ø–∞–º
            for group_name, group_data in data.groupby(group_column):
                group_sequences, group_targets = self._create_sequences_from_group(
                    group_data, target_column
                )
                sequences.extend(group_sequences)
                targets.extend(group_targets)
        else:
            # –°–æ–∑–¥–∞–Ω–∏–µ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π –∏–∑ –≤—Å–µ—Ö –¥–∞–Ω–Ω—ã—Ö
            sequences, targets = self._create_sequences_from_group(data, target_column)
        
        return np.array(sequences), np.array(targets)
    
    def _create_sequences_from_group(self,
                                   data: pd.DataFrame,
                                   target_column: Optional[str] = None) -> Tuple[List, List]:
        """–°–æ–∑–¥–∞–Ω–∏–µ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π –∏–∑ –æ–¥–Ω–æ–π –≥—Ä—É–ø–ø—ã"""
        sequences = []
        targets = []
        
        # –í—ã–±–æ—Ä –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
        feature_columns = [col for col in data.columns 
                          if col not in ['id', 'symbol', 'datetime', 'timestamp']]
        
        if target_column and target_column in data.columns:
            feature_columns = [col for col in feature_columns if col != target_column]
            target_data = data[target_column].values
        else:
            target_data = None
        
        feature_data = data[feature_columns].values
        
        # –°–æ–∑–¥–∞–Ω–∏–µ –æ–∫–æ–Ω
        for i in range(0, len(data) - self.sequence_length + 1, self.stride):
            sequence = feature_data[i:i + self.sequence_length]
            sequences.append(sequence)
            
            if target_data is not None:
                # –¶–µ–ª–µ–≤–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ - —Å–ª–µ–¥—É—é—â–µ–µ –ø–æ—Å–ª–µ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
                if i + self.sequence_length < len(target_data):
                    targets.append(target_data[i + self.sequence_length])
        
        return sequences, targets


def create_train_val_test_split(data: pd.DataFrame,
                              train_ratio: float = 0.7,
                              val_ratio: float = 0.15,
                              test_ratio: float = 0.15,
                              time_column: str = 'datetime',
                              gap_days: int = 2) -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:
    """
    –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –Ω–∞ train/val/test —Å —É—á–µ—Ç–æ–º –≤—Ä–µ–º–µ–Ω–∏ –∏ –≤—Ä–µ–º–µ–Ω–Ω—ã–º gap
    
    Args:
        data: –∏—Å—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
        train_ratio: –¥–æ–ª—è –æ–±—É—á–∞—é—â–µ–π –≤—ã–±–æ—Ä–∫–∏
        val_ratio: –¥–æ–ª—è –≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω–æ–π –≤—ã–±–æ—Ä–∫–∏
        test_ratio: –¥–æ–ª—è —Ç–µ—Å—Ç–æ–≤–æ–π –≤—ã–±–æ—Ä–∫–∏
        time_column: —Å—Ç–æ–ª–±–µ—Ü —Å –≤—Ä–µ–º–µ–Ω–µ–º
        gap_days: –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –¥–Ω–µ–π gap –º–µ–∂–¥—É –≤—ã–±–æ—Ä–∫–∞–º–∏ (–¥–ª—è –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–µ–Ω–∏—è —É—Ç–µ—á–µ–∫)
        
    Returns:
        –ö–æ—Ä—Ç–µ–∂ (train_data, val_data, test_data)
    """
    logger = get_logger("DataSplit")
    
    # –°–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ –ø–æ –≤—Ä–µ–º–µ–Ω–∏
    data_sorted = data.sort_values(time_column)
    
    # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤ datetime –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
    if not pd.api.types.is_datetime64_any_dtype(data_sorted[time_column]):
        data_sorted[time_column] = pd.to_datetime(data_sorted[time_column])
    
    # –ü–æ–ª—É—á–∞–µ–º —É–Ω–∏–∫–∞–ª—å–Ω—ã–µ –¥–∞—Ç—ã
    unique_dates = data_sorted[time_column].dt.date.unique()
    n_dates = len(unique_dates)
    
    # –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ–º –∏–Ω–¥–µ–∫—Å—ã —Å —É—á–µ—Ç–æ–º gap
    train_end_idx = int(n_dates * train_ratio)
    val_start_idx = min(train_end_idx + gap_days, n_dates - 1)
    val_end_idx = min(val_start_idx + int(n_dates * val_ratio), n_dates - 1)
    test_start_idx = min(val_end_idx + gap_days, n_dates - 1)
    
    # –ü–æ–ª—É—á–∞–µ–º –¥–∞—Ç—ã –¥–ª—è —Ä–∞–∑–¥–µ–ª–µ–Ω–∏—è
    train_end_date = unique_dates[train_end_idx]
    val_start_date = unique_dates[val_start_idx]
    val_end_date = unique_dates[val_end_idx]
    test_start_date = unique_dates[test_start_idx]
    
    # –†–∞–∑–¥–µ–ª—è–µ–º –¥–∞–Ω–Ω—ã–µ
    train_data = data_sorted[data_sorted[time_column].dt.date <= train_end_date].copy()
    val_data = data_sorted[(data_sorted[time_column].dt.date >= val_start_date) & 
                          (data_sorted[time_column].dt.date <= val_end_date)].copy()
    test_data = data_sorted[data_sorted[time_column].dt.date >= test_start_date].copy()
    
    # –î–æ–±–∞–≤–ª—è–µ–º –∞—Ç—Ä–∏–±—É—Ç—ã –¥–ª—è –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏—è —Ç–∏–ø–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞
    train_data.attrs['dataset_type'] = 'train'
    val_data.attrs['dataset_type'] = 'val'
    test_data.attrs['dataset_type'] = 'test'
    
    logger.info(f"üìä –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö —Å –≤—Ä–µ–º–µ–Ω–Ω—ã–º gap ({gap_days} –¥–Ω–µ–π):")
    logger.info(f"   - Train: {len(train_data):,} –∑–∞–ø–∏—Å–µ–π (–¥–æ {train_end_date})")
    logger.info(f"   - Val: {len(val_data):,} –∑–∞–ø–∏—Å–µ–π ({val_start_date} - {val_end_date})")
    logger.info(f"   - Test: {len(test_data):,} –∑–∞–ø–∏—Å–µ–π (–æ—Ç {test_start_date})")
    
    return train_data, val_data, test_data