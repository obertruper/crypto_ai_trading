"""
–ò–Ω–∂–µ–Ω–µ—Ä–∏—è –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è –∫—Ä–∏–ø—Ç–æ–≤–∞–ª—é—Ç–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
"""

import pandas as pd
import numpy as np
import ta
from typing import Dict, List, Tuple, Optional
from scipy import stats
from sklearn.preprocessing import StandardScaler, RobustScaler
import warnings
warnings.filterwarnings('ignore')

from utils.logger import get_logger

class FeatureEngineer:
    """–°–æ–∑–¥–∞–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è –º–æ–¥–µ–ª–∏ –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏—è"""
    
    def __init__(self, config: Dict):
        self.config = config
        self.logger = get_logger("FeatureEngineer")
        self.feature_config = config['features']
        self.scalers = {}
    
    @staticmethod
    def safe_divide(numerator: pd.Series, denominator: pd.Series, fill_value=0.0) -> pd.Series:
        """–ë–µ–∑–æ–ø–∞—Å–Ω–æ–µ –¥–µ–ª–µ–Ω–∏–µ —Å –æ–±—Ä–∞–±–æ—Ç–∫–æ–π –Ω—É–ª–µ–π –∏ –º–∞–ª—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π"""
        # –ú–∏–Ω–∏–º–∞–ª—å–Ω–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ –¥–ª—è –∑–Ω–∞–º–µ–Ω–∞—Ç–µ–ª—è
        min_denominator = 1e-10
        
        # –°–æ–∑–¥–∞–µ–º –±–µ–∑–æ–ø–∞—Å–Ω—ã–π –∑–Ω–∞–º–µ–Ω–∞—Ç–µ–ª—å
        safe_denominator = denominator.copy()
        
        # –ó–∞–º–µ–Ω—è–µ–º –Ω—É–ª–∏ –∏ –æ—á–µ–Ω—å –º–∞–ª–µ–Ω—å–∫–∏–µ –∑–Ω–∞—á–µ–Ω–∏—è
        mask_small = (safe_denominator.abs() < min_denominator)
        safe_denominator[mask_small] = np.sign(safe_denominator[mask_small]) * min_denominator
        safe_denominator[safe_denominator == 0] = min_denominator  # –î–ª—è —Ç–æ—á–Ω—ã—Ö –Ω—É–ª–µ–π
        
        # –í—ã–ø–æ–ª–Ω—è–µ–º –¥–µ–ª–µ–Ω–∏–µ
        result = numerator / safe_denominator
        
        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º inf –∏ nan
        # –ï—Å–ª–∏ fill_value - —ç—Ç–æ Series, –∏—Å–ø–æ–ª—å–∑—É–µ–º –¥—Ä—É–≥–æ–π –ø–æ–¥—Ö–æ–¥
        if isinstance(fill_value, pd.Series):
            # –ù–∞—Ö–æ–¥–∏–º –ø–æ–∑–∏—Ü–∏–∏ —Å inf –∏ –∑–∞–º–µ–Ω—è–µ–º –∏—Ö —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏–º–∏ –∑–Ω–∞—á–µ–Ω–∏—è–º–∏ –∏–∑ fill_value
            inf_mask = np.isinf(result)
            result.loc[inf_mask] = fill_value.loc[inf_mask]
        else:
            # –ï—Å–ª–∏ fill_value - —Å–∫–∞–ª—è—Ä, –∏—Å–ø–æ–ª—å–∑—É–µ–º —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π replace
            result = result.replace([np.inf, -np.inf], fill_value)
        
        result = result.fillna(fill_value)
        
        return result
        
    def create_features(self, df: pd.DataFrame, train_end_date: Optional[str] = None) -> pd.DataFrame:
        """–°–æ–∑–¥–∞–Ω–∏–µ –≤—Å–µ—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è –¥–∞—Ç–∞—Å–µ—Ç–∞ —Å walk-forward –≤–∞–ª–∏–¥–∞—Ü–∏–µ–π"""
        self.logger.start_stage("feature_engineering", 
                               symbols=df['symbol'].nunique())
        
        # –í–∞–ª–∏–¥–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö
        self._validate_data(df)
        
        featured_dfs = []
        
        for symbol in df['symbol'].unique():
            symbol_data = df[df['symbol'] == symbol].copy()
            symbol_data = symbol_data.sort_values('datetime')
            
            symbol_data = self._create_basic_features(symbol_data)
            symbol_data = self._create_technical_indicators(symbol_data)
            symbol_data = self._create_microstructure_features(symbol_data)
            symbol_data = self._create_temporal_features(symbol_data)
            symbol_data = self._create_target_variables(symbol_data)
            
            featured_dfs.append(symbol_data)
        
        result_df = pd.concat(featured_dfs, ignore_index=True)
        result_df = self._create_cross_asset_features(result_df)
        
        # –û–±—Ä–∞–±–æ—Ç–∫–∞ NaN –∑–Ω–∞—á–µ–Ω–∏–π
        result_df = self._handle_missing_values(result_df)
        
        # Walk-forward –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –µ—Å–ª–∏ —É–∫–∞–∑–∞–Ω–∞ –¥–∞—Ç–∞
        if train_end_date:
            result_df = self._normalize_walk_forward(result_df, train_end_date)
        else:
            result_df = self._normalize_features(result_df)
        
        self._log_feature_statistics(result_df)
        
        self.logger.end_stage("feature_engineering", 
                            total_features=len(result_df.columns))
        
        return result_df
    
    def _validate_data(self, df: pd.DataFrame):
        """–í–∞–ª–∏–¥–∞—Ü–∏—è —Ü–µ–ª–æ—Å—Ç–Ω–æ—Å—Ç–∏ –¥–∞–Ω–Ω—ã—Ö"""
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—â–∏–µ –∑–Ω–∞—á–µ–Ω–∏—è
        if df.isnull().any().any():
            self.logger.warning("–û–±–Ω–∞—Ä—É–∂–µ–Ω—ã –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è –≤ –¥–∞–Ω–Ω—ã—Ö")
            
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –∞–Ω–æ–º–∞–ª—å–Ω—ã–µ —Ü–µ–Ω—ã
        price_changes = df.groupby('symbol')['close'].pct_change()
        extreme_moves = abs(price_changes) > 0.15  # >15% –∑–∞ 15 –º–∏–Ω—É—Ç
        
        if extreme_moves.sum() > 0:
            self.logger.warning(f"–û–±–Ω–∞—Ä—É–∂–µ–Ω–æ {extreme_moves.sum()} —ç–∫—Å—Ç—Ä–µ–º–∞–ª—å–Ω—ã—Ö –¥–≤–∏–∂–µ–Ω–∏–π —Ü–µ–Ω—ã")
            
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –≥—ç–ø–æ–≤ (—Ç–æ–ª—å–∫–æ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω—ã–µ —Ä–∞–∑—Ä—ã–≤—ã > 2 —á–∞—Å–æ–≤)
        for symbol in df['symbol'].unique():
            symbol_data = df[df['symbol'] == symbol]
            time_diff = symbol_data['datetime'].diff()
            expected_diff = pd.Timedelta('15 minutes')
            # –°—á–∏—Ç–∞–µ–º –±–æ–ª—å—à–∏–º–∏ —Ç–æ–ª—å–∫–æ —Ä–∞–∑—Ä—ã–≤—ã –±–æ–ª—å—à–µ 2 —á–∞—Å–æ–≤ (8 –∏–Ω—Ç–µ—Ä–≤–∞–ª–æ–≤)
            large_gaps = time_diff > expected_diff * 8
            
            if large_gaps.sum() > 0:
                self.logger.warning(f"–°–∏–º–≤–æ–ª {symbol}: –æ–±–Ω–∞—Ä—É–∂–µ–Ω–æ {large_gaps.sum()} –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω—ã—Ö –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ä–∞–∑—Ä—ã–≤–æ–≤ (> 2 —á–∞—Å–æ–≤)")
    
    def _create_basic_features(self, df: pd.DataFrame) -> pd.DataFrame:
        """–ë–∞–∑–æ–≤—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ –∏–∑ OHLCV –¥–∞–Ω–Ω—ã—Ö –±–µ–∑ look-ahead bias"""
        df['returns'] = np.log(df['close'] / df['close'].shift(1))
        
        # –î–æ—Ö–æ–¥–Ω–æ—Å—Ç–∏ –∑–∞ —Ä–∞–∑–Ω—ã–µ –ø–µ—Ä–∏–æ–¥—ã
        for period in [5, 10, 20]:
            df[f'returns_{period}'] = np.log(
                df['close'] / df['close'].shift(period)
            )
        
        # –¶–µ–Ω–æ–≤—ã–µ —Å–æ–æ—Ç–Ω–æ—à–µ–Ω–∏—è
        df['high_low_ratio'] = df['high'] / df['low']
        df['close_open_ratio'] = df['close'] / df['open']
        
        # –ü–æ–∑–∏—Ü–∏—è –∑–∞–∫—Ä—ã—Ç–∏—è –≤ –¥–∏–∞–ø–∞–∑–æ–Ω–µ
        df['close_position'] = (
            (df['close'] - df['low']) / 
            (df['high'] - df['low'] + 1e-10)
        )
        
        # –û–±—ä–µ–º–Ω—ã–µ —Å–æ–æ—Ç–Ω–æ—à–µ–Ω–∏—è —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º —Ç–æ–ª—å–∫–æ –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö
        df['volume_ratio'] = self.safe_divide(
            df['volume'], 
            df['volume'].rolling(20, min_periods=20).mean(),
            fill_value=1.0
        )
        df['turnover_ratio'] = self.safe_divide(
            df['turnover'], 
            df['turnover'].rolling(20, min_periods=20).mean(),
            fill_value=1.0
        )
        
        # VWAP
        df['vwap'] = self.safe_divide(df['turnover'], df['volume'], fill_value=df['close'])
        df['close_vwap_ratio'] = self.safe_divide(df['close'], df['vwap'], fill_value=1.0)
        
        return df
    
    def _create_technical_indicators(self, df: pd.DataFrame) -> pd.DataFrame:
        """–¢–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã"""
        tech_config = self.feature_config['technical']
        
        # SMA
        sma_config = next((c for c in tech_config if c['name'] == 'sma'), None)
        if sma_config:
            for period in sma_config['periods']:
                df[f'sma_{period}'] = ta.trend.sma_indicator(df['close'], period)
                df[f'close_sma_{period}_ratio'] = df['close'] / df[f'sma_{period}']
        
        # EMA
        ema_config = next((c for c in tech_config if c['name'] == 'ema'), None)
        if ema_config:
            for period in ema_config['periods']:
                df[f'ema_{period}'] = ta.trend.ema_indicator(df['close'], period)
                df[f'close_ema_{period}_ratio'] = df['close'] / df[f'ema_{period}']
        
        # RSI
        rsi_config = next((c for c in tech_config if c['name'] == 'rsi'), None)
        if rsi_config:
            df['rsi'] = ta.momentum.RSIIndicator(
                df['close'], 
                window=rsi_config['period']
            ).rsi()
            
            df['rsi_oversold'] = (df['rsi'] < 30).astype(int)
            df['rsi_overbought'] = (df['rsi'] > 70).astype(int)
        
        # MACD
        macd_config = next((c for c in tech_config if c['name'] == 'macd'), None)
        if macd_config:
            macd = ta.trend.MACD(
                df['close'],
                window_slow=macd_config['slow'],
                window_fast=macd_config['fast'],
                window_sign=macd_config['signal']
            )
            df['macd'] = macd.macd()
            df['macd_signal'] = macd.macd_signal()
            df['macd_diff'] = macd.macd_diff()
        
        # Bollinger Bands
        bb_config = next((c for c in tech_config if c['name'] == 'bollinger_bands'), None)
        if bb_config:
            bb = ta.volatility.BollingerBands(
                df['close'],
                window=bb_config['period'],
                window_dev=bb_config['std_dev']
            )
            df['bb_high'] = bb.bollinger_hband()
            df['bb_low'] = bb.bollinger_lband()
            df['bb_middle'] = bb.bollinger_mavg()
            df['bb_width'] = df['bb_high'] - df['bb_low']
            df['bb_position'] = (df['close'] - df['bb_low']) / (df['bb_width'] + 1e-10)
        
        # ATR
        atr_config = next((c for c in tech_config if c['name'] == 'atr'), None)
        if atr_config:
            df['atr'] = ta.volatility.AverageTrueRange(
                df['high'], 
                df['low'], 
                df['close'],
                window=atr_config['period']
            ).average_true_range()
            
            df['atr_pct'] = df['atr'] / df['close']
        
        # Stochastic
        stoch = ta.momentum.StochasticOscillator(
            df['high'], 
            df['low'], 
            df['close'],
            window=14,
            smooth_window=3
        )
        df['stoch_k'] = stoch.stoch()
        df['stoch_d'] = stoch.stoch_signal()
        
        # ADX
        adx = ta.trend.ADXIndicator(df['high'], df['low'], df['close'])
        df['adx'] = adx.adx()
        df['adx_pos'] = adx.adx_pos()
        df['adx_neg'] = adx.adx_neg()
        
        # Parabolic SAR
        psar = ta.trend.PSARIndicator(df['high'], df['low'], df['close'])
        df['psar'] = psar.psar()
        # –í–º–µ—Å—Ç–æ –æ—Ç–¥–µ–ª—å–Ω—ã—Ö psar_up –∏ psar_down, —Å–æ–∑–¥–∞–µ–º –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏—è
        df['psar_trend'] = (df['close'] > df['psar']).astype(float)
        df['psar_distance'] = (df['close'] - df['psar']) / df['close']
        
        return df
    
    def _create_microstructure_features(self, df: pd.DataFrame) -> pd.DataFrame:
        """–ü—Ä–∏–∑–Ω–∞–∫–∏ –º–∏–∫—Ä–æ—Å—Ç—Ä—É–∫—Ç—É—Ä—ã —Ä—ã–Ω–∫–∞"""
        # –°–ø—Ä–µ–¥ high-low
        df['hl_spread'] = self.safe_divide(df['high'] - df['low'], df['close'], fill_value=0.0)
        df['hl_spread_ma'] = df['hl_spread'].rolling(20).mean()
        
        # –ù–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–µ —Ü–µ–Ω—ã –∏ –æ–±—ä–µ–º
        df['price_direction'] = np.sign(df['close'] - df['open'])
        df['directed_volume'] = df['volume'] * df['price_direction']
        df['volume_imbalance'] = df['directed_volume'].rolling(10).sum() / \
                                 df['volume'].rolling(10).sum()
        
        # –¶–µ–Ω–æ–≤–æ–µ –≤–æ–∑–¥–µ–π—Å—Ç–≤–∏–µ
        df['price_impact'] = df['returns'].abs() / (np.log(df['volume'] + 1) + 1e-10)
        df['toxicity'] = 1 / (1 + df['price_impact'])
        
        # –ê–º–∏—Ö—É–¥ –Ω–µ–ª–∏–∫–≤–∏–¥–Ω–æ—Å—Ç—å
        df['amihud_illiquidity'] = df['returns'].abs() / (df['turnover'] + 1e-10)
        df['amihud_ma'] = df['amihud_illiquidity'].rolling(20).mean()
        
        # –ö–∞–π–ª –ª—è–º–±–¥–∞
        df['kyle_lambda'] = df['returns'].rolling(10).std() / \
                           (df['volume'].rolling(10).std() + 1e-10)
        
        # –†–µ–∞–ª–∏–∑–æ–≤–∞–Ω–Ω–∞—è –≤–æ–ª–∞—Ç–∏–ª—å–Ω–æ—Å—Ç—å
        df['realized_vol'] = df['returns'].rolling(20).std() * np.sqrt(96)  # 96 = 24*4 (15–º–∏–Ω –∏–Ω—Ç–µ—Ä–≤–∞–ª—ã)
        
        # –°–æ–æ—Ç–Ω–æ—à–µ–Ω–∏–µ –æ–±—ä–µ–º–∞ –∫ –≤–æ–ª–∞—Ç–∏–ª—å–Ω–æ—Å—Ç–∏
        df['volume_volatility_ratio'] = df['volume'] / (df['realized_vol'] + 1e-10)
        
        return df
    
    def _create_temporal_features(self, df: pd.DataFrame) -> pd.DataFrame:
        """–í—Ä–µ–º–µ–Ω–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏"""
        df['hour'] = df['datetime'].dt.hour
        df['minute'] = df['datetime'].dt.minute
        
        # –¶–∏–∫–ª–∏—á–µ—Å–∫–æ–µ –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ –≤—Ä–µ–º–µ–Ω–∏
        df['hour_sin'] = np.sin(2 * np.pi * df['hour'] / 24)
        df['hour_cos'] = np.cos(2 * np.pi * df['hour'] / 24)
        
        df['dayofweek'] = df['datetime'].dt.dayofweek
        df['is_weekend'] = (df['dayofweek'] >= 5).astype(int)
        
        df['dow_sin'] = np.sin(2 * np.pi * df['dayofweek'] / 7)
        df['dow_cos'] = np.cos(2 * np.pi * df['dayofweek'] / 7)
        
        df['day'] = df['datetime'].dt.day
        df['month'] = df['datetime'].dt.month
        
        df['month_sin'] = np.sin(2 * np.pi * df['month'] / 12)
        df['month_cos'] = np.cos(2 * np.pi * df['month'] / 12)
        
        # –¢–æ—Ä–≥–æ–≤—ã–µ —Å–µ—Å—Å–∏–∏
        df['asian_session'] = ((df['hour'] >= 0) & (df['hour'] < 8)).astype(int)
        df['european_session'] = ((df['hour'] >= 7) & (df['hour'] < 16)).astype(int)
        df['american_session'] = ((df['hour'] >= 13) & (df['hour'] < 22)).astype(int)
        
        # –ü–µ—Ä–µ—Å–µ—á–µ–Ω–∏–µ —Å–µ—Å—Å–∏–π
        df['session_overlap'] = (
            (df['asian_session'] + df['european_session'] + df['american_session']) > 1
        ).astype(int)
        
        return df
    
    def _handle_missing_values(self, df: pd.DataFrame) -> pd.DataFrame:
        """–û–±—Ä–∞–±–æ—Ç–∫–∞ –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π"""
        self.logger.info("–û–±—Ä–∞–±–æ—Ç–∫–∞ –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π...")
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–æ–Ω–Ω—ã–µ –∫–æ–ª–æ–Ω–∫–∏
        info_cols = ['id', 'symbol', 'timestamp', 'datetime', 'open', 'high', 'low', 'close', 'volume', 'turnover']
        
        # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º –ø–æ —Å–∏–º–≤–æ–ª–∞–º –¥–ª—è –ø—Ä–∞–≤–∏–ª—å–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏
        processed_dfs = []
        
        for symbol in df['symbol'].unique():
            symbol_data = df[df['symbol'] == symbol].copy()
            
            # –î–ª—è –∫–∞–∂–¥–æ–π –∫–æ–ª–æ–Ω–∫–∏ –ø—Ä–∏–º–µ–Ω—è–µ–º —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏–π –º–µ—Ç–æ–¥ –∑–∞–ø–æ–ª–Ω–µ–Ω–∏—è
            for col in symbol_data.columns:
                if col in info_cols:
                    continue
                    
                if symbol_data[col].isna().any():
                    # –î–ª—è —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏—Ö –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–æ–≤ –∏—Å–ø–æ–ª—å–∑—É–µ–º forward fill
                    if any(indicator in col for indicator in ['sma', 'ema', 'rsi', 'macd', 'bb_', 'adx']):
                        symbol_data[col] = symbol_data[col].ffill()
                    # –î–ª—è –æ—Å—Ç–∞–ª—å–Ω—ã—Ö –∏—Å–ø–æ–ª—å–∑—É–µ–º 0
                    else:
                        symbol_data[col] = symbol_data[col].fillna(0)
            
            # –£–¥–∞–ª—è–µ–º –ø–µ—Ä–≤—ã–µ —Å—Ç—Ä–æ–∫–∏ –≥–¥–µ –º–æ–≥—É—Ç –±—ã—Ç—å NaN –∏–∑-–∑–∞ —Ä–∞—Å—á–µ—Ç–∞ –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–æ–≤
            # –ù–∞—Ö–æ–¥–∏–º –º–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π –ø–µ—Ä–∏–æ–¥ —Å—Ä–µ–¥–∏ –≤—Å–µ—Ö –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–æ–≤
            max_period = 50  # SMA50 —Ç—Ä–µ–±—É–µ—Ç –º–∏–Ω–∏–º—É–º 50 –ø–µ—Ä–∏–æ–¥–æ–≤
            symbol_data = symbol_data.iloc[max_period:].copy()
            
            processed_dfs.append(symbol_data)
        
        result_df = pd.concat(processed_dfs, ignore_index=True)
        
        # –§–∏–Ω–∞–ª—å–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞
        nan_count = result_df.isna().sum().sum()
        if nan_count > 0:
            self.logger.warning(f"–û—Å—Ç–∞–ª–∏—Å—å {nan_count} NaN –∑–Ω–∞—á–µ–Ω–∏–π –ø–æ—Å–ª–µ –æ–±—Ä–∞–±–æ—Ç–∫–∏")
            # –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ –∑–∞–ø–æ–ª–Ω—è–µ–º –æ—Å—Ç–∞–≤—à–∏–µ—Å—è NaN
            numeric_cols = result_df.select_dtypes(include=[np.number]).columns
            result_df[numeric_cols] = result_df[numeric_cols].fillna(0)
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –±–µ—Å–∫–æ–Ω–µ—á–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è
        numeric_cols = result_df.select_dtypes(include=[np.number]).columns
        inf_count = np.isinf(result_df[numeric_cols]).sum().sum()
        if inf_count > 0:
            self.logger.warning(f"–û–±–Ω–∞—Ä—É–∂–µ–Ω—ã {inf_count} –±–µ—Å–∫–æ–Ω–µ—á–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π, –∑–∞–º–µ–Ω—è–µ–º –Ω–∞ –∫–æ–Ω–µ—á–Ω—ã–µ")
            result_df[numeric_cols] = result_df[numeric_cols].replace([np.inf, -np.inf], [1e10, -1e10])
        
        self.logger.info(f"–û–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞. –ò—Ç–æ–≥–æ–≤—ã–π —Ä–∞–∑–º–µ—Ä: {len(result_df)} –∑–∞–ø–∏—Å–µ–π")
        return result_df
    
    def _create_cross_asset_features(self, df: pd.DataFrame) -> pd.DataFrame:
        """–ö—Ä–æ—Å—Å-–∞–∫—Ç–∏–≤–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏"""
        self.logger.info("–°–æ–∑–¥–∞–Ω–∏–µ –∫—Ä–æ—Å—Å-–∞–∫—Ç–∏–≤–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤...")
        
        # BTC –∫–∞–∫ –±–∞–∑–æ–≤—ã–π –∞–∫—Ç–∏–≤
        btc_data = df[df['symbol'] == 'BTCUSDT'][['datetime', 'close', 'returns']].copy()
        if len(btc_data) > 0:
            btc_data.rename(columns={
                'close': 'btc_close',
                'returns': 'btc_returns'
            }, inplace=True)
            
            df = df.merge(btc_data, on='datetime', how='left')
            
            # –ö–æ—Ä—Ä–µ–ª—è—Ü–∏—è —Å BTC
            for symbol in df['symbol'].unique():
                if symbol != 'BTCUSDT':
                    mask = df['symbol'] == symbol
                    df.loc[mask, 'btc_correlation'] = (
                        df.loc[mask, 'returns']
                        .rolling(window=96)
                        .corr(df.loc[mask, 'btc_returns'])
                    )
            
            df.loc[df['symbol'] == 'BTCUSDT', 'btc_correlation'] = 1.0
            
            # –û—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–∞—è —Å–∏–ª–∞ –∫ BTC
            df['relative_strength_btc'] = df['close'] / df['btc_close']
            df['rs_btc_ma'] = df.groupby('symbol')['relative_strength_btc'].transform(
                lambda x: x.rolling(20).mean()
            )
        else:
            # –ó–∞–ø–æ–ª–Ω—è–µ–º –Ω—É–ª—è–º–∏ –µ—Å–ª–∏ –Ω–µ—Ç –¥–∞–Ω–Ω—ã—Ö BTC
            df['btc_close'] = 0
            df['btc_returns'] = 0
            df['btc_correlation'] = 0
            df['relative_strength_btc'] = 0
            df['rs_btc_ma'] = 0
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Å–µ–∫—Ç–æ—Ä–∞
        defi_tokens = ['AAVEUSDT', 'UNIUSDT', 'CAKEUSDT', 'DYDXUSDT']
        layer1_tokens = ['ETHUSDT', 'SOLUSDT', 'AVAXUSDT', 'DOTUSDT', 'NEARUSDT']
        meme_tokens = ['DOGEUSDT', 'FARTCOINUSDT', 'MELANIAUSDT', 'TRUMPUSDT', 
                      'POPCATUSDT', 'PNUTUSDT', 'ZEREBROUSDT', 'WIFUSDT']
        
        df['sector'] = 'other'
        df.loc[df['symbol'].isin(defi_tokens), 'sector'] = 'defi'
        df.loc[df['symbol'].isin(layer1_tokens), 'sector'] = 'layer1'
        df.loc[df['symbol'].isin(meme_tokens), 'sector'] = 'meme'
        df.loc[df['symbol'] == 'BTCUSDT', 'sector'] = 'btc'
        
        # –°–µ–∫—Ç–æ—Ä–Ω—ã–µ –¥–æ—Ö–æ–¥–Ω–æ—Å—Ç–∏
        df['sector_returns'] = df.groupby(['datetime', 'sector'])['returns'].transform('mean')
        
        # –û—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–∞—è –¥–æ—Ö–æ–¥–Ω–æ—Å—Ç—å –∫ —Å–µ–∫—Ç–æ—Ä—É
        df['relative_to_sector'] = df['returns'] - df['sector_returns']
        
        # –†–∞–Ω–∫ –¥–æ—Ö–æ–¥–Ω–æ—Å—Ç–∏
        df['returns_rank'] = df.groupby('datetime')['returns'].rank(pct=True)
        
        # 24-—á–∞—Å–æ–≤–æ–π –º–æ–º–µ–Ω—Ç—É–º
        df['momentum_24h'] = df.groupby('symbol')['returns'].transform(
            lambda x: x.rolling(96).sum()  # 96 = 24*4 (15–º–∏–Ω –∏–Ω—Ç–µ—Ä–≤–∞–ª—ã)
        )
        df['is_momentum_leader'] = (
            df.groupby('datetime')['momentum_24h']
            .rank(ascending=False) <= 5
        ).astype(int)
        
        return df
    
    def _create_target_variables(self, df: pd.DataFrame) -> pd.DataFrame:
        """–°–æ–∑–¥–∞–Ω–∏–µ —Ü–µ–ª–µ–≤—ã—Ö –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è"""
        risk_config = self.config['risk_management']
        
        # –ë—É–¥—É—â–∏–µ —Ü–µ–Ω—ã
        for horizon in range(1, 5):
            df[f'future_close_{horizon}'] = df.groupby('symbol')['close'].shift(-horizon)
            df[f'future_return_{horizon}'] = (
                df[f'future_close_{horizon}'] / df['close'] - 1
            ) * 100
        
        # –¶–µ–ª–∏ –ø–æ take profit
        for tp_level in risk_config['take_profit_targets']:
            df[f'target_tp_{tp_level}'] = 0
            
            for horizon in range(1, 5):
                future_return_col = f'future_return_{horizon}'
                if future_return_col in df.columns:
                    df[f'target_tp_{tp_level}'] = np.maximum(
                        df[f'target_tp_{tp_level}'],
                        (df[future_return_col] >= tp_level).astype(int)
                    )
        
        # Stop loss
        sl_level = risk_config['stop_loss_pct']
        df['target_sl_hit'] = 0
        
        for horizon in range(1, 5):
            future_return_col = f'future_return_{horizon}'
            if future_return_col in df.columns:
                df['target_sl_hit'] = np.maximum(
                    df['target_sl_hit'],
                    (df[future_return_col] <= -sl_level).astype(int)
                )
        
        # –û–ø—Ç–∏–º–∞–ª—å–Ω–æ–µ –¥–µ–π—Å—Ç–≤–∏–µ
        df['optimal_action'] = 0
        
        for i, tp_level in enumerate(risk_config['take_profit_targets'], 1):
            condition = (df[f'target_tp_{tp_level}'] == 1) & (df['target_sl_hit'] == 0)
            df.loc[condition, 'optimal_action'] = i
        
        # –ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è –∏ –º–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –±—É–¥—É—â–∞—è –¥–æ—Ö–æ–¥–Ω–æ—Å—Ç—å
        future_return_cols = [f'future_return_{i}' for i in range(1, 5) 
                             if f'future_return_{i}' in df.columns]
        
        if future_return_cols:
            df['future_min_return'] = df[future_return_cols].min(axis=1)
            df['future_max_return'] = df[future_return_cols].max(axis=1)
        
        return df
    
    def _normalize_features(self, df: pd.DataFrame) -> pd.DataFrame:
        """–ò–°–ü–†–ê–í–õ–ï–ù–ù–ê–Ø –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ —Å –æ–±—Ä–∞–±–æ—Ç–∫–æ–π inf/nan"""
        self.logger.info("–ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –ø—Ä–∏–∑–Ω–∞–∫–æ–≤...")
        
        # –°—Ç–æ–ª–±—Ü—ã –¥–ª—è –∏—Å–∫–ª—é—á–µ–Ω–∏—è –∏–∑ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏
        exclude_cols = [
            'id', 'symbol', 'timestamp', 'datetime', 'sector',
            'open', 'high', 'low', 'close', 'volume', 'turnover'
        ]
        
        # –¶–µ–ª–µ–≤—ã–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ
        target_cols = [col for col in df.columns if col.startswith(('target_', 'future_', 'optimal_'))]
        exclude_cols.extend(target_cols)
        
        # –í—Ä–µ–º–µ–Ω–Ω—ã–µ –∫–æ–ª–æ–Ω–∫–∏
        time_cols = ['hour', 'minute', 'dayofweek', 'day', 'month', 'is_weekend',
                    'asian_session', 'european_session', 'american_session', 'session_overlap']
        exclude_cols.extend(time_cols)
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –ø—Ä–∏–∑–Ω–∞–∫–∏ –¥–ª—è –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏
        feature_cols = [col for col in df.columns if col not in exclude_cols]
        
        # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –ø–æ —Å–∏–º–≤–æ–ª–∞–º
        for symbol in df['symbol'].unique():
            mask = df['symbol'] == symbol
            
            if symbol not in self.scalers:
                self.scalers[symbol] = RobustScaler()
            
            # –¢–æ–ª—å–∫–æ –≤–∞–ª–∏–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ (–±–µ–∑ NaN –∏ inf)
            valid_mask = mask & df[feature_cols].notna().all(axis=1)
            
            if valid_mask.sum() > 0:
                data_to_scale = df.loc[valid_mask, feature_cols].copy()
                
                # –ö–†–ò–¢–ò–ß–ï–°–ö–û–ï –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï: –¢—â–∞—Ç–µ–ª—å–Ω–∞—è –æ—á–∏—Å—Ç–∫–∞ inf –∏ —ç–∫—Å—Ç—Ä–µ–º–∞–ª—å–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π
                for col in feature_cols:
                    # 1. –ó–∞–º–µ–Ω—è–µ–º inf –Ω–∞ NaN
                    data_to_scale[col] = data_to_scale[col].replace([np.inf, -np.inf], np.nan)
                    
                    # 2. –ó–∞–ø–æ–ª–Ω—è–µ–º NaN –º–µ–¥–∏–∞–Ω–æ–π
                    if data_to_scale[col].isna().any():
                        median_val = data_to_scale[col].median()
                        if pd.isna(median_val):  # –ï—Å–ª–∏ –≤—Å–µ –∑–Ω–∞—á–µ–Ω–∏—è NaN
                            median_val = 0.0
                        data_to_scale[col] = data_to_scale[col].fillna(median_val)
                    
                    # 3. –ë–æ–ª–µ–µ –∞–≥—Ä–µ—Å—Å–∏–≤–Ω—ã–π –∫–ª–∏–ø–ø–∏–Ω–≥ —ç–∫—Å—Ç—Ä–µ–º–∞–ª—å–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π
                    q05 = data_to_scale[col].quantile(0.05)  # –ò–∑–º–µ–Ω–µ–Ω–æ —Å 0.01 –Ω–∞ 0.05
                    q95 = data_to_scale[col].quantile(0.95)  # –ò–∑–º–µ–Ω–µ–Ω–æ —Å 0.99 –Ω–∞ 0.95
                    
                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –≤–∞–ª–∏–¥–Ω–æ—Å—Ç—å –∫–≤–∞–Ω—Ç–∏–ª–µ–π
                    if pd.isna(q05) or pd.isna(q95) or q05 == q95:
                        # –ï—Å–ª–∏ –∫–≤–∞–Ω—Ç–∏–ª–∏ –Ω–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã, –∏—Å–ø–æ–ª—å–∑—É–µ–º —Å–∏–º–º–µ—Ç—Ä–∏—á–Ω–æ–µ –∫–ª–∏–ø–ø–∏–Ω–≥
                        std_val = data_to_scale[col].std()
                        mean_val = data_to_scale[col].mean()
                        if pd.notna(std_val) and std_val > 0:
                            q05 = mean_val - 3 * std_val
                            q95 = mean_val + 3 * std_val
                        else:
                            q05, q95 = -1, 1  # –î–µ—Ñ–æ–ª—Ç–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è
                    
                    data_to_scale[col] = data_to_scale[col].clip(lower=q05, upper=q95)
                    
                    # 4. –§–∏–Ω–∞–ª—å–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ inf (–Ω–∞ –≤—Å—è–∫–∏–π —Å–ª—É—á–∞–π)
                    if np.isinf(data_to_scale[col]).any():
                        self.logger.warning(f"–û–±–Ω–∞—Ä—É–∂–µ–Ω—ã inf –≤ {col} –ø–æ—Å–ª–µ –æ–±—Ä–∞–±–æ—Ç–∫–∏, –∑–∞–º–µ–Ω—è–µ–º –Ω–∞ 0")
                        data_to_scale[col] = data_to_scale[col].replace([np.inf, -np.inf], 0)
                
                # 5. –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ –¥–∞–Ω–Ω—ã–µ –≥–æ—Ç–æ–≤—ã –¥–ª—è —Å–∫–µ–π–ª–∏–Ω–≥–∞
                if data_to_scale.shape[0] > 0 and not data_to_scale.isna().any().any():
                    try:
                        df.loc[valid_mask, feature_cols] = self.scalers[symbol].fit_transform(data_to_scale)
                    except Exception as e:
                        self.logger.error(f"–û—à–∏–±–∫–∞ —Å–∫–µ–π–ª–∏–Ω–≥–∞ –¥–ª—è {symbol}: {e}")
                        # –ï—Å–ª–∏ —Å–∫–µ–π–ª–∏–Ω–≥ –Ω–µ —É–¥–∞–ª—Å—è, –æ—Å—Ç–∞–≤–ª—è–µ–º –∏—Å—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
                        df.loc[valid_mask, feature_cols] = data_to_scale.fillna(0)
                else:
                    self.logger.warning(f"–ù–µ—Ç –≤–∞–ª–∏–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Å–∫–µ–π–ª–∏–Ω–≥–∞ {symbol}")
        
        return df
    
    def _normalize_walk_forward(self, df: pd.DataFrame, train_end_date: str) -> pd.DataFrame:
        """Walk-forward –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –±–µ–∑ data leakage"""
        self.logger.info(f"Walk-forward –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –¥–æ {train_end_date}...")
        
        # –°—Ç–æ–ª–±—Ü—ã –¥–ª—è –∏—Å–∫–ª—é—á–µ–Ω–∏—è –∏–∑ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏
        exclude_cols = [
            'id', 'symbol', 'timestamp', 'datetime', 'sector',
            'open', 'high', 'low', 'close', 'volume', 'turnover'
        ]
        
        # –¶–µ–ª–µ–≤—ã–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ
        target_cols = [col for col in df.columns if col.startswith(('target_', 'future_', 'optimal_'))]
        exclude_cols.extend(target_cols)
        
        # –í—Ä–µ–º–µ–Ω–Ω—ã–µ –∫–æ–ª–æ–Ω–∫–∏
        time_cols = ['hour', 'minute', 'dayofweek', 'day', 'month', 'is_weekend',
                    'asian_session', 'european_session', 'american_session', 'session_overlap']
        exclude_cols.extend(time_cols)
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –ø—Ä–∏–∑–Ω–∞–∫–∏ –¥–ª—è –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏
        feature_cols = [col for col in df.columns if col not in exclude_cols]
        
        # –ú–∞—Å–∫–∞ –¥–ª—è –æ–±—É—á–∞—é—â–∏—Ö –¥–∞–Ω–Ω—ã—Ö
        train_mask = df['datetime'] <= pd.to_datetime(train_end_date)
        
        # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –ø–æ —Å–∏–º–≤–æ–ª–∞–º
        for symbol in df['symbol'].unique():
            symbol_mask = df['symbol'] == symbol
            train_symbol_mask = symbol_mask & train_mask
            
            if train_symbol_mask.sum() > 0:
                if symbol not in self.scalers:
                    self.scalers[symbol] = StandardScaler()
                
                # –û–±—É—á–∞–µ–º scaler —Ç–æ–ª—å–∫–æ –Ω–∞ train –¥–∞–Ω–Ω—ã—Ö
                train_data = df.loc[train_symbol_mask, feature_cols].dropna()
                if len(train_data) > 0:
                    self.scalers[symbol].fit(train_data)
                    
                    # –ü—Ä–∏–º–µ–Ω—è–µ–º –∫–æ –≤—Å–µ–º –¥–∞–Ω–Ω—ã–º —Å–∏–º–≤–æ–ª–∞
                    valid_mask = symbol_mask & df[feature_cols].notna().all(axis=1)
                    if valid_mask.sum() > 0:
                        df.loc[valid_mask, feature_cols] = self.scalers[symbol].transform(
                            df.loc[valid_mask, feature_cols]
                        )
        
        return df
    
    def _log_feature_statistics(self, df: pd.DataFrame):
        """–õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –ø–æ –ø—Ä–∏–∑–Ω–∞–∫–∞–º"""
        feature_counts = {
            'basic': len([col for col in df.columns if col in [
                'returns', 'high_low_ratio', 'close_open_ratio', 'volume_ratio'
            ]]),
            'technical': len([col for col in df.columns if any(
                ind in col for ind in ['sma', 'ema', 'rsi', 'macd', 'bb', 'atr']
            )]),
            'microstructure': len([col for col in df.columns if any(
                ms in col for ms in ['spread', 'imbalance', 'toxicity', 'illiquidity']
            )]),
            'temporal': len([col for col in df.columns if any(
                t in col for t in ['hour', 'day', 'month', 'session']
            )]),
            'cross_asset': len([col for col in df.columns if any(
                ca in col for ca in ['btc_', 'sector', 'rank', 'momentum']
            )])
        }
        
        self.logger.info(f"üìä –°–æ–∑–¥–∞–Ω–æ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º: {feature_counts}")
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π
        missing_counts = df.isnull().sum()
        if missing_counts.sum() > 0:
            self.logger.warning(
                f"‚ö†Ô∏è –û–±–Ω–∞—Ä—É–∂–µ–Ω—ã –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è –≤ {missing_counts[missing_counts > 0].shape[0]} –ø—Ä–∏–∑–Ω–∞–∫–∞—Ö"
            )
    
    def get_feature_names(self, include_targets: bool = False) -> List[str]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å–ø–∏—Å–∫–∞ –Ω–∞–∑–≤–∞–Ω–∏–π –ø—Ä–∏–∑–Ω–∞–∫–æ–≤"""
        # TODO: –†–µ–∞–ª–∏–∑–æ–≤–∞—Ç—å –ø—Ä–∞–≤–∏–ª—å–Ω–æ–µ —Ö—Ä–∞–Ω–µ–Ω–∏–µ –Ω–∞–∑–≤–∞–Ω–∏–π –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
        return []
    
    def save_scalers(self, path: str):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Å–∫–µ–π–ª–µ—Ä–æ–≤ –¥–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –≤ –ø—Ä–æ–¥–∞–∫—à–µ–Ω–µ"""
        import pickle
        with open(path, 'wb') as f:
            pickle.dump(self.scalers, f)
        
        self.logger.info(f"–°–∫–µ–π–ª–µ—Ä—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ {path}")
    
    def load_scalers(self, path: str):
        """–ó–∞–≥—Ä—É–∑–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω—ã—Ö —Å–∫–µ–π–ª–µ—Ä–æ–≤"""
        import pickle
        with open(path, 'rb') as f:
            self.scalers = pickle.load(f)
        
        self.logger.info(f"–°–∫–µ–π–ª–µ—Ä—ã –∑–∞–≥—Ä—É–∂–µ–Ω—ã –∏–∑ {path}")