"""
PyTorch Dataset –∫–ª–∞—Å—Å—ã –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ –∏ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ä—è–¥–æ–≤
"""

import torch
from torch.utils.data import Dataset, DataLoader
import pandas as pd
import numpy as np
from typing import Dict, List, Optional, Tuple
from pathlib import Path
import warnings
warnings.filterwarnings('ignore')
from sklearn.preprocessing import RobustScaler
import pickle

from utils.logger import get_logger
from data.constants import TRADING_TARGET_VARIABLES, SERVICE_COLUMNS, get_feature_columns

class TimeSeriesDataset(Dataset):
    """Dataset –¥–ª—è –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ä—è–¥–æ–≤ –∫—Ä–∏–ø—Ç–æ–≤–∞–ª—é—Ç"""
    
    def __init__(self, 
                 data: pd.DataFrame,
                 context_window: int = 168,
                 prediction_window: int = 4,
                 feature_cols: List[str] = None,
                 target_cols: List[str] = None,
                 stride: int = 1,
                 normalize: bool = True,
                 scaler_path: Optional[str] = None,
                 fit_scaler: bool = False):
        """
        Args:
            data: DataFrame —Å –¥–∞–Ω–Ω—ã–º–∏
            context_window: —Ä–∞–∑–º–µ—Ä –≤—Ö–æ–¥–Ω–æ–≥–æ –æ–∫–Ω–∞
            prediction_window: —Ä–∞–∑–º–µ—Ä –æ–∫–Ω–∞ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
            feature_cols: —Å–ø–∏—Å–æ–∫ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
            target_cols: —Å–ø–∏—Å–æ–∫ —Ü–µ–ª–µ–≤—ã—Ö –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö
            stride: —à–∞–≥ –º–µ–∂–¥—É –æ–∫–Ω–∞–º–∏
        """
        self.logger = get_logger("TimeSeriesDataset")
        self.data = data.sort_values(['symbol', 'datetime']).reset_index(drop=True)
        self.context_window = context_window
        self.prediction_window = prediction_window
        self.target_window = prediction_window  # –î–æ–±–∞–≤–ª—è–µ–º target_window –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏
        self.stride = stride
        
        # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –∏ —Ü–µ–ª–µ–≤—ã—Ö –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö
        if feature_cols is None:
            self.feature_cols = [col for col in data.columns 
                               if col not in ['id', 'symbol', 'datetime', 'timestamp', 'sector']
                               and not col.startswith(('target_', 'future_', 'optimal_'))]
        else:
            self.feature_cols = feature_cols
            
        if target_cols is None:
            # –û–±–Ω–æ–≤–ª–µ–Ω–Ω—ã–π —Å–ø–∏—Å–æ–∫ —Ü–µ–ª–µ–≤—ã—Ö –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –¥–ª—è —Ç–æ—Ä–≥–æ–≤–æ–π –º–æ–¥–µ–ª–∏
            self.target_cols = [col for col in data.columns 
                              if col.startswith(('target_', 'future_return_', 'long_tp', 'short_tp', 
                                               'long_sl', 'short_sl', 'long_optimal', 'short_optimal',
                                               'best_direction'))]
        else:
            self.target_cols = target_cols
        
        # –°–æ–∑–¥–∞–Ω–∏–µ –∏–Ω–¥–µ–∫—Å–æ–≤ –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ –¥–æ—Å—Ç—É–ø–∞
        self._create_indices()
        
        # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö
        self.normalize = normalize
        self.scaler = None
        self.volume_based_cols = []
        self.price_based_cols = []
        self.ratio_cols = []
        
        if self.normalize:
            self._setup_normalization(scaler_path, fit_scaler)
        
        self.logger.info(f"Dataset —Å–æ–∑–¥–∞–Ω: {len(self)} –ø—Ä–∏–º–µ—Ä–æ–≤, "
                        f"{len(self.feature_cols)} –ø—Ä–∏–∑–Ω–∞–∫–æ–≤, "
                        f"{len(self.target_cols)} —Ü–µ–ª–µ–≤—ã—Ö –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö")
    
    def _create_indices(self):
        """–°–æ–∑–¥–∞–Ω–∏–µ –∏–Ω–¥–µ–∫—Å–æ–≤ –¥–ª—è –≤–∞–ª–∏–¥–Ω—ã—Ö –æ–∫–æ–Ω"""
        self.indices = []
        
        # –ì—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∞ –ø–æ —Å–∏–º–≤–æ–ª–∞–º
        for symbol in self.data['symbol'].unique():
            symbol_data = self.data[self.data['symbol'] == symbol]
            symbol_indices = symbol_data.index.tolist()
            
            # –°–æ–∑–¥–∞–Ω–∏–µ –æ–∫–æ–Ω —Å —É—á–µ—Ç–æ–º stride
            for i in range(0, len(symbol_indices) - self.context_window - self.prediction_window + 1, self.stride):
                # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–µ–ø—Ä–µ—Ä—ã–≤–Ω–æ—Å—Ç–∏ –≤—Ä–µ–º–µ–Ω–Ω–æ–≥–æ —Ä—è–¥–∞
                window_indices = symbol_indices[i:i + self.context_window + self.prediction_window]
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –≤—Å–µ –∏–Ω–¥–µ–∫—Å—ã –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω—ã
                if all(window_indices[j+1] - window_indices[j] == 1 for j in range(len(window_indices)-1)):
                    self.indices.append({
                        'symbol': symbol,
                        'start_idx': window_indices[0],
                        'context_end_idx': window_indices[self.context_window - 1],
                        'target_end_idx': window_indices[-1]
                    })
    
    def _setup_normalization(self, scaler_path: Optional[str] = None, fit_scaler: bool = False):
        """–ù–∞—Å—Ç—Ä–æ–π–∫–∞ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏ –¥–∞–Ω–Ω—ã—Ö"""
        self.logger.info("üîß –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏ –¥–∞–Ω–Ω—ã—Ö...")
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–∏–ø—ã –∫–æ–ª–æ–Ω–æ–∫ –¥–ª—è —Ä–∞–∑–Ω–æ–π –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏
        self.volume_based_cols = [col for col in self.feature_cols if any(
            pattern in col.lower() for pattern in ['volume', 'turnover', 'obv', 'liquidity', 'cmf', 'mfi']
        )]
        
        self.price_based_cols = [col for col in self.feature_cols if any(
            pattern in col.lower() for pattern in ['price', 'vwap', 'high', 'low', 'open', 'close']
        )]
        
        self.ratio_cols = [col for col in self.feature_cols if any(
            pattern in col.lower() for pattern in ['ratio', 'rsi', 'stoch', 'bb_', 'pct', 'toxicity']
        )]
        
        # –ó–∞–≥—Ä—É–∑–∫–∞ –∏–ª–∏ —Å–æ–∑–¥–∞–Ω–∏–µ scaler
        if scaler_path and Path(scaler_path).exists() and not fit_scaler:
            self.logger.info(f"üì• –ó–∞–≥—Ä—É–∑–∫–∞ scaler –∏–∑ {scaler_path}")
            with open(scaler_path, 'rb') as f:
                self.scaler = pickle.load(f)
        else:
            self.logger.info("üî® –°–æ–∑–¥–∞–Ω–∏–µ –Ω–æ–≤–æ–≥–æ scaler...")
            self.scaler = RobustScaler(quantile_range=(5, 95))
            
            if fit_scaler:
                # –§–∏—Ç–∏–º scaler –Ω–∞ —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
                self.logger.info("üìä –û–±—É—á–µ–Ω–∏–µ scaler –Ω–∞ –¥–∞–Ω–Ω—ã—Ö...")
                
                # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è scaler
                scaler_data = self.data[self.feature_cols].copy()
                
                # –ü—Ä–∏–º–µ–Ω—è–µ–º log-—Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏—é –∫ –æ–±—ä–µ–º–Ω—ã–º –∫–æ–ª–æ–Ω–∫–∞–º
                for col in self.volume_based_cols:
                    if col in scaler_data.columns:
                        # –ò–°–ü–†–ê–í–õ–ï–ù–û: –ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è –≤ —á–∏—Å–ª–æ–≤–æ–π —Ç–∏–ø –ø–µ—Ä–µ–¥ log-—Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏–µ–π
                        scaler_data[col] = pd.to_numeric(scaler_data[col], errors='coerce')
                        # Log —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏—è —Å –∑–∞—â–∏—Ç–æ–π –æ—Ç –æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π
                        scaler_data[col] = np.log1p(np.clip(scaler_data[col], 0, None))
                
                # –ö–ª–∏–ø–ø–∏–Ω–≥ —ç–∫—Å—Ç—Ä–µ–º–∞–ª—å–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π –ø–µ—Ä–µ–¥ –æ–±—É—á–µ–Ω–∏–µ–º scaler
                for col in scaler_data.columns:
                    if col not in self.ratio_cols:  # –ù–µ –∫–ª–∏–ø–ø–∏–º ratio –∫–æ–ª–æ–Ω–∫–∏
                        # –ò–°–ü–†–ê–í–õ–ï–ù–û: –ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è –≤ —á–∏—Å–ª–æ–≤–æ–π —Ç–∏–ø –ø–µ—Ä–µ–¥ –∫–≤–∞–Ω—Ç–∏–ª—è–º–∏
                        scaler_data[col] = pd.to_numeric(scaler_data[col], errors='coerce')
                        
                        # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –∫–æ–ª–æ–Ω–∫–∏ —Å —Ç–æ–ª—å–∫–æ NaN –∑–Ω–∞—á–µ–Ω–∏—è–º–∏
                        if scaler_data[col].notna().sum() == 0:
                            self.logger.warning(f"–ö–æ–ª–æ–Ω–∫–∞ '{col}' —Å–æ–¥–µ—Ä–∂–∏—Ç —Ç–æ–ª—å–∫–æ NaN –∑–Ω–∞—á–µ–Ω–∏—è, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º")
                            continue
                            
                        q99 = scaler_data[col].quantile(0.99)
                        q01 = scaler_data[col].quantile(0.01)
                        scaler_data[col] = np.clip(scaler_data[col], q01, q99)
                
                # –û–±—É—á–∞–µ–º scaler
                self.scaler.fit(scaler_data.values)
                
                # –°–æ—Ö—Ä–∞–Ω—è–µ–º scaler –µ—Å–ª–∏ —É–∫–∞–∑–∞–Ω –ø—É—Ç—å
                if scaler_path:
                    Path(scaler_path).parent.mkdir(parents=True, exist_ok=True)
                    with open(scaler_path, 'wb') as f:
                        pickle.dump(self.scaler, f)
                    self.logger.info(f"üíæ Scaler —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤ {scaler_path}")
        
        self.logger.info(f"‚úÖ –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –Ω–∞—Å—Ç—Ä–æ–µ–Ω–∞: {len(self.volume_based_cols)} –æ–±—ä–µ–º–Ω—ã—Ö, "
                        f"{len(self.price_based_cols)} —Ü–µ–Ω–æ–≤—ã—Ö, {len(self.ratio_cols)} ratio –∫–æ–ª–æ–Ω–æ–∫")
    
    def __len__(self):
        return len(self.indices)
    
    def __getitem__(self, idx):
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –æ–¥–Ω–æ–≥–æ –ø—Ä–∏–º–µ—Ä–∞"""
        index_info = self.indices[idx]
        
        # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
        context_start = index_info['start_idx']
        context_end = index_info['context_end_idx'] + 1
        context_data = self.data.iloc[context_start:context_end]
        
        # –ò–°–ü–†–ê–í–õ–ï–ù–û: –¶–µ–ª–µ–≤—ã–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –±–µ—Ä–µ–º –∏–∑ –ø–æ—Å–ª–µ–¥–Ω–µ–π —Å—Ç—Ä–æ–∫–∏ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
        # –∞ –Ω–µ –∏–∑ –±—É–¥—É—â–∏—Ö —Å—Ç—Ä–æ–∫!
        
        # –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –≤ —Ç–µ–Ω–∑–æ—Ä—ã —Å –æ–±—Ä–∞–±–æ—Ç–∫–æ–π object —Ç–∏–ø–æ–≤
        feature_data = context_data[self.feature_cols]
        
        # –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï: –ù–∞–¥—ë–∂–Ω–∞—è –∫–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è –≤ —á–∏—Å–ª–æ–≤—ã–µ —Ç–∏–ø—ã
        feature_data = feature_data.copy()
        
        # –ü—Ä–∏–º–µ–Ω—è–µ–º pd.to_numeric –∫–æ –≤—Å–µ–º –∫–æ–ª–æ–Ω–∫–∞–º –¥–ª—è –Ω–∞–¥—ë–∂–Ω–æ—Å—Ç–∏
        for col in feature_data.columns:
            try:
                # –°–Ω–∞—á–∞–ª–∞ –ø—Ä–æ–≤–µ—Ä—è–µ–º —Ç–∏–ø
                if pd.api.types.is_object_dtype(feature_data[col]) or pd.api.types.is_categorical_dtype(feature_data[col]):
                    if pd.api.types.is_categorical_dtype(feature_data[col]):
                        # –ö–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –∫–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ –∫–æ–¥—ã
                        feature_data[col] = feature_data[col].cat.codes.astype(np.float32)
                    else:
                        # Object —Ç–∏–ø—ã –∫–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º —á–µ—Ä–µ–∑ pd.to_numeric
                        feature_data[col] = pd.to_numeric(feature_data[col], errors='coerce').fillna(0.0).astype(np.float32)
                else:
                    # –£–∂–µ —á–∏—Å–ª–æ–≤—ã–µ —Ç–∏–ø—ã –ø—Ä–æ—Å—Ç–æ –ø—Ä–∏–≤–æ–¥–∏–º –∫ float32
                    feature_data[col] = feature_data[col].astype(np.float32)
            except Exception as e:
                # –ï—Å–ª–∏ —á—Ç–æ-—Ç–æ –ø–æ—à–ª–æ –Ω–µ —Ç–∞–∫, –∑–∞–ø–æ–ª–Ω—è–µ–º –Ω—É–ª—è–º–∏
                feature_data[col] = np.zeros(len(feature_data), dtype=np.float32)
        
        # –ü–æ–ª—É—á–∞–µ–º –º–∞—Å—Å–∏–≤ –∏ —Ñ–∏–Ω–∞–ª—å–Ω–∞—è –æ—á–∏—Å—Ç–∫–∞ –æ—Ç inf/nan
        feature_values = feature_data.values
        feature_values = np.nan_to_num(feature_values, nan=0.0, posinf=0.0, neginf=0.0)
        
        # –ü—Ä–∏–º–µ–Ω—è–µ–º –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—é –µ—Å–ª–∏ –≤–∫–ª—é—á–µ–Ω–∞
        if self.normalize and self.scaler is not None:
            # –°–æ–∑–¥–∞–µ–º –∫–æ–ø–∏—é –¥–ª—è –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏
            norm_values = feature_values.copy()
            
            # –ü—Ä–∏–º–µ–Ω—è–µ–º log-—Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏—é –∫ –æ–±—ä–µ–º–Ω—ã–º –∫–æ–ª–æ–Ω–∫–∞–º
            for i, col in enumerate(self.feature_cols):
                if col in self.volume_based_cols:
                    # Log —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏—è —Å –∑–∞—â–∏—Ç–æ–π –æ—Ç –æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π
                    norm_values[:, i] = np.log1p(np.clip(norm_values[:, i], 0, None))
            
            # –ö–ª–∏–ø–ø–∏–Ω–≥ —ç–∫—Å—Ç—Ä–µ–º–∞–ª—å–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π
            for i, col in enumerate(self.feature_cols):
                if col not in self.ratio_cols:  # –ù–µ –∫–ª–∏–ø–ø–∏–º ratio –∫–æ–ª–æ–Ω–∫–∏
                    # –ò—Å–ø–æ–ª—å–∑—É–µ–º –∫–≤–∞–Ω—Ç–∏–ª–∏ –∏–∑ –æ–±—É—á–∞—é—â–∏—Ö –¥–∞–Ω–Ω—ã—Ö
                    q99 = np.percentile(norm_values[:, i], 99)
                    q01 = np.percentile(norm_values[:, i], 1)
                    norm_values[:, i] = np.clip(norm_values[:, i], q01, q99)
            
            # –ü—Ä–∏–º–µ–Ω—è–µ–º RobustScaler
            try:
                norm_values = self.scaler.transform(norm_values)
            except Exception as e:
                self.logger.warning(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏: {e}")
                # Fallback –∫ –∏—Å—Ö–æ–¥–Ω—ã–º –∑–Ω–∞—á–µ–Ω–∏—è–º
                norm_values = feature_values
            
            # –§–∏–Ω–∞–ª—å–Ω—ã–π –∫–ª–∏–ø–ø–∏–Ω–≥ –ø–æ—Å–ª–µ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏
            norm_values = np.clip(norm_values, -10, 10)
            
            X = torch.FloatTensor(norm_values)
        else:
            X = torch.FloatTensor(feature_values)
        
        # –û–±—Ä–∞–±–æ—Ç–∫–∞ —Ü–µ–ª–µ–≤—ã—Ö –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö
        if len(self.target_cols) > 0:
            # –ë–µ—Ä–µ–º —Ü–µ–ª–µ–≤—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è –∏–∑ –ü–û–°–õ–ï–î–ù–ï–ô —Å—Ç—Ä–æ–∫–∏ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
            # –≠—Ç–æ –∑–Ω–∞—á–µ–Ω–∏—è future_return_X –∫–æ—Ç–æ—Ä—ã–µ —É–∂–µ —Å–æ–¥–µ—Ä–∂–∞—Ç –±—É–¥—É—â—É—é –¥–æ—Ö–æ–¥–Ω–æ—Å—Ç—å
            y_data = context_data.iloc[-1][self.target_cols]
            
            # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –≤ —á–∏—Å–ª–æ–≤—ã–µ
            y_values = []
            for col in self.target_cols:
                value = y_data[col]
                
                # –û–±—Ä–∞–±–æ—Ç–∫–∞ –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω–æ–π –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π best_direction
                if col == 'best_direction':
                    if pd.api.types.is_categorical_dtype(value) or isinstance(value, str):
                        # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ —á–∏—Å–ª–æ–≤–æ–π –∫–æ–¥: LONG=0, SHORT=1, NEUTRAL=2
                        if value == 'LONG':
                            y_values.append(0)
                        elif value == 'SHORT':
                            y_values.append(1)
                        elif value == 'NEUTRAL':
                            y_values.append(2)
                        else:
                            y_values.append(2)  # fallback –∫ NEUTRAL
                    else:
                        y_values.append(float(value))
                else:
                    # –û–±—ã—á–Ω—ã–µ —á–∏—Å–ª–æ–≤—ã–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ
                    y_values.append(float(value))
            
            y_values = np.array(y_values, dtype=np.float32)
            
            # –ï—Å–ª–∏ –º–æ–¥–µ–ª—å –æ–∂–∏–¥–∞–µ—Ç –Ω–µ—Å–∫–æ–ª—å–∫–æ –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —à–∞–≥–æ–≤, –¥—É–±–ª–∏—Ä—É–µ–º –∑–Ω–∞—á–µ–Ω–∏–µ
            if self.target_window > 1:
                y = torch.FloatTensor([y_values] * self.target_window)
            else:
                y = torch.FloatTensor([y_values])
        else:
            y = torch.FloatTensor([])
        
        # –î–∞–Ω–Ω—ã–µ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ (–æ—Å—Ç–∞–≤–ª—è–µ–º –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏)
        target_start = index_info['context_end_idx'] + 1
        target_end = index_info['target_end_idx'] + 1
        target_data = self.data.iloc[target_start:target_end]
        
        # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è
        info = {
            'symbol': index_info['symbol'],
            'context_start_time': str(context_data.iloc[0]['datetime']),
            'context_end_time': str(context_data.iloc[-1]['datetime']),
            'target_start_time': str(target_data.iloc[0]['datetime']) if len(target_data) > 0 else None,
            'target_end_time': str(target_data.iloc[-1]['datetime']) if len(target_data) > 0 else None
        }
        
        return X, y, info


class TradingDataset(TimeSeriesDataset):
    """–°–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π Dataset –¥–ª—è —Ç–æ—Ä–≥–æ–≤—ã—Ö —Å–∏–≥–Ω–∞–ª–æ–≤"""
    
    def __init__(self, 
                 data: pd.DataFrame,
                 config: Dict = None,
                 context_window: int = 168,
                 prediction_window: int = 4,
                 feature_cols: List[str] = None,
                 target_cols: List[str] = None,
                 include_price_data: bool = True,
                 **kwargs):
        
        # –ö–†–ò–¢–ò–ß–ù–û–ï –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï: –ü—Ä–∞–≤–∏–ª—å–Ω–∞—è —Ä–∞–±–æ—Ç–∞ —Å —Ç–æ—Ä–≥–æ–≤—ã–º–∏ —Ü–µ–ª–µ–≤—ã–º–∏ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–º–∏
        if target_cols is not None:
            # –ï—Å–ª–∏ target_cols –ø–µ—Ä–µ–¥–∞–Ω—ã —è–≤–Ω–æ, –∏—Å–ø–æ–ª—å–∑—É–µ–º –∏—Ö
            trading_targets = target_cols
            print(f"‚úÖ –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è {len(trading_targets)} —Ü–µ–ª–µ–≤—ã—Ö –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –∏–∑ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤")
        elif target_cols is None and config is not None:
            # –ü–æ–ª—É—á–∞–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é –º–æ–¥–µ–ª–∏
            model_config = config.get('model', {})
            task_type = model_config.get('task_type', 'regression')
            
            if task_type == 'trading':
                # –î–ª—è —Ç–æ—Ä–≥–æ–≤–æ–π –º–æ–¥–µ–ª–∏ –∏—Å–ø–æ–ª—å–∑—É–µ–º —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ —Ü–µ–ª–µ–≤—ã–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ
                trading_target_variables = model_config.get('target_variables', [])
                
                if trading_target_variables:
                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–∞–∫–∏–µ —Ü–µ–ª–µ–≤—ã–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –µ—Å—Ç—å –≤ –¥–∞–Ω–Ω—ã—Ö
                    available_trading_targets = [var for var in trading_target_variables if var in data.columns]
                    
                    if available_trading_targets:
                        trading_targets = available_trading_targets
                        print(f"‚úÖ –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —Ç–æ—Ä–≥–æ–≤–∞—è –º–æ–¥–µ–ª—å —Å {len(trading_targets)} —Ü–µ–ª–µ–≤—ã–º–∏ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–º–∏")
                        print(f"   –î–æ—Å—Ç—É–ø–Ω—ã–µ —Ü–µ–ª–∏: {trading_targets}")
                    else:
                        print("‚ùå –¢–æ—Ä–≥–æ–≤—ã–µ —Ü–µ–ª–µ–≤—ã–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –∏–∑ –∫–æ–Ω—Ñ–∏–≥–∞ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã –≤ –¥–∞–Ω–Ω—ã—Ö!")
                        # Fallback –∫ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–º—É –ø–æ–∏—Å–∫—É —Ç–æ—Ä–≥–æ–≤—ã—Ö —Ü–µ–ª–µ–π
                        auto_trading_targets = [col for col in data.columns 
                                              if col.startswith(('long_tp', 'short_tp', 'long_sl', 'short_sl', 'best_direction'))]
                        
                        if auto_trading_targets:
                            trading_targets = auto_trading_targets[:11]  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –æ—Å–Ω–æ–≤–Ω—ã–º–∏
                            print(f"üîß –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –Ω–∞–π–¥–µ–Ω–æ {len(trading_targets)} —Ç–æ—Ä–≥–æ–≤—ã—Ö —Ü–µ–ª–µ–π")
                        else:
                            raise ValueError("‚ùå –¢–æ—Ä–≥–æ–≤—ã–µ —Ü–µ–ª–µ–≤—ã–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã!")
                else:
                    # –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–∏—Å–∫ —Ç–æ—Ä–≥–æ–≤—ã—Ö —Ü–µ–ª–µ–≤—ã—Ö –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö
                    auto_trading_targets = [col for col in data.columns 
                                          if col.startswith(('long_tp', 'short_tp', 'long_sl', 'short_sl', 'best_direction'))]
                    
                    if auto_trading_targets:
                        # –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç–Ω—ã–µ —Ç–æ—Ä–≥–æ–≤—ã–µ —Ü–µ–ª–∏
                        priority_targets = [
                            'long_tp1_reached', 'long_tp2_reached', 'long_tp3_reached', 'long_sl_reached',
                            'short_tp1_reached', 'short_tp2_reached', 'short_tp3_reached', 'short_sl_reached',
                            'best_direction'
                        ]
                        trading_targets = [t for t in priority_targets if t in data.columns]
                        
                        if len(trading_targets) < 5:  # –ú–∏–Ω–∏–º—É–º –¥–ª—è —Ç–æ—Ä–≥–æ–≤–æ–π –º–æ–¥–µ–ª–∏
                            trading_targets = auto_trading_targets[:11]  # –ë–µ—Ä–µ–º –ø–µ—Ä–≤—ã–µ 11
                        
                        print(f"üîß –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –≤—ã–±—Ä–∞–Ω–æ {len(trading_targets)} —Ç–æ—Ä–≥–æ–≤—ã—Ö —Ü–µ–ª–µ–π")
                    else:
                        raise ValueError("‚ùå –¢–æ—Ä–≥–æ–≤—ã–µ —Ü–µ–ª–µ–≤—ã–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã –≤ –¥–∞–Ω–Ω—ã—Ö!")
            else:
                # –î–ª—è —Ä–µ–≥—Ä–µ—Å—Å–∏–∏/–∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ –∏—Å–ø–æ–ª—å–∑—É–µ–º —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—É—é –ª–æ–≥–∏–∫—É
                target_variable = model_config.get('target_variable', 'future_return_4')
                
                # –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï: –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ —Ü–µ–ª–µ–≤–∞—è –ø–µ—Ä–µ–º–µ–Ω–Ω–∞—è —Å—É—â–µ—Å—Ç–≤—É–µ—Ç –≤ –¥–∞–Ω–Ω—ã—Ö
                if target_variable in data.columns:
                    trading_targets = [target_variable]
                    print(f"‚úÖ –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —Ü–µ–ª–µ–≤–∞—è –ø–µ—Ä–µ–º–µ–Ω–Ω–∞—è: {target_variable} (—Ç–∏–ø: {task_type})")
                else:
                    # Fallback –∫ –¥–æ—Å—Ç—É–ø–Ω—ã–º —Ü–µ–ª–µ–≤—ã–º –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–º
                    available_targets = [col for col in data.columns 
                                   if col.startswith(('target_', 'future_return_'))]
                    
                    if available_targets:
                        # –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç: future_return_4 > future_return_3 > future_return_2 > future_return_1
                        preferred_targets = ['future_return_4', 'future_return_3', 'future_return_2', 'future_return_1']
                        trading_targets = None
                        for pref_target in preferred_targets:
                            if pref_target in available_targets:
                                trading_targets = [pref_target]
                                break
                        
                        if trading_targets is None:
                            trading_targets = [available_targets[0]]  # –ë–µ—Ä–µ–º –ø–µ—Ä–≤—É—é –¥–æ—Å—Ç—É–ø–Ω—É—é
                        
                        print(f"‚ö†Ô∏è  –¶–µ–ª–µ–≤–∞—è –ø–µ—Ä–µ–º–µ–Ω–Ω–∞—è {target_variable} –Ω–µ –Ω–∞–π–¥–µ–Ω–∞. –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è: {trading_targets[0]}")
                    else:
                        # –ö–†–ò–¢–ò–ß–ù–û–ï –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï: –ù–ï —Å–æ–∑–¥–∞–µ–º –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –∑–∞–Ω–æ–≤–æ –ø—Ä–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–∏ –∫—ç—à–∞!
                        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –µ—Å—Ç—å –ª–∏ ANY —Ç–æ—Ä–≥–æ–≤—ã–µ —Ü–µ–ª–µ–≤—ã–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –≤ –∫—ç—à–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
                        any_trading_targets = [col for col in data.columns 
                                             if any(pattern in col for pattern in ['_hit', '_reached', '_tp', '_sl', 'best_direction', 'future_return'])]
                        
                        if any_trading_targets:
                            # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ —Ç–æ—Ä–≥–æ–≤—ã–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –∏–∑ –∫—ç—à–∞
                            trading_targets = any_trading_targets[:36]  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–æ 36 –¥–ª—è –º–æ–¥–µ–ª–∏
                            print(f"‚úÖ –ù–∞–π–¥–µ–Ω–æ {len(trading_targets)} —Ç–æ—Ä–≥–æ–≤—ã—Ö –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –≤ –∫—ç—à–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö")
                            print(f"   –ò—Å–ø–æ–ª—å–∑—É–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ —Ü–µ–ª–∏ –∏–∑ –∫—ç—à–∞: {trading_targets[:5]}...")
                        else:
                            raise ValueError("‚ùå –ö—ç—à–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –Ω–µ —Å–æ–¥–µ—Ä–∂–∞—Ç —Ç–æ—Ä–≥–æ–≤—ã—Ö —Ü–µ–ª–µ–≤—ã—Ö –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö! –ü–µ—Ä–µ—Å–æ–∑–¥–∞–π—Ç–µ –∫—ç—à: python prepare_trading_data.py --force-recreate")
                    
        elif target_cols is None:
            # –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –Ω–∞—Ö–æ–¥–∏–º —Ü–µ–ª–µ–≤—ã–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ (—Å—Ç–∞—Ä–æ–µ –ø–æ–≤–µ–¥–µ–Ω–∏–µ)
            available_targets = [col for col in data.columns 
                               if col.startswith(('target_', 'future_return_'))]
            
            if available_targets:
                # –ò–°–ü–†–ê–í–õ–ï–ù–û: –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ç–æ–ª—å–∫–æ future_return_4 –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏ —Å pred_len=4
                if 'future_return_4' in available_targets:
                    trading_targets = ['future_return_4']
                else:
                    # Fallback –∫ –ª—é–±–æ–º—É future_return
                    future_returns = [col for col in available_targets if 'future_return_' in col]
                    trading_targets = future_returns[:1] if future_returns else [available_targets[0]]
            else:
                raise ValueError("‚ùå –¶–µ–ª–µ–≤—ã–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã –≤ –¥–∞–Ω–Ω—ã—Ö –∏ config –Ω–µ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω!")
        else:
            trading_targets = target_cols
        
        # –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï: –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏ —Ü–µ–ª–µ–π —Å –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–µ–π –º–æ–¥–µ–ª–∏
        if config is not None:
            model_config = config.get('model', {})
            task_type = model_config.get('task_type', 'regression')
            expected_output_size = model_config.get('output_size', 1)
            
            # –î–ª—è —Ç–æ—Ä–≥–æ–≤–æ–π –º–æ–¥–µ–ª–∏ –ù–ï –æ–±—Ä–µ–∑–∞–µ–º —Ü–µ–ª–µ–≤—ã–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ
            if task_type != 'trading' and len(trading_targets) != expected_output_size:
                print(f"‚ö†Ô∏è  –ù–µ—Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–µ–π: target_cols={len(trading_targets)}, output_size={expected_output_size}")
                
                # –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–µ: –±–µ—Ä–µ–º —Ç–æ–ª—å–∫–æ –Ω—É–∂–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ü–µ–ª–µ–π
                if len(trading_targets) > expected_output_size:
                    trading_targets = trading_targets[:expected_output_size]
                    print(f"üîß –ò—Å–ø—Ä–∞–≤–ª–µ–Ω–æ: –∏—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è –ø–µ—Ä–≤—ã–µ {expected_output_size} —Ü–µ–ª–µ–π: {trading_targets}")
            elif task_type == 'trading':
                print(f"‚úÖ –¢–æ—Ä–≥–æ–≤–∞—è –º–æ–¥–µ–ª—å: –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è {len(trading_targets)} —Ü–µ–ª–µ–≤—ã—Ö –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö")
        
        # –í–ê–ñ–ù–û: –ù–ï –ø–µ—Ä–µ—Å–æ–∑–¥–∞–µ–º –¥–∞–Ω–Ω—ã–µ –µ—Å–ª–∏ –æ–Ω–∏ —É–∂–µ –µ—Å—Ç—å –≤ –∫—ç—à–µ!
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –≤—Å–µ –ª–∏ –Ω—É–∂–Ω—ã–µ —Ü–µ–ª–µ–≤—ã–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –ø—Ä–∏—Å—É—Ç—Å—Ç–≤—É—é—Ç
        missing_targets = [target for target in trading_targets if target not in data.columns]
        
        if missing_targets:
            print(f"‚ö†Ô∏è  –û—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç —Ü–µ–ª–µ–≤—ã–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ: {missing_targets}")
            print("‚ùå –ü–µ—Ä–µ—Å–æ–∑–¥–∞–π—Ç–µ –∫—ç—à —Å –ø—Ä–∞–≤–∏–ª—å–Ω—ã–º–∏ —Ü–µ–ª–µ–≤—ã–º–∏ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–º–∏:")
            print("   python prepare_trading_data.py --force-recreate")
            raise ValueError(f"–û—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç —Ü–µ–ª–µ–≤—ã–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –≤ –∫—ç—à–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö: {missing_targets}")
        else:
            print(f"‚úÖ –í—Å–µ {len(trading_targets)} —Ü–µ–ª–µ–≤—ã—Ö –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –Ω–∞–π–¥–µ–Ω—ã –≤ –∫—ç—à–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö")
        
        # –î–æ–±–∞–≤–ª—è–µ–º target_window –µ—Å–ª–∏ –æ–Ω –ø–µ—Ä–µ–¥–∞–Ω –≤ kwargs
        if 'target_window' in kwargs:
            target_window = kwargs.pop('target_window')
        else:
            target_window = prediction_window
            
        super().__init__(
            data=data,
            context_window=context_window,
            prediction_window=target_window,
            feature_cols=feature_cols,
            target_cols=trading_targets,
            **kwargs
        )
        
        self.include_price_data = include_price_data
        
        # –ò–Ω–¥–µ–∫—Å—ã —Ü–µ–Ω–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö
        if include_price_data:
            self.price_cols = ['open', 'high', 'low', 'close', 'volume']
    
    def __getitem__(self, idx):
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –ø—Ä–∏–º–µ—Ä–∞ —Å —Ç–æ—Ä–≥–æ–≤–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π"""
        X, y, info = super().__getitem__(idx)
        
        index_info = self.indices[idx]
        
        # –î–æ–±–∞–≤–ª—è–µ–º —Ü–µ–Ω–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
        if self.include_price_data:
            context_start = index_info['start_idx']
            context_end = index_info['context_end_idx'] + 1
            price_data = self.data.iloc[context_start:context_end][self.price_cols]
            info['price_data'] = torch.FloatTensor(price_data.values)
        
        # –î–æ–±–∞–≤–ª—è–µ–º –ø–æ—Å–ª–µ–¥–Ω—é—é —Ü–µ–Ω—É –¥–ª—è —Ä–∞—Å—á–µ—Ç–∞ —É—Ä–æ–≤–Ω–µ–π
        last_idx = index_info['context_end_idx']
        info['last_close'] = self.data.iloc[last_idx]['close']
        info['atr'] = self.data.iloc[last_idx].get('atr', info['last_close'] * 0.02)
        
        return X, y, info


class MultiSymbolDataset(Dataset):
    """Dataset –¥–ª—è –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ–π —Ä–∞–±–æ—Ç—ã —Å –Ω–µ—Å–∫–æ–ª—å–∫–∏–º–∏ —Å–∏–º–≤–æ–ª–∞–º–∏"""
    
    def __init__(self,
                 data: pd.DataFrame,
                 symbols: List[str],
                 context_window: int = 168,
                 prediction_window: int = 4,
                 feature_cols: List[str] = None,
                 synchronize: bool = True):
        """
        Args:
            data: DataFrame —Å–æ –≤—Å–µ–º–∏ –¥–∞–Ω–Ω—ã–º–∏
            symbols: —Å–ø–∏—Å–æ–∫ —Å–∏–º–≤–æ–ª–æ–≤ –¥–ª—è –±–∞—Ç—á–∞
            context_window: —Ä–∞–∑–º–µ—Ä –≤—Ö–æ–¥–Ω–æ–≥–æ –æ–∫–Ω–∞
            prediction_window: —Ä–∞–∑–º–µ—Ä –æ–∫–Ω–∞ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
            feature_cols: —Å–ø–∏—Å–æ–∫ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
            synchronize: —Å–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∏—Ä–æ–≤–∞—Ç—å –≤—Ä–µ–º–µ–Ω–Ω—ã–µ –º–µ—Ç–∫–∏ –º–µ–∂–¥—É —Å–∏–º–≤–æ–ª–∞–º–∏
        """
        self.logger = get_logger("MultiSymbolDataset")
        self.symbols = symbols
        self.context_window = context_window
        self.prediction_window = prediction_window
        self.synchronize = synchronize
        
        # –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö –ø–æ —Å–∏–º–≤–æ–ª–∞–º
        self.data = data[data['symbol'].isin(symbols)].copy()
        
        # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
        if feature_cols is None:
            self.feature_cols = [col for col in data.columns 
                               if col not in ['id', 'symbol', 'datetime', 'timestamp', 'sector']
                               and not col.startswith(('target_', 'future_', 'optimal_'))]
        else:
            self.feature_cols = feature_cols
        
        # –°–æ–∑–¥–∞–Ω–∏–µ —Å–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –∏–Ω–¥–µ–∫—Å–æ–≤
        if synchronize:
            self._create_synchronized_indices()
        else:
            self._create_independent_indices()
        
        self.logger.info(f"MultiSymbolDataset —Å–æ–∑–¥–∞–Ω: {len(self)} –±–∞—Ç—á–µ–π, "
                        f"{len(symbols)} —Å–∏–º–≤–æ–ª–æ–≤")
    
    def _create_synchronized_indices(self):
        """–°–æ–∑–¥–∞–Ω–∏–µ —Å–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –ø–æ –≤—Ä–µ–º–µ–Ω–∏ –∏–Ω–¥–µ–∫—Å–æ–≤"""
        self.indices = []
        
        # –ù–∞—Ö–æ–¥–∏–º –æ–±—â–∏–µ –≤—Ä–µ–º–µ–Ω–Ω—ã–µ –º–µ—Ç–∫–∏
        common_times = None
        for symbol in self.symbols:
            symbol_times = set(self.data[self.data['symbol'] == symbol]['datetime'].values)
            if common_times is None:
                common_times = symbol_times
            else:
                common_times = common_times.intersection(symbol_times)
        
        common_times = sorted(list(common_times))
        
        # –°–æ–∑–¥–∞–µ–º –æ–∫–Ω–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ –æ–±—â–∏—Ö –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –º–µ—Ç–æ–∫
        window_size = self.context_window + self.prediction_window
        for i in range(len(common_times) - window_size + 1):
            window_times = common_times[i:i + window_size]
            self.indices.append({
                'start_time': window_times[0],
                'context_end_time': window_times[self.context_window - 1],
                'end_time': window_times[-1]
            })
    
    def _create_independent_indices(self):
        """–°–æ–∑–¥–∞–Ω–∏–µ –Ω–µ–∑–∞–≤–∏—Å–∏–º—ã—Ö –∏–Ω–¥–µ–∫—Å–æ–≤ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Å–∏–º–≤–æ–ª–∞"""
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º TimeSeriesDataset –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Å–∏–º–≤–æ–ª–∞
        self.symbol_datasets = {}
        
        for symbol in self.symbols:
            symbol_data = self.data[self.data['symbol'] == symbol]
            self.symbol_datasets[symbol] = TimeSeriesDataset(
                data=symbol_data,
                context_window=self.context_window,
                prediction_window=self.prediction_window,
                feature_cols=self.feature_cols
            )
        
        # –ò–Ω–¥–µ–∫—Å—ã - —ç—Ç–æ –º–∏–Ω–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–∏–º–µ—Ä–æ–≤ —Å—Ä–µ–¥–∏ –≤—Å–µ—Ö —Å–∏–º–≤–æ–ª–æ–≤
        min_length = min(len(ds) for ds in self.symbol_datasets.values())
        self.indices = list(range(min_length))
    
    def __len__(self):
        return len(self.indices)
    
    def __getitem__(self, idx):
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –±–∞—Ç—á–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –≤—Å–µ—Ö —Å–∏–º–≤–æ–ª–æ–≤"""
        batch_X = []
        batch_y = []
        batch_info = []
        
        if self.synchronize:
            # –°–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –≤—ã–±–æ—Ä–∫–∞
            index_info = self.indices[idx]
            
            for symbol in self.symbols:
                # –ü–æ–ª—É—á–∞–µ–º –¥–∞–Ω–Ω—ã–µ –¥–ª—è –≤—Ä–µ–º–µ–Ω–Ω–æ–≥–æ –æ–∫–Ω–∞
                symbol_data = self.data[
                    (self.data['symbol'] == symbol) &
                    (self.data['datetime'] >= index_info['start_time']) &
                    (self.data['datetime'] <= index_info['end_time'])
                ].sort_values('datetime')
                
                if len(symbol_data) == self.context_window + self.prediction_window:
                    # –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –Ω–∞ –∫–æ–Ω—Ç–µ–∫—Å—Ç –∏ —Ü–µ–ª—å
                    context_data = symbol_data.iloc[:self.context_window]
                    target_data = symbol_data.iloc[self.context_window:]
                    
                    X = torch.FloatTensor(context_data[self.feature_cols].values)
                    y = torch.FloatTensor(target_data[self.feature_cols].values)
                    
                    batch_X.append(X)
                    batch_y.append(y)
                    batch_info.append({
                        'symbol': symbol,
                        'start_time': index_info['start_time'],
                        'end_time': index_info['end_time']
                    })
        else:
            # –ù–µ–∑–∞–≤–∏—Å–∏–º–∞—è –≤—ã–±–æ—Ä–∫–∞
            for symbol in self.symbols:
                X, y, info = self.symbol_datasets[symbol][idx]
                batch_X.append(X)
                batch_y.append(y)
                batch_info.append(info)
        
        # –°—Ç–µ–∫–∏—Ä–æ–≤–∞–Ω–∏–µ –≤ –±–∞—Ç—á
        batch_X = torch.stack(batch_X)
        batch_y = torch.stack(batch_y) if len(batch_y[0]) > 0 else torch.tensor([])
        
        return batch_X, batch_y, batch_info


def create_data_loaders(train_data: pd.DataFrame,
                       val_data: pd.DataFrame,
                       test_data: pd.DataFrame,
                       config: Dict,
                       feature_cols: List[str] = None,
                       target_cols: List[str] = None) -> Tuple[DataLoader, DataLoader, DataLoader]:
    """–°–æ–∑–¥–∞–Ω–∏–µ DataLoader'–æ–≤ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è"""
    
    logger = get_logger("DataLoaders")
    
    batch_size = config['model']['batch_size']
    context_window = config['model']['context_window']
    pred_window = config['model']['pred_len']
    num_workers = config['performance']['num_workers']
    persistent_workers = config['performance'].get('persistent_workers', True) if num_workers > 0 else False
    prefetch_factor = config['performance'].get('prefetch_factor', 2)
    
    logger.info("üì¶ –°–æ–∑–¥–∞–Ω–∏–µ —É–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –¥–∞—Ç–∞—Å–µ—Ç–æ–≤...")
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –≤–∞–ª–∏–¥–Ω–æ—Å—Ç–∏ –¥–∞–Ω–Ω—ã—Ö
    if len(train_data) == 0 or len(val_data) == 0 or len(test_data) == 0:
        raise ValueError("‚ùå –û–¥–∏–Ω –∏–∑ –¥–∞—Ç–∞—Å–µ—Ç–æ–≤ –ø—É—Å—Ç!")
    
    # –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –¥–∞–Ω–Ω—ã—Ö
    logger.info(f"üìä –ò—Å—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ:")
    logger.info(f"   - Train: {len(train_data):,} –∑–∞–ø–∏—Å–µ–π, {len(train_data.columns)} –∫–æ–ª–æ–Ω–æ–∫")
    logger.info(f"   - Val: {len(val_data):,} –∑–∞–ø–∏—Å–µ–π, {len(val_data.columns)} –∫–æ–ª–æ–Ω–æ–∫") 
    logger.info(f"   - Test: {len(test_data):,} –∑–∞–ø–∏—Å–µ–π, {len(test_data.columns)} –∫–æ–ª–æ–Ω–æ–∫")
    
    if feature_cols:
        logger.info(f"   - –ü—Ä–∏–∑–Ω–∞–∫–æ–≤: {len(feature_cols)}")
    if target_cols:
        logger.info(f"   - –¶–µ–ª–µ–≤—ã—Ö –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö: {len(target_cols)}")
    
    # –ü–æ–ª—É—á–∞–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏ –∏–∑ –∫–æ–Ω—Ñ–∏–≥–∞
    normalize = config.get('data', {}).get('normalize', True)
    scaler_path = config.get('data', {}).get('scaler_path', 'models_saved/data_scaler.pkl')
    
    # –°–æ–∑–¥–∞–Ω–∏–µ –¥–∞—Ç–∞—Å–µ—Ç–æ–≤ —Å –ø—Ä–æ–≤–µ—Ä–∫–æ–π –æ—à–∏–±–æ–∫
    try:
        train_dataset = TradingDataset(
            data=train_data,
            config=config,
            context_window=context_window,
            prediction_window=pred_window,
            feature_cols=feature_cols,
            target_cols=target_cols,
            stride=1,  # –î–ª—è –æ–±—É—á–µ–Ω–∏—è –∏—Å–ø–æ–ª—å–∑—É–µ–º –≤—Å–µ –≤–æ–∑–º–æ–∂–Ω—ã–µ –æ–∫–Ω–∞
            normalize=normalize,
            scaler_path=scaler_path,
            fit_scaler=True  # –û–±—É—á–∞–µ–º scaler –Ω–∞ train –¥–∞–Ω–Ω—ã—Ö
        )
        
        val_dataset = TradingDataset(
            data=val_data,
            config=config,
            context_window=context_window,
            prediction_window=pred_window,
            feature_cols=feature_cols,
            target_cols=target_cols,
            stride=4,  # –î–ª—è –≤–∞–ª–∏–¥–∞—Ü–∏–∏ –º–æ–∂–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –±–æ–ª—å—à–∏–π stride
            normalize=normalize,
            scaler_path=scaler_path,
            fit_scaler=False  # –ò—Å–ø–æ–ª—å–∑—É–µ–º —É–∂–µ –æ–±—É—á–µ–Ω–Ω—ã–π scaler
        )
        
        test_dataset = TradingDataset(
            data=test_data,
            config=config,
            context_window=context_window,
            prediction_window=pred_window,
            feature_cols=feature_cols,
            target_cols=target_cols,
            stride=4,
            normalize=normalize,
            scaler_path=scaler_path,
            fit_scaler=False  # –ò—Å–ø–æ–ª—å–∑—É–µ–º —É–∂–µ –æ–±—É—á–µ–Ω–Ω—ã–π scaler
        )
    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ–∑–¥–∞–Ω–∏–∏ –¥–∞—Ç–∞—Å–µ—Ç–æ–≤: {e}")
        raise
    
    logger.info(f"–†–∞–∑–º–µ—Ä—ã –¥–∞—Ç–∞—Å–µ—Ç–æ–≤ –ø–æ—Å–ª–µ —Å–æ–∑–¥–∞–Ω–∏—è –æ–∫–æ–Ω:")
    logger.info(f"   - Train: {len(train_dataset):,} –æ–∫–æ–Ω (–∏–∑ {len(train_data):,} –∑–∞–ø–∏—Å–µ–π)")
    logger.info(f"   - Val: {len(val_dataset):,} –æ–∫–æ–Ω (–∏–∑ {len(val_data):,} –∑–∞–ø–∏—Å–µ–π, stride={val_dataset.stride})")
    logger.info(f"   - Test: {len(test_dataset):,} –æ–∫–æ–Ω (–∏–∑ {len(test_data):,} –∑–∞–ø–∏—Å–µ–π, stride={test_dataset.stride})")
    
    # –°–æ–∑–¥–∞–Ω–∏–µ DataLoader'–æ–≤
    train_loader = DataLoader(
        train_dataset,
        batch_size=batch_size,
        shuffle=True,
        num_workers=num_workers,
        pin_memory=True,
        drop_last=True,
        persistent_workers=persistent_workers,
        prefetch_factor=prefetch_factor if num_workers > 0 else None
    )
    
    val_loader = DataLoader(
        val_dataset,
        batch_size=batch_size,
        shuffle=False,
        num_workers=num_workers,
        pin_memory=True,
        persistent_workers=persistent_workers,
        prefetch_factor=prefetch_factor if num_workers > 0 else None
    )
    
    test_loader = DataLoader(
        test_dataset,
        batch_size=batch_size,
        shuffle=False,
        num_workers=num_workers,
        pin_memory=True,
        persistent_workers=persistent_workers,
        prefetch_factor=prefetch_factor if num_workers > 0 else None
    )
    
    return train_loader, val_loader, test_loader


def collate_trading_batch(batch):
    """–ö–∞—Å—Ç–æ–º–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏—è –±–∞—Ç—á–µ–π —Å –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π"""
    X_list, y_list, info_list = zip(*batch)
    
    # –°—Ç–µ–∫–∏—Ä–æ–≤–∞–Ω–∏–µ —Ç–µ–Ω–∑–æ—Ä–æ–≤
    X = torch.stack(X_list)
    y = torch.stack(y_list) if len(y_list[0]) > 0 else None
    
    # –û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏
    batch_info = {
        'symbols': [info['symbol'] for info in info_list],
        'last_closes': torch.tensor([info['last_close'] for info in info_list]),
        'atrs': torch.tensor([info['atr'] for info in info_list])
    }
    
    if 'price_data' in info_list[0]:
        batch_info['price_data'] = torch.stack([info['price_data'] for info in info_list])
    
    return X, y, batch_info