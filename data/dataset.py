"""
PyTorch Dataset –∫–ª–∞—Å—Å—ã –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ –∏ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ä—è–¥–æ–≤
"""

import torch
from torch.utils.data import Dataset, DataLoader
import pandas as pd
import numpy as np
from typing import Dict, List, Optional, Tuple
from pathlib import Path
import warnings
warnings.filterwarnings('ignore')
from sklearn.preprocessing import RobustScaler
import pickle
from tqdm import tqdm

from utils.logger import get_logger
from data.constants import TRADING_TARGET_VARIABLES, SERVICE_COLUMNS, get_feature_columns

class TimeSeriesDataset(Dataset):
    """Dataset –¥–ª—è –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ä—è–¥–æ–≤ –∫—Ä–∏–ø—Ç–æ–≤–∞–ª—é—Ç"""
    
    def __init__(self, 
                 data: pd.DataFrame,
                 context_window: int = 168,
                 prediction_window: int = 4,
                 feature_cols: List[str] = None,
                 target_cols: List[str] = None,
                 stride: int = 1,
                 normalize: bool = True,
                 scaler_path: Optional[str] = None,
                 fit_scaler: bool = False):
        """
        Args:
            data: DataFrame —Å –¥–∞–Ω–Ω—ã–º–∏
            context_window: —Ä–∞–∑–º–µ—Ä –≤—Ö–æ–¥–Ω–æ–≥–æ –æ–∫–Ω–∞
            prediction_window: —Ä–∞–∑–º–µ—Ä –æ–∫–Ω–∞ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
            feature_cols: —Å–ø–∏—Å–æ–∫ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
            target_cols: —Å–ø–∏—Å–æ–∫ —Ü–µ–ª–µ–≤—ã—Ö –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö
            stride: —à–∞–≥ –º–µ–∂–¥—É –æ–∫–Ω–∞–º–∏
        """
        self.logger = get_logger("TimeSeriesDataset")
        self.data = data.sort_values(['symbol', 'datetime']).reset_index(drop=True)
        self.context_window = context_window
        self.prediction_window = prediction_window
        self.target_window = prediction_window  # –î–æ–±–∞–≤–ª—è–µ–º target_window –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏
        self.stride = stride
        
        # –ö–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ v4.0 –∏ –∏—Ö –º–∞–ø–ø–∏–Ω–≥
        self.categorical_targets = {
            'direction_15m': {'UP': 0, 'DOWN': 1, 'FLAT': 2},
            'direction_1h': {'UP': 0, 'DOWN': 1, 'FLAT': 2},
            'direction_4h': {'UP': 0, 'DOWN': 1, 'FLAT': 2},
            'direction_12h': {'UP': 0, 'DOWN': 1, 'FLAT': 2}
        }
        
        # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –∏ —Ü–µ–ª–µ–≤—ã—Ö –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö
        if feature_cols is None:
            self.feature_cols = [col for col in data.columns 
                               if col not in ['id', 'symbol', 'datetime', 'timestamp', 'sector']
                               and not col.startswith(('target_', 'future_', 'optimal_'))]
        else:
            self.feature_cols = feature_cols
            
        if target_cols is None:
            # –û–±–Ω–æ–≤–ª–µ–Ω–Ω—ã–π —Å–ø–∏—Å–æ–∫ —Ü–µ–ª–µ–≤—ã—Ö –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –¥–ª—è —Ç–æ—Ä–≥–æ–≤–æ–π –º–æ–¥–µ–ª–∏ v4.0 (20 –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö)
            self.target_cols = [col for col in data.columns 
                              if col.startswith(('future_return_', 'direction_', 
                                               'long_will_reach_', 'short_will_reach_',
                                               'max_drawdown_', 'max_rally_'))]
        else:
            self.target_cols = target_cols
        
        # –°–æ–∑–¥–∞–Ω–∏–µ –∏–Ω–¥–µ–∫—Å–æ–≤ –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ –¥–æ—Å—Ç—É–ø–∞
        self._create_indices()
        
        # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö
        self.normalize = normalize
        self.scaler = None
        self.volume_based_cols = []
        self.price_based_cols = []
        self.ratio_cols = []
        
        if self.normalize:
            self._setup_normalization(scaler_path, fit_scaler)
        
        self.logger.info(f"Dataset —Å–æ–∑–¥–∞–Ω: {len(self)} –ø—Ä–∏–º–µ—Ä–æ–≤, "
                        f"{len(self.feature_cols)} –ø—Ä–∏–∑–Ω–∞–∫–æ–≤, "
                        f"{len(self.target_cols)} —Ü–µ–ª–µ–≤—ã—Ö –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö")
    
    def _create_indices(self):
        """–°–æ–∑–¥–∞–Ω–∏–µ –∏–Ω–¥–µ–∫—Å–æ–≤ –¥–ª—è –≤–∞–ª–∏–¥–Ω—ã—Ö –æ–∫–æ–Ω"""
        self.indices = []
        
        # –ü–æ–ª—É—á–∞–µ–º —É–Ω–∏–∫–∞–ª—å–Ω—ã–µ —Å–∏–º–≤–æ–ª—ã
        symbols = self.data['symbol'].unique()
        self.logger.info(f"üîÑ –°–æ–∑–¥–∞–Ω–∏–µ –∏–Ω–¥–µ–∫—Å–æ–≤ –¥–ª—è {len(symbols)} —Å–∏–º–≤–æ–ª–æ–≤...")
        
        # –ì—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∞ –ø–æ —Å–∏–º–≤–æ–ª–∞–º —Å –ø—Ä–æ–≥—Ä–µ—Å—Å-–±–∞—Ä–æ–º
        for symbol in tqdm(symbols, desc="–°–æ–∑–¥–∞–Ω–∏–µ –æ–∫–æ–Ω", leave=False):
            symbol_data = self.data[self.data['symbol'] == symbol]
            symbol_indices = symbol_data.index.tolist()
            
            # –°–æ–∑–¥–∞–Ω–∏–µ –æ–∫–æ–Ω —Å —É—á–µ—Ç–æ–º stride
            window_count = 0
            for i in range(0, len(symbol_indices) - self.context_window - self.prediction_window + 1, self.stride):
                # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–µ–ø—Ä–µ—Ä—ã–≤–Ω–æ—Å—Ç–∏ –≤—Ä–µ–º–µ–Ω–Ω–æ–≥–æ —Ä—è–¥–∞ (–æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è)
                start_idx = i
                end_idx = i + self.context_window + self.prediction_window
                
                # –ë—ã—Å—Ç—Ä–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ —á–µ—Ä–µ–∑ —Ä–∞–∑–Ω–æ—Å—Ç—å –ø–µ—Ä–≤–æ–≥–æ –∏ –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ –∏–Ω–¥–µ–∫—Å–∞
                if symbol_indices[end_idx - 1] - symbol_indices[start_idx] == end_idx - start_idx - 1:
                    self.indices.append({
                        'symbol': symbol,
                        'start_idx': symbol_indices[start_idx],
                        'context_end_idx': symbol_indices[start_idx + self.context_window - 1],
                        'target_end_idx': symbol_indices[end_idx - 1]
                    })
                    window_count += 1
            
            if window_count > 0:
                self.logger.debug(f"  {symbol}: —Å–æ–∑–¥–∞–Ω–æ {window_count} –æ–∫–æ–Ω")
        
        self.logger.info(f"‚úÖ –°–æ–∑–¥–∞–Ω–æ {len(self.indices)} –æ–∫–æ–Ω —Å stride={self.stride}")
    
    def _setup_normalization(self, scaler_path: Optional[str] = None, fit_scaler: bool = False):
        """–ù–∞—Å—Ç—Ä–æ–π–∫–∞ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏ –¥–∞–Ω–Ω—ã—Ö"""
        self.logger.info("üîß –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏ –¥–∞–Ω–Ω—ã—Ö...")
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–∏–ø—ã –∫–æ–ª–æ–Ω–æ–∫ –¥–ª—è —Ä–∞–∑–Ω–æ–π –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏
        self.volume_based_cols = [col for col in self.feature_cols if any(
            pattern in col.lower() for pattern in ['volume', 'turnover', 'obv', 'liquidity', 'cmf', 'mfi']
        )]
        
        self.price_based_cols = [col for col in self.feature_cols if any(
            pattern in col.lower() for pattern in ['price', 'vwap', 'high', 'low', 'open', 'close']
        )]
        
        self.ratio_cols = [col for col in self.feature_cols if any(
            pattern in col.lower() for pattern in ['ratio', 'rsi', 'stoch', 'bb_', 'pct', 'toxicity']
        )]
        
        # –ó–∞–≥—Ä—É–∑–∫–∞ –∏–ª–∏ —Å–æ–∑–¥–∞–Ω–∏–µ scaler
        if scaler_path and Path(scaler_path).exists() and not fit_scaler:
            self.logger.info(f"üì• –ó–∞–≥—Ä—É–∑–∫–∞ scaler –∏–∑ {scaler_path}")
            with open(scaler_path, 'rb') as f:
                self.scaler = pickle.load(f)
        else:
            self.logger.info("üî® –°–æ–∑–¥–∞–Ω–∏–µ –Ω–æ–≤–æ–≥–æ scaler...")
            self.scaler = RobustScaler(quantile_range=(5, 95))
            
            if fit_scaler:
                # –§–∏—Ç–∏–º scaler –Ω–∞ —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
                self.logger.info("üìä –û–±—É—á–µ–Ω–∏–µ scaler –Ω–∞ –¥–∞–Ω–Ω—ã—Ö...")
                
                # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è scaler
                scaler_data = self.data[self.feature_cols].copy()
                
                # –ü—Ä–∏–º–µ–Ω—è–µ–º log-—Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏—é –∫ –æ–±—ä–µ–º–Ω—ã–º –∫–æ–ª–æ–Ω–∫–∞–º
                for col in self.volume_based_cols:
                    if col in scaler_data.columns:
                        # –ò–°–ü–†–ê–í–õ–ï–ù–û: –ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è –≤ —á–∏—Å–ª–æ–≤–æ–π —Ç–∏–ø –ø–µ—Ä–µ–¥ log-—Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏–µ–π
                        scaler_data[col] = pd.to_numeric(scaler_data[col], errors='coerce')
                        # Log —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏—è —Å –∑–∞—â–∏—Ç–æ–π –æ—Ç –æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π
                        scaler_data[col] = np.log1p(np.clip(scaler_data[col], 0, None))
                
                # –ö–ª–∏–ø–ø–∏–Ω–≥ —ç–∫—Å—Ç—Ä–µ–º–∞–ª—å–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π –ø–µ—Ä–µ–¥ –æ–±—É—á–µ–Ω–∏–µ–º scaler
                for col in scaler_data.columns:
                    if col not in self.ratio_cols:  # –ù–µ –∫–ª–∏–ø–ø–∏–º ratio –∫–æ–ª–æ–Ω–∫–∏
                        # –ò–°–ü–†–ê–í–õ–ï–ù–û: –ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è –≤ —á–∏—Å–ª–æ–≤–æ–π —Ç–∏–ø –ø–µ—Ä–µ–¥ –∫–≤–∞–Ω—Ç–∏–ª—è–º–∏
                        scaler_data[col] = pd.to_numeric(scaler_data[col], errors='coerce')
                        
                        # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –∫–æ–ª–æ–Ω–∫–∏ —Å —Ç–æ–ª—å–∫–æ NaN –∑–Ω–∞—á–µ–Ω–∏—è–º–∏
                        if scaler_data[col].notna().sum() == 0:
                            self.logger.warning(f"–ö–æ–ª–æ–Ω–∫–∞ '{col}' —Å–æ–¥–µ—Ä–∂–∏—Ç —Ç–æ–ª—å–∫–æ NaN –∑–Ω–∞—á–µ–Ω–∏—è, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º")
                            continue
                            
                        q99 = scaler_data[col].quantile(0.99)
                        q01 = scaler_data[col].quantile(0.01)
                        scaler_data[col] = np.clip(scaler_data[col], q01, q99)
                
                # –û–±—É—á–∞–µ–º scaler
                self.scaler.fit(scaler_data.values)
                
                # –°–æ—Ö—Ä–∞–Ω—è–µ–º scaler –µ—Å–ª–∏ —É–∫–∞–∑–∞–Ω –ø—É—Ç—å
                if scaler_path:
                    Path(scaler_path).parent.mkdir(parents=True, exist_ok=True)
                    with open(scaler_path, 'wb') as f:
                        pickle.dump(self.scaler, f)
                    self.logger.info(f"üíæ Scaler —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤ {scaler_path}")
        
        self.logger.info(f"‚úÖ –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –Ω–∞—Å—Ç—Ä–æ–µ–Ω–∞: {len(self.volume_based_cols)} –æ–±—ä–µ–º–Ω—ã—Ö, "
                        f"{len(self.price_based_cols)} —Ü–µ–Ω–æ–≤—ã—Ö, {len(self.ratio_cols)} ratio –∫–æ–ª–æ–Ω–æ–∫")
    
    def __len__(self):
        return len(self.indices)
    
    def __getitem__(self, idx):
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –æ–¥–Ω–æ–≥–æ –ø—Ä–∏–º–µ—Ä–∞"""
        index_info = self.indices[idx]
        
        # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
        context_start = index_info['start_idx']
        context_end = index_info['context_end_idx'] + 1
        context_data = self.data.iloc[context_start:context_end]
        
        # –ò–°–ü–†–ê–í–õ–ï–ù–û: –¶–µ–ª–µ–≤—ã–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –±–µ—Ä–µ–º –∏–∑ –ø–æ—Å–ª–µ–¥–Ω–µ–π —Å—Ç—Ä–æ–∫–∏ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
        # –∞ –Ω–µ –∏–∑ –±—É–¥—É—â–∏—Ö —Å—Ç—Ä–æ–∫!
        
        # –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –≤ —Ç–µ–Ω–∑–æ—Ä—ã —Å –æ–±—Ä–∞–±–æ—Ç–∫–æ–π object —Ç–∏–ø–æ–≤
        feature_data = context_data[self.feature_cols]
        
        # –ü–û–õ–ù–ê–Ø –≤–µ—Ä—Å–∏—è: –Ω–∞–¥—ë–∂–Ω–∞—è –∫–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è –≤ —á–∏—Å–ª–æ–≤—ã–µ —Ç–∏–ø—ã
        feature_data = feature_data.copy()
        
        # –ü—Ä–∏–º–µ–Ω—è–µ–º pd.to_numeric –∫–æ –≤—Å–µ–º –∫–æ–ª–æ–Ω–∫–∞–º –¥–ª—è –Ω–∞–¥—ë–∂–Ω–æ—Å—Ç–∏
        for col in feature_data.columns:
            try:
                # –°–Ω–∞—á–∞–ª–∞ –ø—Ä–æ–≤–µ—Ä—è–µ–º —Ç–∏–ø
                if pd.api.types.is_object_dtype(feature_data[col]) or pd.api.types.is_categorical_dtype(feature_data[col]):
                    if pd.api.types.is_categorical_dtype(feature_data[col]):
                        # –ö–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –∫–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ –∫–æ–¥—ã
                        feature_data[col] = feature_data[col].cat.codes.astype(np.float32)
                    else:
                        # Object —Ç–∏–ø—ã –∫–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º —á–µ—Ä–µ–∑ pd.to_numeric
                        feature_data[col] = pd.to_numeric(feature_data[col], errors='coerce').fillna(0.0).astype(np.float32)
                else:
                    # –£–∂–µ —á–∏—Å–ª–æ–≤—ã–µ —Ç–∏–ø—ã –ø—Ä–æ—Å—Ç–æ –ø—Ä–∏–≤–æ–¥–∏–º –∫ float32
                    feature_data[col] = feature_data[col].astype(np.float32)
            except Exception as e:
                # –ï—Å–ª–∏ —á—Ç–æ-—Ç–æ –ø–æ—à–ª–æ –Ω–µ —Ç–∞–∫, –∑–∞–ø–æ–ª–Ω—è–µ–º –Ω—É–ª—è–º–∏
                feature_data[col] = np.zeros(len(feature_data), dtype=np.float32)
        
        # –ü–æ–ª—É—á–∞–µ–º –º–∞—Å—Å–∏–≤ –∏ —Ñ–∏–Ω–∞–ª—å–Ω–∞—è –æ—á–∏—Å—Ç–∫–∞ –æ—Ç inf/nan
        feature_values = feature_data.values
        feature_values = np.nan_to_num(feature_values, nan=0.0, posinf=0.0, neginf=0.0)
        
        # –ü—Ä–∏–º–µ–Ω—è–µ–º –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—é –µ—Å–ª–∏ –≤–∫–ª—é—á–µ–Ω–∞
        if self.normalize and self.scaler is not None:
            # –°–æ–∑–¥–∞–µ–º –∫–æ–ø–∏—é –¥–ª—è –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏
            norm_values = feature_values.copy()
            
            # –ü—Ä–∏–º–µ–Ω—è–µ–º log-—Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏—é –∫ –æ–±—ä–µ–º–Ω—ã–º –∫–æ–ª–æ–Ω–∫–∞–º
            for i, col in enumerate(self.feature_cols):
                if col in self.volume_based_cols:
                    # Log —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏—è —Å –∑–∞—â–∏—Ç–æ–π –æ—Ç –æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π
                    norm_values[:, i] = np.log1p(np.clip(norm_values[:, i], 0, None))
            
            # –ö–ª–∏–ø–ø–∏–Ω–≥ —ç–∫—Å—Ç—Ä–µ–º–∞–ª—å–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π
            for i, col in enumerate(self.feature_cols):
                if col not in self.ratio_cols:  # –ù–µ –∫–ª–∏–ø–ø–∏–º ratio –∫–æ–ª–æ–Ω–∫–∏
                    # –ò—Å–ø–æ–ª—å–∑—É–µ–º –∫–≤–∞–Ω—Ç–∏–ª–∏ –∏–∑ –æ–±—É—á–∞—é—â–∏—Ö –¥–∞–Ω–Ω—ã—Ö
                    q99 = np.percentile(norm_values[:, i], 99)
                    q01 = np.percentile(norm_values[:, i], 1)
                    norm_values[:, i] = np.clip(norm_values[:, i], q01, q99)
            
            # –ü—Ä–∏–º–µ–Ω—è–µ–º RobustScaler
            try:
                norm_values = self.scaler.transform(norm_values)
            except Exception as e:
                self.logger.warning(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏: {e}")
                # Fallback –∫ –∏—Å—Ö–æ–¥–Ω—ã–º –∑–Ω–∞—á–µ–Ω–∏—è–º
                norm_values = feature_values
            
            # –§–∏–Ω–∞–ª—å–Ω—ã–π –∫–ª–∏–ø–ø–∏–Ω–≥ –ø–æ—Å–ª–µ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏
            norm_values = np.clip(norm_values, -10, 10)
            
            X = torch.FloatTensor(norm_values)
        else:
            X = torch.FloatTensor(feature_values)
        
        # –û–±—Ä–∞–±–æ—Ç–∫–∞ —Ü–µ–ª–µ–≤—ã—Ö –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö
        if len(self.target_cols) > 0:
            # –ë–µ—Ä–µ–º —Ü–µ–ª–µ–≤—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è –∏–∑ –ü–û–°–õ–ï–î–ù–ï–ô —Å—Ç—Ä–æ–∫–∏ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
            # –≠—Ç–æ –∑–Ω–∞—á–µ–Ω–∏—è future_return_X –∫–æ—Ç–æ—Ä—ã–µ —É–∂–µ —Å–æ–¥–µ—Ä–∂–∞—Ç –±—É–¥—É—â—É—é –¥–æ—Ö–æ–¥–Ω–æ—Å—Ç—å
            y_data = context_data.iloc[-1][self.target_cols]
            
            # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –≤ —á–∏—Å–ª–æ–≤—ã–µ
            y_values = []
            for col in self.target_cols:
                value = y_data[col]
                
                # –û–±—Ä–∞–±–æ—Ç–∫–∞ –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω–æ–π –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π best_direction
                if col == 'best_direction':
                    if pd.api.types.is_categorical_dtype(value) or isinstance(value, str):
                        # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ —á–∏—Å–ª–æ–≤–æ–π –∫–æ–¥: LONG=0, SHORT=1, NEUTRAL=2
                        if value == 'LONG':
                            y_values.append(0)
                        elif value == 'SHORT':
                            y_values.append(1)
                        elif value == 'NEUTRAL':
                            y_values.append(2)
                        else:
                            y_values.append(2)  # fallback –∫ NEUTRAL
                    else:
                        y_values.append(float(value))
                else:
                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ —ç—Ç–æ –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω–æ–π –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π v4.0
                    if col in self.categorical_targets:
                        # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ –≤ —á–∏—Å–ª–æ–≤–æ–µ
                        mapping = self.categorical_targets[col]
                        # –ë–µ–∑–æ–ø–∞—Å–Ω–æ–µ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ —Å –¥–µ—Ñ–æ–ª—Ç–Ω—ã–º –∑–Ω–∞—á–µ–Ω–∏–µ–º
                        numeric_value = mapping.get(str(value), 2)  # 2 = FLAT/HOLD –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
                        y_values.append(float(numeric_value))
                    else:
                        # –û–±—ã—á–Ω—ã–µ —á–∏—Å–ª–æ–≤—ã–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ
                        try:
                            y_values.append(float(value))
                        except (ValueError, TypeError):
                            # –ï—Å–ª–∏ –Ω–µ —É–¥–∞–µ—Ç—Å—è –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞—Ç—å, –∏—Å–ø–æ–ª—å–∑—É–µ–º 0
                            self.logger.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞—Ç—å –∑–Ω–∞—á–µ–Ω–∏–µ '{value}' –¥–ª—è –∫–æ–ª–æ–Ω–∫–∏ '{col}', –∏—Å–ø–æ–ª—å–∑—É–µ–º 0")
                            y_values.append(0.0)
            
            y_values = np.array(y_values, dtype=np.float32)
            
            # –ï—Å–ª–∏ –º–æ–¥–µ–ª—å –æ–∂–∏–¥–∞–µ—Ç –Ω–µ—Å–∫–æ–ª—å–∫–æ –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —à–∞–≥–æ–≤, –¥—É–±–ª–∏—Ä—É–µ–º –∑–Ω–∞—á–µ–Ω–∏–µ
            if self.target_window > 1:
                y = torch.FloatTensor([y_values] * self.target_window)
            else:
                y = torch.FloatTensor([y_values])
        else:
            y = torch.FloatTensor([])
        
        # –î–∞–Ω–Ω—ã–µ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ (–æ—Å—Ç–∞–≤–ª—è–µ–º –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏)
        target_start = index_info['context_end_idx'] + 1
        target_end = index_info['target_end_idx'] + 1
        target_data = self.data.iloc[target_start:target_end]
        
        # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è
        info = {
            'symbol': index_info['symbol'],
            'context_start_time': str(context_data.iloc[0]['datetime']),
            'context_end_time': str(context_data.iloc[-1]['datetime']),
            'target_start_time': str(target_data.iloc[0]['datetime']) if len(target_data) > 0 else None,
            'target_end_time': str(target_data.iloc[-1]['datetime']) if len(target_data) > 0 else None
        }
        
        return X, y, info


class TradingDataset(TimeSeriesDataset):
    """–°–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π Dataset –¥–ª—è —Ç–æ—Ä–≥–æ–≤—ã—Ö —Å–∏–≥–Ω–∞–ª–æ–≤"""
    
    def __init__(self, 
                 data: pd.DataFrame,
                 config: Dict = None,
                 context_window: int = 168,
                 prediction_window: int = 4,
                 feature_cols: List[str] = None,
                 target_cols: List[str] = None,
                 include_price_data: bool = True,
                 **kwargs):
        
        # –ö–†–ò–¢–ò–ß–ù–û–ï –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï: –ü—Ä–∞–≤–∏–ª—å–Ω–∞—è —Ä–∞–±–æ—Ç–∞ —Å —Ç–æ—Ä–≥–æ–≤—ã–º–∏ —Ü–µ–ª–µ–≤—ã–º–∏ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–º–∏
        if target_cols is not None:
            # –ï—Å–ª–∏ target_cols –ø–µ—Ä–µ–¥–∞–Ω—ã —è–≤–Ω–æ, –∏—Å–ø–æ–ª—å–∑—É–µ–º –∏—Ö
            trading_targets = target_cols
            print(f"‚úÖ –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è {len(trading_targets)} —Ü–µ–ª–µ–≤—ã—Ö –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –∏–∑ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤")
        elif target_cols is None and config is not None:
            # –ü–æ–ª—É—á–∞–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é –º–æ–¥–µ–ª–∏
            model_config = config.get('model', {})
            task_type = model_config.get('task_type', 'regression')
            
            if task_type == 'trading':
                # –î–ª—è —Ç–æ—Ä–≥–æ–≤–æ–π –º–æ–¥–µ–ª–∏ –∏—Å–ø–æ–ª—å–∑—É–µ–º —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ —Ü–µ–ª–µ–≤—ã–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ
                trading_target_variables = model_config.get('target_variables', [])
                
                if trading_target_variables:
                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–∞–∫–∏–µ —Ü–µ–ª–µ–≤—ã–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –µ—Å—Ç—å –≤ –¥–∞–Ω–Ω—ã—Ö
                    available_trading_targets = [var for var in trading_target_variables if var in data.columns]
                    
                    if available_trading_targets:
                        trading_targets = available_trading_targets
                        print(f"‚úÖ –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —Ç–æ—Ä–≥–æ–≤–∞—è –º–æ–¥–µ–ª—å —Å {len(trading_targets)} —Ü–µ–ª–µ–≤—ã–º–∏ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–º–∏")
                        print(f"   –î–æ—Å—Ç—É–ø–Ω—ã–µ —Ü–µ–ª–∏: {trading_targets}")
                    else:
                        print("‚ùå –¢–æ—Ä–≥–æ–≤—ã–µ —Ü–µ–ª–µ–≤—ã–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –∏–∑ –∫–æ–Ω—Ñ–∏–≥–∞ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã –≤ –¥–∞–Ω–Ω—ã—Ö!")
                        # Fallback –∫ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–º—É –ø–æ–∏—Å–∫—É —Ç–æ—Ä–≥–æ–≤—ã—Ö —Ü–µ–ª–µ–π
                        auto_trading_targets = [col for col in data.columns 
                                              if col.startswith(('long_tp', 'short_tp', 'long_sl', 'short_sl', 'best_direction'))]
                        
                        if auto_trading_targets:
                            trading_targets = auto_trading_targets[:11]  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –æ—Å–Ω–æ–≤–Ω—ã–º–∏
                            print(f"üîß –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –Ω–∞–π–¥–µ–Ω–æ {len(trading_targets)} —Ç–æ—Ä–≥–æ–≤—ã—Ö —Ü–µ–ª–µ–π")
                        else:
                            raise ValueError("‚ùå –¢–æ—Ä–≥–æ–≤—ã–µ —Ü–µ–ª–µ–≤—ã–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã!")
                else:
                    # –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–∏—Å–∫ —Ç–æ—Ä–≥–æ–≤—ã—Ö —Ü–µ–ª–µ–≤—ã—Ö –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö
                    auto_trading_targets = [col for col in data.columns 
                                          if col.startswith(('long_tp', 'short_tp', 'long_sl', 'short_sl', 'best_direction'))]
                    
                    if auto_trading_targets:
                        # –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç–Ω—ã–µ —Ç–æ—Ä–≥–æ–≤—ã–µ —Ü–µ–ª–∏
                        priority_targets = [
                            'long_tp1_reached', 'long_tp2_reached', 'long_tp3_reached', 'long_sl_reached',
                            'short_tp1_reached', 'short_tp2_reached', 'short_tp3_reached', 'short_sl_reached',
                            'best_direction'
                        ]
                        trading_targets = [t for t in priority_targets if t in data.columns]
                        
                        if len(trading_targets) < 5:  # –ú–∏–Ω–∏–º—É–º –¥–ª—è —Ç–æ—Ä–≥–æ–≤–æ–π –º–æ–¥–µ–ª–∏
                            trading_targets = auto_trading_targets[:11]  # –ë–µ—Ä–µ–º –ø–µ—Ä–≤—ã–µ 11
                        
                        print(f"üîß –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –≤—ã–±—Ä–∞–Ω–æ {len(trading_targets)} —Ç–æ—Ä–≥–æ–≤—ã—Ö —Ü–µ–ª–µ–π")
                    else:
                        raise ValueError("‚ùå –¢–æ—Ä–≥–æ–≤—ã–µ —Ü–µ–ª–µ–≤—ã–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã –≤ –¥–∞–Ω–Ω—ã—Ö!")
            else:
                # –î–ª—è —Ä–µ–≥—Ä–µ—Å—Å–∏–∏/–∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ –∏—Å–ø–æ–ª—å–∑—É–µ–º —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—É—é –ª–æ–≥–∏–∫—É
                target_variable = model_config.get('target_variable', 'future_return_4')
                
                # –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï: –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ —Ü–µ–ª–µ–≤–∞—è –ø–µ—Ä–µ–º–µ–Ω–Ω–∞—è —Å—É—â–µ—Å—Ç–≤—É–µ—Ç –≤ –¥–∞–Ω–Ω—ã—Ö
                if target_variable in data.columns:
                    trading_targets = [target_variable]
                    print(f"‚úÖ –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —Ü–µ–ª–µ–≤–∞—è –ø–µ—Ä–µ–º–µ–Ω–Ω–∞—è: {target_variable} (—Ç–∏–ø: {task_type})")
                else:
                    # Fallback –∫ –¥–æ—Å—Ç—É–ø–Ω—ã–º —Ü–µ–ª–µ–≤—ã–º –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–º
                    available_targets = [col for col in data.columns 
                                   if col.startswith(('target_', 'future_return_'))]
                    
                    if available_targets:
                        # –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç: future_return_4 > future_return_3 > future_return_2 > future_return_1
                        preferred_targets = ['future_return_4', 'future_return_3', 'future_return_2', 'future_return_1']
                        trading_targets = None
                        for pref_target in preferred_targets:
                            if pref_target in available_targets:
                                trading_targets = [pref_target]
                                break
                        
                        if trading_targets is None:
                            trading_targets = [available_targets[0]]  # –ë–µ—Ä–µ–º –ø–µ—Ä–≤—É—é –¥–æ—Å—Ç—É–ø–Ω—É—é
                        
                        print(f"‚ö†Ô∏è  –¶–µ–ª–µ–≤–∞—è –ø–µ—Ä–µ–º–µ–Ω–Ω–∞—è {target_variable} –Ω–µ –Ω–∞–π–¥–µ–Ω–∞. –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è: {trading_targets[0]}")
                    else:
                        # –ö–†–ò–¢–ò–ß–ù–û–ï –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï: –ù–ï —Å–æ–∑–¥–∞–µ–º –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –∑–∞–Ω–æ–≤–æ –ø—Ä–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–∏ –∫—ç—à–∞!
                        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –µ—Å—Ç—å –ª–∏ ANY —Ç–æ—Ä–≥–æ–≤—ã–µ —Ü–µ–ª–µ–≤—ã–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –≤ –∫—ç—à–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
                        any_trading_targets = [col for col in data.columns 
                                             if any(pattern in col for pattern in ['_hit', '_reached', '_tp', '_sl', 'best_direction', 'future_return'])]
                        
                        if any_trading_targets:
                            # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ —Ç–æ—Ä–≥–æ–≤—ã–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –∏–∑ –∫—ç—à–∞
                            trading_targets = any_trading_targets[:36]  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–æ 36 –¥–ª—è –º–æ–¥–µ–ª–∏
                            print(f"‚úÖ –ù–∞–π–¥–µ–Ω–æ {len(trading_targets)} —Ç–æ—Ä–≥–æ–≤—ã—Ö –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –≤ –∫—ç—à–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö")
                            print(f"   –ò—Å–ø–æ–ª—å–∑—É–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ —Ü–µ–ª–∏ –∏–∑ –∫—ç—à–∞: {trading_targets[:5]}...")
                        else:
                            raise ValueError("‚ùå –ö—ç—à–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –Ω–µ —Å–æ–¥–µ—Ä–∂–∞—Ç —Ç–æ—Ä–≥–æ–≤—ã—Ö —Ü–µ–ª–µ–≤—ã—Ö –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö! –ü–µ—Ä–µ—Å–æ–∑–¥–∞–π—Ç–µ –∫—ç—à: python prepare_trading_data.py --force-recreate")
                    
        elif target_cols is None:
            # –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –Ω–∞—Ö–æ–¥–∏–º —Ü–µ–ª–µ–≤—ã–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ (—Å—Ç–∞—Ä–æ–µ –ø–æ–≤–µ–¥–µ–Ω–∏–µ)
            available_targets = [col for col in data.columns 
                               if col.startswith(('target_', 'future_return_'))]
            
            if available_targets:
                # –ò–°–ü–†–ê–í–õ–ï–ù–û: –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ç–æ–ª—å–∫–æ future_return_4 –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏ —Å pred_len=4
                if 'future_return_4' in available_targets:
                    trading_targets = ['future_return_4']
                else:
                    # Fallback –∫ –ª—é–±–æ–º—É future_return
                    future_returns = [col for col in available_targets if 'future_return_' in col]
                    trading_targets = future_returns[:1] if future_returns else [available_targets[0]]
            else:
                raise ValueError("‚ùå –¶–µ–ª–µ–≤—ã–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã –≤ –¥–∞–Ω–Ω—ã—Ö –∏ config –Ω–µ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω!")
        else:
            trading_targets = target_cols
        
        # –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï: –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏ —Ü–µ–ª–µ–π —Å –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–µ–π –º–æ–¥–µ–ª–∏
        if config is not None:
            model_config = config.get('model', {})
            task_type = model_config.get('task_type', 'regression')
            expected_output_size = model_config.get('output_size', 1)
            
            # –î–ª—è —Ç–æ—Ä–≥–æ–≤–æ–π –º–æ–¥–µ–ª–∏ –ù–ï –æ–±—Ä–µ–∑–∞–µ–º —Ü–µ–ª–µ–≤—ã–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ
            if task_type != 'trading' and len(trading_targets) != expected_output_size:
                print(f"‚ö†Ô∏è  –ù–µ—Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–µ–π: target_cols={len(trading_targets)}, output_size={expected_output_size}")
                
                # –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–µ: –±–µ—Ä–µ–º —Ç–æ–ª—å–∫–æ –Ω—É–∂–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ü–µ–ª–µ–π
                if len(trading_targets) > expected_output_size:
                    trading_targets = trading_targets[:expected_output_size]
                    print(f"üîß –ò—Å–ø—Ä–∞–≤–ª–µ–Ω–æ: –∏—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è –ø–µ—Ä–≤—ã–µ {expected_output_size} —Ü–µ–ª–µ–π: {trading_targets}")
            elif task_type == 'trading':
                print(f"‚úÖ –¢–æ—Ä–≥–æ–≤–∞—è –º–æ–¥–µ–ª—å: –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è {len(trading_targets)} —Ü–µ–ª–µ–≤—ã—Ö –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö")
        
        # –í–ê–ñ–ù–û: –ù–ï –ø–µ—Ä–µ—Å–æ–∑–¥–∞–µ–º –¥–∞–Ω–Ω—ã–µ –µ—Å–ª–∏ –æ–Ω–∏ —É–∂–µ –µ—Å—Ç—å –≤ –∫—ç—à–µ!
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –≤—Å–µ –ª–∏ –Ω—É–∂–Ω—ã–µ —Ü–µ–ª–µ–≤—ã–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –ø—Ä–∏—Å—É—Ç—Å—Ç–≤—É—é—Ç
        missing_targets = [target for target in trading_targets if target not in data.columns]
        
        if missing_targets:
            print(f"‚ö†Ô∏è  –û—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç —Ü–µ–ª–µ–≤—ã–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ: {missing_targets}")
            print("‚ùå –ü–µ—Ä–µ—Å–æ–∑–¥–∞–π—Ç–µ –∫—ç—à —Å –ø—Ä–∞–≤–∏–ª—å–Ω—ã–º–∏ —Ü–µ–ª–µ–≤—ã–º–∏ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–º–∏:")
            print("   python prepare_trading_data.py --force-recreate")
            raise ValueError(f"–û—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç —Ü–µ–ª–µ–≤—ã–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –≤ –∫—ç—à–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö: {missing_targets}")
        else:
            print(f"‚úÖ –í—Å–µ {len(trading_targets)} —Ü–µ–ª–µ–≤—ã—Ö –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –Ω–∞–π–¥–µ–Ω—ã –≤ –∫—ç—à–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö")
        
        # –î–æ–±–∞–≤–ª—è–µ–º target_window –µ—Å–ª–∏ –æ–Ω –ø–µ—Ä–µ–¥–∞–Ω –≤ kwargs
        if 'target_window' in kwargs:
            target_window = kwargs.pop('target_window')
        else:
            target_window = prediction_window
            
        super().__init__(
            data=data,
            context_window=context_window,
            prediction_window=target_window,
            feature_cols=feature_cols,
            target_cols=trading_targets,
            **kwargs
        )
        
        self.include_price_data = include_price_data
        
        # –ò–Ω–¥–µ–∫—Å—ã —Ü–µ–Ω–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö
        if include_price_data:
            self.price_cols = ['open', 'high', 'low', 'close', 'volume']
    
    def __getitem__(self, idx):
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –ø—Ä–∏–º–µ—Ä–∞ —Å —Ç–æ—Ä–≥–æ–≤–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π"""
        X, y, info = super().__getitem__(idx)
        
        index_info = self.indices[idx]
        
        # –î–æ–±–∞–≤–ª—è–µ–º —Ü–µ–Ω–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
        if self.include_price_data:
            context_start = index_info['start_idx']
            context_end = index_info['context_end_idx'] + 1
            price_data = self.data.iloc[context_start:context_end][self.price_cols]
            info['price_data'] = torch.FloatTensor(price_data.values)
        
        # –î–æ–±–∞–≤–ª—è–µ–º –ø–æ—Å–ª–µ–¥–Ω—é—é —Ü–µ–Ω—É –¥–ª—è —Ä–∞—Å—á–µ—Ç–∞ —É—Ä–æ–≤–Ω–µ–π
        last_idx = index_info['context_end_idx']
        info['last_close'] = self.data.iloc[last_idx]['close']
        info['atr'] = self.data.iloc[last_idx].get('atr', info['last_close'] * 0.02)
        
        return X, y, info


class MultiSymbolDataset(Dataset):
    """Dataset –¥–ª—è –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ–π —Ä–∞–±–æ—Ç—ã —Å –Ω–µ—Å–∫–æ–ª—å–∫–∏–º–∏ —Å–∏–º–≤–æ–ª–∞–º–∏"""
    
    def __init__(self,
                 data: pd.DataFrame,
                 symbols: List[str],
                 context_window: int = 168,
                 prediction_window: int = 4,
                 feature_cols: List[str] = None,
                 synchronize: bool = True):
        """
        Args:
            data: DataFrame —Å–æ –≤—Å–µ–º–∏ –¥–∞–Ω–Ω—ã–º–∏
            symbols: —Å–ø–∏—Å–æ–∫ —Å–∏–º–≤–æ–ª–æ–≤ –¥–ª—è –±–∞—Ç—á–∞
            context_window: —Ä–∞–∑–º–µ—Ä –≤—Ö–æ–¥–Ω–æ–≥–æ –æ–∫–Ω–∞
            prediction_window: —Ä–∞–∑–º–µ—Ä –æ–∫–Ω–∞ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
            feature_cols: —Å–ø–∏—Å–æ–∫ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
            synchronize: —Å–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∏—Ä–æ–≤–∞—Ç—å –≤—Ä–µ–º–µ–Ω–Ω—ã–µ –º–µ—Ç–∫–∏ –º–µ–∂–¥—É —Å–∏–º–≤–æ–ª–∞–º–∏
        """
        self.logger = get_logger("MultiSymbolDataset")
        self.symbols = symbols
        self.context_window = context_window
        self.prediction_window = prediction_window
        self.synchronize = synchronize
        
        # –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö –ø–æ —Å–∏–º–≤–æ–ª–∞–º
        self.data = data[data['symbol'].isin(symbols)].copy()
        
        # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
        if feature_cols is None:
            self.feature_cols = [col for col in data.columns 
                               if col not in ['id', 'symbol', 'datetime', 'timestamp', 'sector']
                               and not col.startswith(('target_', 'future_', 'optimal_'))]
        else:
            self.feature_cols = feature_cols
        
        # –°–æ–∑–¥–∞–Ω–∏–µ —Å–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –∏–Ω–¥–µ–∫—Å–æ–≤
        if synchronize:
            self._create_synchronized_indices()
        else:
            self._create_independent_indices()
        
        self.logger.info(f"MultiSymbolDataset —Å–æ–∑–¥–∞–Ω: {len(self)} –±–∞—Ç—á–µ–π, "
                        f"{len(symbols)} —Å–∏–º–≤–æ–ª–æ–≤")
    
    def _create_synchronized_indices(self):
        """–°–æ–∑–¥–∞–Ω–∏–µ —Å–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –ø–æ –≤—Ä–µ–º–µ–Ω–∏ –∏–Ω–¥–µ–∫—Å–æ–≤"""
        self.indices = []
        
        # –ù–∞—Ö–æ–¥–∏–º –æ–±—â–∏–µ –≤—Ä–µ–º–µ–Ω–Ω—ã–µ –º–µ—Ç–∫–∏
        common_times = None
        for symbol in self.symbols:
            symbol_times = set(self.data[self.data['symbol'] == symbol]['datetime'].values)
            if common_times is None:
                common_times = symbol_times
            else:
                common_times = common_times.intersection(symbol_times)
        
        common_times = sorted(list(common_times))
        
        # –°–æ–∑–¥–∞–µ–º –æ–∫–Ω–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ –æ–±—â–∏—Ö –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –º–µ—Ç–æ–∫
        window_size = self.context_window + self.prediction_window
        for i in range(len(common_times) - window_size + 1):
            window_times = common_times[i:i + window_size]
            self.indices.append({
                'start_time': window_times[0],
                'context_end_time': window_times[self.context_window - 1],
                'end_time': window_times[-1]
            })
    
    def _create_independent_indices(self):
        """–°–æ–∑–¥–∞–Ω–∏–µ –Ω–µ–∑–∞–≤–∏—Å–∏–º—ã—Ö –∏–Ω–¥–µ–∫—Å–æ–≤ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Å–∏–º–≤–æ–ª–∞"""
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º TimeSeriesDataset –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Å–∏–º–≤–æ–ª–∞
        self.symbol_datasets = {}
        
        for symbol in self.symbols:
            symbol_data = self.data[self.data['symbol'] == symbol]
            self.symbol_datasets[symbol] = TimeSeriesDataset(
                data=symbol_data,
                context_window=self.context_window,
                prediction_window=self.prediction_window,
                feature_cols=self.feature_cols
            )
        
        # –ò–Ω–¥–µ–∫—Å—ã - —ç—Ç–æ –º–∏–Ω–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–∏–º–µ—Ä–æ–≤ —Å—Ä–µ–¥–∏ –≤—Å–µ—Ö —Å–∏–º–≤–æ–ª–æ–≤
        min_length = min(len(ds) for ds in self.symbol_datasets.values())
        self.indices = list(range(min_length))
    
    def __len__(self):
        return len(self.indices)
    
    def __getitem__(self, idx):
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –±–∞—Ç—á–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –≤—Å–µ—Ö —Å–∏–º–≤–æ–ª–æ–≤"""
        batch_X = []
        batch_y = []
        batch_info = []
        
        if self.synchronize:
            # –°–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –≤—ã–±–æ—Ä–∫–∞
            index_info = self.indices[idx]
            
            for symbol in self.symbols:
                # –ü–æ–ª—É—á–∞–µ–º –¥–∞–Ω–Ω—ã–µ –¥–ª—è –≤—Ä–µ–º–µ–Ω–Ω–æ–≥–æ –æ–∫–Ω–∞
                symbol_data = self.data[
                    (self.data['symbol'] == symbol) &
                    (self.data['datetime'] >= index_info['start_time']) &
                    (self.data['datetime'] <= index_info['end_time'])
                ].sort_values('datetime')
                
                if len(symbol_data) == self.context_window + self.prediction_window:
                    # –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –Ω–∞ –∫–æ–Ω—Ç–µ–∫—Å—Ç –∏ —Ü–µ–ª—å
                    context_data = symbol_data.iloc[:self.context_window]
                    target_data = symbol_data.iloc[self.context_window:]
                    
                    X = torch.FloatTensor(context_data[self.feature_cols].values)
                    y = torch.FloatTensor(target_data[self.feature_cols].values)
                    
                    batch_X.append(X)
                    batch_y.append(y)
                    batch_info.append({
                        'symbol': symbol,
                        'start_time': index_info['start_time'],
                        'end_time': index_info['end_time']
                    })
        else:
            # –ù–µ–∑–∞–≤–∏—Å–∏–º–∞—è –≤—ã–±–æ—Ä–∫–∞
            for symbol in self.symbols:
                X, y, info = self.symbol_datasets[symbol][idx]
                batch_X.append(X)
                batch_y.append(y)
                batch_info.append(info)
        
        # –°—Ç–µ–∫–∏—Ä–æ–≤–∞–Ω–∏–µ –≤ –±–∞—Ç—á
        batch_X = torch.stack(batch_X)
        batch_y = torch.stack(batch_y) if len(batch_y[0]) > 0 else torch.tensor([])
        
        return batch_X, batch_y, batch_info


def create_data_loaders(train_data: pd.DataFrame,
                       val_data: pd.DataFrame,
                       test_data: pd.DataFrame,
                       config: Dict,
                       feature_cols: List[str] = None,
                       target_cols: List[str] = None) -> Tuple[DataLoader, DataLoader, DataLoader]:
    """–°–æ–∑–¥–∞–Ω–∏–µ DataLoader'–æ–≤ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è"""
    
    logger = get_logger("DataLoaders")
    
    batch_size = config['model']['batch_size']
    context_window = config['model']['context_window']
    pred_window = config['model']['pred_len']
    num_workers = config['performance']['num_workers']
    persistent_workers = config['performance'].get('persistent_workers', True) if num_workers > 0 else False
    prefetch_factor = config['performance'].get('prefetch_factor', 2)
    
    logger.info("üì¶ –°–æ–∑–¥–∞–Ω–∏–µ —É–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –¥–∞—Ç–∞—Å–µ—Ç–æ–≤...")
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –≤–∞–ª–∏–¥–Ω–æ—Å—Ç–∏ –¥–∞–Ω–Ω—ã—Ö
    if len(train_data) == 0 or len(val_data) == 0 or len(test_data) == 0:
        raise ValueError("‚ùå –û–¥–∏–Ω –∏–∑ –¥–∞—Ç–∞—Å–µ—Ç–æ–≤ –ø—É—Å—Ç!")
    
    # –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –¥–∞–Ω–Ω—ã—Ö
    logger.info(f"üìä –ò—Å—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ:")
    logger.info(f"   - Train: {len(train_data):,} –∑–∞–ø–∏—Å–µ–π, {len(train_data.columns)} –∫–æ–ª–æ–Ω–æ–∫")
    logger.info(f"   - Val: {len(val_data):,} –∑–∞–ø–∏—Å–µ–π, {len(val_data.columns)} –∫–æ–ª–æ–Ω–æ–∫") 
    logger.info(f"   - Test: {len(test_data):,} –∑–∞–ø–∏—Å–µ–π, {len(test_data.columns)} –∫–æ–ª–æ–Ω–æ–∫")
    
    if feature_cols:
        logger.info(f"   - –ü—Ä–∏–∑–Ω–∞–∫–æ–≤: {len(feature_cols)}")
    if target_cols:
        logger.info(f"   - –¶–µ–ª–µ–≤—ã—Ö –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö: {len(target_cols)}")
    
    # –ü–æ–ª—É—á–∞–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏ –∏–∑ –∫–æ–Ω—Ñ–∏–≥–∞
    normalize = config.get('data', {}).get('normalize', True)
    scaler_path = config.get('data', {}).get('scaler_path', 'models_saved/data_scaler.pkl')
    
    # –ü–æ–ª—É—á–∞–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã DataLoader –∏–∑ –∫–æ–Ω—Ñ–∏–≥–∞
    pin_memory = config['performance'].get('dataloader_pin_memory', True)
    drop_last = config['performance'].get('dataloader_drop_last', True)
    
    # –ü–æ–ª—É—á–∞–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã stride –∏–∑ –∫–æ–Ω—Ñ–∏–≥–∞
    train_stride = config.get('data', {}).get('train_stride', 8)  # –£–≤–µ–ª–∏—á–µ–Ω –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è
    val_stride = config.get('data', {}).get('val_stride', 16)    # –ï—â–µ –±–æ–ª—å—à–µ –¥–ª—è –≤–∞–ª–∏–¥–∞—Ü–∏–∏
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞–ª–∏—á–∏—è scaler –¥–ª—è —Ä–µ—à–µ–Ω–∏—è –æ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏ –æ–±—É—á–µ–Ω–∏—è
    from pathlib import Path
    scaler_exists = Path(scaler_path).exists()
    if scaler_exists:
        logger.info(f"‚úÖ –ù–∞–π–¥–µ–Ω —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–π scaler: {scaler_path}")
    else:
        logger.info(f"‚ö†Ô∏è Scaler –Ω–µ –Ω–∞–π–¥–µ–Ω, –±—É–¥–µ—Ç —Å–æ–∑–¥–∞–Ω –Ω–æ–≤—ã–π: {scaler_path}")
    
    # –°–æ–∑–¥–∞–Ω–∏–µ –¥–∞—Ç–∞—Å–µ—Ç–æ–≤ —Å –ø—Ä–æ–≤–µ—Ä–∫–æ–π –æ—à–∏–±–æ–∫
    try:
        train_dataset = TradingDataset(
            data=train_data,
            config=config,
            context_window=context_window,
            prediction_window=pred_window,
            feature_cols=feature_cols,
            target_cols=target_cols,
            stride=train_stride,  # –ò—Å–ø–æ–ª—å–∑—É–µ–º stride –∏–∑ –∫–æ–Ω—Ñ–∏–≥–∞ –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è
            normalize=normalize,
            scaler_path=scaler_path,
            fit_scaler=not scaler_exists  # –û–±—É—á–∞–µ–º scaler —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ –µ–≥–æ –Ω–µ—Ç
        )
        
        val_dataset = TradingDataset(
            data=val_data,
            config=config,
            context_window=context_window,
            prediction_window=pred_window,
            feature_cols=feature_cols,
            target_cols=target_cols,
            stride=val_stride,  # –ò—Å–ø–æ–ª—å–∑—É–µ–º stride –∏–∑ –∫–æ–Ω—Ñ–∏–≥–∞
            normalize=normalize,
            scaler_path=scaler_path,
            fit_scaler=False  # –ò—Å–ø–æ–ª—å–∑—É–µ–º —É–∂–µ –æ–±—É—á–µ–Ω–Ω—ã–π scaler
        )
        
        test_dataset = TradingDataset(
            data=test_data,
            config=config,
            context_window=context_window,
            prediction_window=pred_window,
            feature_cols=feature_cols,
            target_cols=target_cols,
            stride=4,
            normalize=normalize,
            scaler_path=scaler_path,
            fit_scaler=False  # –ò—Å–ø–æ–ª—å–∑—É–µ–º —É–∂–µ –æ–±—É—á–µ–Ω–Ω—ã–π scaler
        )
    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ–∑–¥–∞–Ω–∏–∏ –¥–∞—Ç–∞—Å–µ—Ç–æ–≤: {e}")
        raise
    
    logger.info(f"–†–∞–∑–º–µ—Ä—ã –¥–∞—Ç–∞—Å–µ—Ç–æ–≤ –ø–æ—Å–ª–µ —Å–æ–∑–¥–∞–Ω–∏—è –æ–∫–æ–Ω:")
    logger.info(f"   - Train: {len(train_dataset):,} –æ–∫–æ–Ω (–∏–∑ {len(train_data):,} –∑–∞–ø–∏—Å–µ–π)")
    logger.info(f"   - Val: {len(val_dataset):,} –æ–∫–æ–Ω (–∏–∑ {len(val_data):,} –∑–∞–ø–∏—Å–µ–π, stride={val_dataset.stride})")
    logger.info(f"   - Test: {len(test_dataset):,} –æ–∫–æ–Ω (–∏–∑ {len(test_data):,} –∑–∞–ø–∏—Å–µ–π, stride={test_dataset.stride})")
    
    # –°–æ–∑–¥–∞–Ω–∏–µ DataLoader'–æ–≤ —Å –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–µ–π –¥–ª—è GPU
    train_loader = DataLoader(
        train_dataset,
        batch_size=batch_size,
        shuffle=True,
        num_workers=num_workers,
        pin_memory=pin_memory,
        drop_last=drop_last,
        persistent_workers=persistent_workers,
        prefetch_factor=prefetch_factor if num_workers > 0 else None
    )
    
    val_loader = DataLoader(
        val_dataset,
        batch_size=batch_size,
        shuffle=False,
        num_workers=num_workers,
        pin_memory=pin_memory,
        drop_last=drop_last,  # –¢–∞–∫–∂–µ –≤–∫–ª—é—á–∞–µ–º –¥–ª—è –≤–∞–ª–∏–¥–∞—Ü–∏–∏ –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏
        persistent_workers=persistent_workers,
        prefetch_factor=prefetch_factor if num_workers > 0 else None
    )
    
    test_loader = DataLoader(
        test_dataset,
        batch_size=batch_size,
        shuffle=False,
        num_workers=num_workers,
        pin_memory=pin_memory,
        drop_last=False,  # –î–ª—è —Ç–µ—Å—Ç–∞ –æ—Å—Ç–∞–≤–ª—è–µ–º –≤—Å–µ –¥–∞–Ω–Ω—ã–µ
        persistent_workers=persistent_workers,
        prefetch_factor=prefetch_factor if num_workers > 0 else None
    )
    
    return train_loader, val_loader, test_loader


def collate_trading_batch(batch):
    """–ö–∞—Å—Ç–æ–º–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏—è –±–∞—Ç—á–µ–π —Å –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π"""
    X_list, y_list, info_list = zip(*batch)
    
    # –°—Ç–µ–∫–∏—Ä–æ–≤–∞–Ω–∏–µ —Ç–µ–Ω–∑–æ—Ä–æ–≤
    X = torch.stack(X_list)
    y = torch.stack(y_list) if len(y_list[0]) > 0 else None
    
    # –û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏
    batch_info = {
        'symbols': [info['symbol'] for info in info_list],
        'last_closes': torch.tensor([info['last_close'] for info in info_list]),
        'atrs': torch.tensor([info['atr'] for info in info_list])
    }
    
    if 'price_data' in info_list[0]:
        batch_info['price_data'] = torch.stack([info['price_data'] for info in info_list])
    
    return X, y, batch_info