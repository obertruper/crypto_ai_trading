"""
–ó–∞–≥—Ä—É–∑—á–∏–∫ –¥–∞–Ω–Ω—ã—Ö –∏–∑ PostgreSQL
"""

import pandas as pd
import numpy as np
from typing import List, Optional, Tuple, Dict
from datetime import datetime
import psycopg2
from psycopg2 import pool
from sqlalchemy import create_engine
import pickle
from pathlib import Path
import hashlib
from tqdm import tqdm
from contextlib import contextmanager
import os

from utils.logger import get_logger

class CryptoDataLoader:
    """–ó–∞–≥—Ä—É–∑—á–∏–∫ –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö –∫—Ä–∏–ø—Ç–æ–≤–∞–ª—é—Ç–Ω—ã—Ö —Ñ—å—é—á–µ—Ä—Å–æ–≤"""
    
    def __init__(self, config: Dict):
        self.config = config
        self.logger = get_logger("DataLoader")
        self.cache_dir = Path(config.get('performance', {}).get('cache_dir', 'cache'))
        self.cache_dir.mkdir(exist_ok=True)
        self.engine = self._create_engine()
        
    def _create_engine(self):
        """–°–æ–∑–¥–∞–Ω–∏–µ SQLAlchemy engine"""
        db_config = self.config['database']
        connection_string = (
            f"postgresql://{db_config['user']}:{db_config['password']}@"
            f"{db_config['host']}:{db_config['port']}/{db_config['database']}"
        )
        
        self.logger.info("–ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ –±–∞–∑–µ –¥–∞–Ω–Ω—ã—Ö...")
        return create_engine(connection_string, pool_size=10, max_overflow=20)
    
    def _get_cache_key(self, symbols: List[str], start_date: str, end_date: str) -> str:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∫–ª—é—á–∞ –∫—ç—à–∞"""
        key_string = f"{','.join(sorted(symbols))}_{start_date}_{end_date}"
        return hashlib.md5(key_string.encode()).hexdigest()
    
    def _load_from_cache(self, cache_key: str) -> Optional[pd.DataFrame]:
        """–ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –∏–∑ –∫—ç—à–∞"""
        cache_file = self.cache_dir / f"{cache_key}.pkl"
        
        if cache_file.exists() and self.config.get('performance', {}).get('cache_features', True):
            self.logger.info(f"–ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –∏–∑ –∫—ç—à–∞: {cache_file}")
            try:
                with open(cache_file, 'rb') as f:
                    return pickle.load(f)
            except Exception as e:
                self.logger.warning(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –∫—ç—à–∞: {e}")
                return None
        return None
    
    def _save_to_cache(self, data: pd.DataFrame, cache_key: str):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –≤ –∫—ç—à"""
        if self.config.get('performance', {}).get('cache_features', True):
            cache_file = self.cache_dir / f"{cache_key}.pkl"
            self.logger.info(f"–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –≤ –∫—ç—à: {cache_file}")
            
            try:
                with open(cache_file, 'wb') as f:
                    pickle.dump(data, f, protocol=pickle.HIGHEST_PROTOCOL)
            except Exception as e:
                self.logger.warning(f"–û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –∫—ç—à–∞: {e}")
    
    def load_data(self, 
                  symbols: Optional[List[str]] = None,
                  start_date: Optional[str] = None,
                  end_date: Optional[str] = None) -> pd.DataFrame:
        """–ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –∏–∑ –ë–î —Å –≤–∞–ª–∏–¥–∞—Ü–∏–µ–π"""
        symbols = symbols or self.config['data']['symbols']
        start_date = start_date or self.config['data']['start_date']
        end_date = end_date or self.config['data']['end_date']
        
        # –í–∞–ª–∏–¥–∞—Ü–∏—è –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
        self._validate_symbols(symbols)
        self._validate_dates(start_date, end_date)
        
        cache_key = self._get_cache_key(symbols, start_date, end_date)
        cached_data = self._load_from_cache(cache_key)
        if cached_data is not None:
            return cached_data
        
        self.logger.start_stage("data_loading", symbols_count=len(symbols))
        
        try:
            query = """
            SELECT 
                id,
                symbol,
                timestamp,
                datetime,
                open,
                high,
                low,
                close,
                volume,
                turnover
            FROM raw_market_data
            WHERE 
                symbol = ANY(%(symbols)s)
                AND datetime >= %(start_date)s
                AND datetime <= %(end_date)s
                AND market_type = 'futures'
                AND interval_minutes = 15
            ORDER BY symbol, datetime
            """
            
            chunk_size = 100000
            chunks = []
            
            with self.engine.connect() as conn:
                # –ü–æ–¥—Å—á–µ—Ç –æ–±—â–µ–≥–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –∑–∞–ø–∏—Å–µ–π
                count_query = """
                SELECT COUNT(*) 
                FROM raw_market_data 
                WHERE 
                    symbol = ANY(%(symbols)s)
                    AND datetime >= %(start_date)s
                    AND datetime <= %(end_date)s
                    AND market_type = 'futures'
                    AND interval_minutes = 15
                """
                
                total_records = conn.execute(
                    count_query,
                    {"symbols": symbols, "start_date": start_date, "end_date": end_date}
                ).scalar()
                
                self.logger.info(f"–ó–∞–≥—Ä—É–∑–∫–∞ {total_records:,} –∑–∞–ø–∏—Å–µ–π...")
                
                # –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –ø–æ —á–∞—Å—Ç—è–º
                with tqdm(total=total_records, desc="–ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö") as pbar:
                    for chunk in pd.read_sql(
                        query,
                        conn,
                        params={
                            "symbols": symbols,
                            "start_date": start_date,
                            "end_date": end_date
                        },
                        chunksize=chunk_size
                    ):
                        chunks.append(chunk)
                        pbar.update(len(chunk))
            
            df = pd.concat(chunks, ignore_index=True)
            
            # –û–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–∏–ø–æ–≤ –¥–∞–Ω–Ω—ã—Ö
            df['datetime'] = pd.to_datetime(df['datetime'])
            numeric_columns = ['open', 'high', 'low', 'close', 'volume', 'turnover']
            df[numeric_columns] = df[numeric_columns].astype(np.float32)
            
            self._log_data_statistics(df)
            self._save_to_cache(df, cache_key)
            
            self.logger.end_stage("data_loading", records=len(df))
            
            return df
            
        except Exception as e:
            self.logger.log_error(e, "load_data")
            raise
    
    def _log_data_statistics(self, df: pd.DataFrame):
        """–õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –ø–æ –∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã–º –¥–∞–Ω–Ω—ã–º"""
        self.logger.info("üìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö:")
        
        for symbol in df['symbol'].unique():
            symbol_data = df[df['symbol'] == symbol]
            date_range = (
                symbol_data['datetime'].min().strftime('%Y-%m-%d'),
                symbol_data['datetime'].max().strftime('%Y-%m-%d')
            )
            
            self.logger.log_data_info(
                symbol=symbol,
                records=len(symbol_data),
                date_range=date_range
            )
    
    def load_symbol_data(self, symbol: str, 
                        start_date: Optional[str] = None,
                        end_date: Optional[str] = None) -> pd.DataFrame:
        """–ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–¥–Ω–æ–≥–æ —Å–∏–º–≤–æ–ª–∞"""
        return self.load_data([symbol], start_date, end_date)
    
    def get_available_symbols(self) -> List[str]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å–ø–∏—Å–∫–∞ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö —Å–∏–º–≤–æ–ª–æ–≤ –≤ –ë–î"""
        query = """
        SELECT DISTINCT symbol 
        FROM raw_market_data 
        WHERE market_type = 'futures'
        ORDER BY symbol
        """
        
        with self.engine.connect() as conn:
            result = conn.execute(query)
            symbols = [row[0] for row in result]
        
        self.logger.info(f"–ù–∞–π–¥–µ–Ω–æ {len(symbols)} –¥–æ—Å—Ç—É–ø–Ω—ã—Ö —Å–∏–º–≤–æ–ª–æ–≤")
        return symbols
    
    def get_date_range(self, symbol: Optional[str] = None) -> Tuple[datetime, datetime]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –¥–∏–∞–ø–∞–∑–æ–Ω–∞ –¥–∞—Ç –¥–ª—è —Å–∏–º–≤–æ–ª–∞ –∏–ª–∏ –≤—Å–µ—Ö –¥–∞–Ω–Ω—ã—Ö"""
        if symbol:
            query = """
            SELECT MIN(datetime), MAX(datetime)
            FROM raw_market_data
            WHERE symbol = %(symbol)s AND market_type = 'futures'
            """
            params = {"symbol": symbol}
        else:
            query = """
            SELECT MIN(datetime), MAX(datetime)
            FROM raw_market_data
            WHERE market_type = 'futures'
            """
            params = {}
        
        with self.engine.connect() as conn:
            result = conn.execute(query, params).fetchone()
            
        return result[0], result[1]
    
    def validate_data_quality(self, df: pd.DataFrame) -> Dict[str, Dict]:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞ –¥–∞–Ω–Ω—ã—Ö"""
        self.logger.start_stage("data_validation")
        
        quality_report = {}
        
        for symbol in df['symbol'].unique():
            symbol_data = df[df['symbol'] == symbol].copy()
            symbol_data = symbol_data.sort_values('datetime')
            
            report = {
                'total_records': len(symbol_data),
                'missing_values': symbol_data.isnull().sum().to_dict(),
                'duplicates': symbol_data.duplicated(subset=['datetime']).sum(),
                'gaps': 0,
                'anomalies': {}
            }
            
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø—Ä–æ–ø—É—Å–∫–æ–≤ –≤–æ –≤—Ä–µ–º–µ–Ω–∏
            expected_freq = pd.Timedelta(minutes=15)
            time_diff = symbol_data['datetime'].diff()
            gaps = time_diff[time_diff > expected_freq * 1.5]
            report['gaps'] = len(gaps)
            
            if len(gaps) > 0:
                self.logger.warning(
                    f"–û–±–Ω–∞—Ä—É–∂–µ–Ω–æ {len(gaps)} –ø—Ä–æ–ø—É—Å–∫–æ–≤ –≤ –¥–∞–Ω–Ω—ã—Ö {symbol}"
                )
            
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω—É–ª–µ–≤–æ–≥–æ –æ–±—ä–µ–º–∞
            zero_volume = (symbol_data['volume'] == 0).sum()
            if zero_volume > 0:
                report['anomalies']['zero_volume'] = zero_volume
            
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ –æ—Ç—Å—É—Ç—Å—Ç–≤–∏—è –¥–≤–∏–∂–µ–Ω–∏—è —Ü–µ–Ω—ã
            no_movement = (
                (symbol_data['open'] == symbol_data['high']) & 
                (symbol_data['high'] == symbol_data['low']) & 
                (symbol_data['low'] == symbol_data['close'])
            ).sum()
            if no_movement > 0:
                report['anomalies']['no_price_movement'] = no_movement
            
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ —ç–∫—Å—Ç—Ä–µ–º–∞–ª—å–Ω—ã—Ö –∏–∑–º–µ–Ω–µ–Ω–∏–π —Ü–µ–Ω—ã
            price_change = symbol_data['close'].pct_change()
            extreme_changes = (price_change.abs() > 0.2).sum()
            if extreme_changes > 0:
                report['anomalies']['extreme_price_changes'] = extreme_changes
            
            quality_report[symbol] = report
        
        self.logger.end_stage("data_validation", issues_found=sum(
            len(r['anomalies']) for r in quality_report.values()
        ))
        
        return quality_report
    
    def resample_data(self, df: pd.DataFrame, target_interval: str) -> pd.DataFrame:
        """–ò–∑–º–µ–Ω–µ–Ω–∏–µ –≤—Ä–µ–º–µ–Ω–Ω–æ–≥–æ –∏–Ω—Ç–µ—Ä–≤–∞–ª–∞ –¥–∞–Ω–Ω—ã—Ö"""
        self.logger.info(f"–†–µ—Å–µ–º–ø–ª–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –∫ –∏–Ω—Ç–µ—Ä–≤–∞–ª—É: {target_interval}")
        
        resampled_dfs = []
        
        for symbol in df['symbol'].unique():
            symbol_data = df[df['symbol'] == symbol].copy()
            symbol_data.set_index('datetime', inplace=True)
            
            agg_rules = {
                'open': 'first',
                'high': 'max',
                'low': 'min',
                'close': 'last',
                'volume': 'sum',
                'turnover': 'sum'
            }
            
            resampled = symbol_data.resample(target_interval).agg(agg_rules)
            resampled['symbol'] = symbol
            resampled.reset_index(inplace=True)
            
            resampled_dfs.append(resampled)
        
        return pd.concat(resampled_dfs, ignore_index=True)
    
    def _validate_symbols(self, symbols: List[str]):
        """–í–∞–ª–∏–¥–∞—Ü–∏—è —Å–∏–º–≤–æ–ª–æ–≤"""
        if not symbols:
            raise ValueError("–°–ø–∏—Å–æ–∫ —Å–∏–º–≤–æ–ª–æ–≤ –Ω–µ –º–æ–∂–µ—Ç –±—ã—Ç—å –ø—É—Å—Ç—ã–º")
        
        for symbol in symbols:
            if not isinstance(symbol, str) or not symbol.replace('USDT', '').replace('1000', '').isalnum():
                raise ValueError(f"–ù–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–π —Å–∏–º–≤–æ–ª: {symbol}")
    
    def _validate_dates(self, start_date: str, end_date: str):
        """–í–∞–ª–∏–¥–∞—Ü–∏—è –¥–∞—Ç"""
        try:
            start = pd.to_datetime(start_date)
            end = pd.to_datetime(end_date)
            if start >= end:
                raise ValueError("–ù–∞—á–∞–ª—å–Ω–∞—è –¥–∞—Ç–∞ –¥–æ–ª–∂–Ω–∞ –±—ã—Ç—å —Ä–∞–Ω—å—à–µ –∫–æ–Ω–µ—á–Ω–æ–π")
        except Exception as e:
            raise ValueError(f"–ù–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç –¥–∞—Ç—ã: {e}")
    
    def get_data_completeness(self, symbols: List[str], start_date: str, end_date: str) -> Dict:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –ø–æ–ª–Ω–æ—Ç—ã –¥–∞–Ω–Ω—ã—Ö"""
        query = """
        SELECT 
            symbol,
            COUNT(*) as actual_records,
            MIN(datetime) as first_record,
            MAX(datetime) as last_record
        FROM raw_market_data 
        WHERE symbol = ANY(%(symbols)s)
        AND datetime BETWEEN %(start_date)s AND %(end_date)s
        AND market_type = 'futures'
        GROUP BY symbol
        """
        
        with self.engine.connect() as conn:
            result = conn.execute(query, {
                'symbols': symbols,
                'start_date': start_date,
                'end_date': end_date
            })
            
            completeness = {}
            for row in result:
                symbol, actual, first, last = row
                
                # –í—ã—á–∏—Å–ª—è–µ–º –æ–∂–∏–¥–∞–µ–º–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∑–∞–ø–∏—Å–µ–π (15-–º–∏–Ω—É—Ç–Ω—ã–µ –∏–Ω—Ç–µ—Ä–≤–∞–ª—ã)
                time_diff = (last - first).total_seconds()
                expected_records = int(time_diff / (15 * 60)) + 1
                
                completeness[symbol] = {
                    'actual_records': actual,
                    'expected_records': expected_records,
                    'completeness_pct': (actual / expected_records) * 100 if expected_records > 0 else 0,
                    'first_record': first,
                    'last_record': last
                }
        
        return completeness