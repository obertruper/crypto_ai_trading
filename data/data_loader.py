"""
Ğ—Ğ°Ğ³Ñ€ÑƒĞ·Ñ‡Ğ¸Ğº Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸Ğ· PostgreSQL
"""

import pandas as pd
import numpy as np
from typing import List, Optional, Tuple, Dict
from datetime import datetime
import psycopg2
from psycopg2 import pool
from sqlalchemy import create_engine
import pickle
from pathlib import Path
import hashlib
from tqdm import tqdm
from contextlib import contextmanager
import os

from utils.logger import get_logger

class CryptoDataLoader:
    """Ğ—Ğ°Ğ³Ñ€ÑƒĞ·Ñ‡Ğ¸Ğº Ğ¸ÑÑ‚Ğ¾Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… ĞºÑ€Ğ¸Ğ¿Ñ‚Ğ¾Ğ²Ğ°Ğ»ÑÑ‚Ğ½Ñ‹Ñ… Ñ„ÑŒÑÑ‡ĞµÑ€ÑĞ¾Ğ²"""
    
    def __init__(self, config: Dict):
        self.config = config
        self.logger = get_logger("DataLoader")
        self.cache_dir = Path(config.get('performance', {}).get('cache_dir', 'cache'))
        self.cache_dir.mkdir(exist_ok=True)
        self.engine = self._create_engine()
        
    def _create_engine(self):
        """Ğ¡Ğ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğµ SQLAlchemy engine"""
        db_config = self.config['database']
        connection_string = (
            f"postgresql://{db_config['user']}:{db_config['password']}@"
            f"{db_config['host']}:{db_config['port']}/{db_config['database']}"
        )
        
        self.logger.info("ĞŸĞ¾Ğ´ĞºĞ»ÑÑ‡ĞµĞ½Ğ¸Ğµ Ğº Ğ±Ğ°Ğ·Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…...")
        return create_engine(connection_string, pool_size=10, max_overflow=20)
    
    def _get_cache_key(self, symbols: List[str], start_date: str, end_date: str) -> str:
        """Ğ“ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ ĞºĞ»ÑÑ‡Ğ° ĞºÑÑˆĞ°"""
        key_string = f"{','.join(sorted(symbols))}_{start_date}_{end_date}"
        return hashlib.md5(key_string.encode()).hexdigest()
    
    def _load_from_cache(self, cache_key: str) -> Optional[pd.DataFrame]:
        """Ğ—Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸Ğ· ĞºÑÑˆĞ°"""
        cache_file = self.cache_dir / f"{cache_key}.pkl"
        
        if cache_file.exists() and self.config.get('performance', {}).get('cache_features', True):
            self.logger.info(f"Ğ—Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸Ğ· ĞºÑÑˆĞ°: {cache_file}")
            try:
                with open(cache_file, 'rb') as f:
                    return pickle.load(f)
            except Exception as e:
                self.logger.warning(f"ĞÑˆĞ¸Ğ±ĞºĞ° Ğ·Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞ¸ ĞºÑÑˆĞ°: {e}")
                return None
        return None
    
    def _save_to_cache(self, data: pd.DataFrame, cache_key: str):
        """Ğ¡Ğ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ² ĞºÑÑˆ"""
        if self.config.get('performance', {}).get('cache_features', True):
            cache_file = self.cache_dir / f"{cache_key}.pkl"
            self.logger.info(f"Ğ¡Ğ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ² ĞºÑÑˆ: {cache_file}")
            
            try:
                with open(cache_file, 'wb') as f:
                    pickle.dump(data, f, protocol=pickle.HIGHEST_PROTOCOL)
            except Exception as e:
                self.logger.warning(f"ĞÑˆĞ¸Ğ±ĞºĞ° ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ñ ĞºÑÑˆĞ°: {e}")
    
    def load_data(self, 
                  symbols: Optional[List[str]] = None,
                  start_date: Optional[str] = None,
                  end_date: Optional[str] = None) -> pd.DataFrame:
        """Ğ—Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸Ğ· Ğ‘Ğ” Ñ Ğ²Ğ°Ğ»Ğ¸Ğ´Ğ°Ñ†Ğ¸ĞµĞ¹"""
        symbols = symbols or self.config['data']['symbols']
        start_date = start_date or self.config['data']['start_date']
        end_date = end_date or self.config['data']['end_date']
        
        # Ğ’Ğ°Ğ»Ğ¸Ğ´Ğ°Ñ†Ğ¸Ñ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ²
        self._validate_symbols(symbols)
        self._validate_dates(start_date, end_date)
        
        cache_key = self._get_cache_key(symbols, start_date, end_date)
        cached_data = self._load_from_cache(cache_key)
        if cached_data is not None:
            return cached_data
        
        self.logger.start_stage("data_loading", symbols_count=len(symbols))
        
        try:
            query = """
            SELECT 
                id,
                symbol,
                timestamp,
                datetime,
                open,
                high,
                low,
                close,
                volume,
                turnover
            FROM raw_market_data
            WHERE 
                symbol = ANY(%(symbols)s)
                AND datetime >= %(start_date)s
                AND datetime <= %(end_date)s
                AND market_type = 'futures'
                AND interval_minutes = 15
            ORDER BY symbol, datetime
            """
            
            chunk_size = 100000
            chunks = []
            
            with self.engine.connect() as conn:
                # ĞŸĞ¾Ğ´ÑÑ‡ĞµÑ‚ Ğ¾Ğ±Ñ‰ĞµĞ³Ğ¾ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ·Ğ°Ğ¿Ğ¸ÑĞµĞ¹
                count_query = """
                SELECT COUNT(*) 
                FROM raw_market_data 
                WHERE 
                    symbol = ANY(%(symbols)s)
                    AND datetime >= %(start_date)s
                    AND datetime <= %(end_date)s
                    AND market_type = 'futures'
                    AND interval_minutes = 15
                """
                
                total_records = conn.execute(
                    count_query,
                    {"symbols": symbols, "start_date": start_date, "end_date": end_date}
                ).scalar()
                
                self.logger.info(f"Ğ—Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞ° {total_records:,} Ğ·Ğ°Ğ¿Ğ¸ÑĞµĞ¹...")
                
                # Ğ—Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¿Ğ¾ Ñ‡Ğ°ÑÑ‚ÑĞ¼
                with tqdm(total=total_records, desc="Ğ—Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…") as pbar:
                    for chunk in pd.read_sql(
                        query,
                        conn,
                        params={
                            "symbols": symbols,
                            "start_date": start_date,
                            "end_date": end_date
                        },
                        chunksize=chunk_size
                    ):
                        chunks.append(chunk)
                        pbar.update(len(chunk))
            
            df = pd.concat(chunks, ignore_index=True)
            
            # ĞĞ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ° Ñ‚Ğ¸Ğ¿Ğ¾Ğ² Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…
            df['datetime'] = pd.to_datetime(df['datetime'])
            numeric_columns = ['open', 'high', 'low', 'close', 'volume', 'turnover']
            df[numeric_columns] = df[numeric_columns].astype(np.float32)
            
            self._log_data_statistics(df)
            self._save_to_cache(df, cache_key)
            
            self.logger.end_stage("data_loading", records=len(df))
            
            return df
            
        except Exception as e:
            self.logger.log_error(e, "load_data")
            raise
    
    def _log_data_statistics(self, df: pd.DataFrame):
        """Ğ›Ğ¾Ğ³Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ ÑÑ‚Ğ°Ñ‚Ğ¸ÑÑ‚Ğ¸ĞºĞ¸ Ğ¿Ğ¾ Ğ·Ğ°Ğ³Ñ€ÑƒĞ¶ĞµĞ½Ğ½Ñ‹Ğ¼ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¼"""
        self.logger.info("ğŸ“Š Ğ¡Ñ‚Ğ°Ñ‚Ğ¸ÑÑ‚Ğ¸ĞºĞ° Ğ·Ğ°Ğ³Ñ€ÑƒĞ¶ĞµĞ½Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…:")
        
        for symbol in df['symbol'].unique():
            symbol_data = df[df['symbol'] == symbol]
            date_range = (
                symbol_data['datetime'].min().strftime('%Y-%m-%d'),
                symbol_data['datetime'].max().strftime('%Y-%m-%d')
            )
            
            self.logger.log_data_info(
                symbol=symbol,
                records=len(symbol_data),
                date_range=date_range
            )
    
    def load_symbol_data(self, symbol: str, 
                        start_date: Optional[str] = None,
                        end_date: Optional[str] = None) -> pd.DataFrame:
        """Ğ—Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ¾Ğ´Ğ½Ğ¾Ğ³Ğ¾ ÑĞ¸Ğ¼Ğ²Ğ¾Ğ»Ğ°"""
        return self.load_data([symbol], start_date, end_date)
    
    def get_available_symbols(self) -> List[str]:
        """ĞŸĞ¾Ğ»ÑƒÑ‡ĞµĞ½Ğ¸Ğµ ÑĞ¿Ğ¸ÑĞºĞ° Ğ´Ğ¾ÑÑ‚ÑƒĞ¿Ğ½Ñ‹Ñ… ÑĞ¸Ğ¼Ğ²Ğ¾Ğ»Ğ¾Ğ² Ğ² Ğ‘Ğ”"""
        query = """
        SELECT DISTINCT symbol 
        FROM raw_market_data 
        WHERE market_type = 'futures'
        ORDER BY symbol
        """
        
        with self.engine.connect() as conn:
            result = conn.execute(query)
            symbols = [row[0] for row in result]
        
        self.logger.info(f"ĞĞ°Ğ¹Ğ´ĞµĞ½Ğ¾ {len(symbols)} Ğ´Ğ¾ÑÑ‚ÑƒĞ¿Ğ½Ñ‹Ñ… ÑĞ¸Ğ¼Ğ²Ğ¾Ğ»Ğ¾Ğ²")
        return symbols
    
    def get_date_range(self, symbol: Optional[str] = None) -> Tuple[datetime, datetime]:
        """ĞŸĞ¾Ğ»ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ´Ğ¸Ğ°Ğ¿Ğ°Ğ·Ğ¾Ğ½Ğ° Ğ´Ğ°Ñ‚ Ğ´Ğ»Ñ ÑĞ¸Ğ¼Ğ²Ğ¾Ğ»Ğ° Ğ¸Ğ»Ğ¸ Ğ²ÑĞµÑ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…"""
        if symbol:
            query = """
            SELECT MIN(datetime), MAX(datetime)
            FROM raw_market_data
            WHERE symbol = %(symbol)s AND market_type = 'futures'
            """
            params = {"symbol": symbol}
        else:
            query = """
            SELECT MIN(datetime), MAX(datetime)
            FROM raw_market_data
            WHERE market_type = 'futures'
            """
            params = {}
        
        with self.engine.connect() as conn:
            result = conn.execute(query, params).fetchone()
            
        return result[0], result[1]
    
    def validate_data_quality(self, df: pd.DataFrame) -> Dict[str, Dict]:
        """ĞŸÑ€Ğ¾Ğ²ĞµÑ€ĞºĞ° ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…"""
        self.logger.start_stage("data_validation")
        
        quality_report = {}
        
        for symbol in df['symbol'].unique():
            symbol_data = df[df['symbol'] == symbol].copy()
            symbol_data = symbol_data.sort_values('datetime')
            
            report = {
                'total_records': len(symbol_data),
                'missing_values': symbol_data.isnull().sum().to_dict(),
                'duplicates': symbol_data.duplicated(subset=['datetime']).sum(),
                'gaps': 0,
                'anomalies': {}
            }
            
            # ĞŸÑ€Ğ¾Ğ²ĞµÑ€ĞºĞ° Ğ¿Ñ€Ğ¾Ğ¿ÑƒÑĞºĞ¾Ğ² Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸
            expected_freq = pd.Timedelta(minutes=15)
            time_diff = symbol_data['datetime'].diff()
            gaps = time_diff[time_diff > expected_freq * 1.5]
            report['gaps'] = len(gaps)
            
            if len(gaps) > 0:
                self.logger.warning(
                    f"ĞĞ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¾ {len(gaps)} Ğ¿Ñ€Ğ¾Ğ¿ÑƒÑĞºĞ¾Ğ² Ğ² Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… {symbol}"
                )
            
            # ĞŸÑ€Ğ¾Ğ²ĞµÑ€ĞºĞ° Ğ½ÑƒĞ»ĞµĞ²Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑŠĞµĞ¼Ğ°
            zero_volume = (symbol_data['volume'] == 0).sum()
            if zero_volume > 0:
                report['anomalies']['zero_volume'] = zero_volume
            
            # ĞŸÑ€Ğ¾Ğ²ĞµÑ€ĞºĞ° Ğ¾Ñ‚ÑÑƒÑ‚ÑÑ‚Ğ²Ğ¸Ñ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ñ†ĞµĞ½Ñ‹
            no_movement = (
                (symbol_data['open'] == symbol_data['high']) & 
                (symbol_data['high'] == symbol_data['low']) & 
                (symbol_data['low'] == symbol_data['close'])
            ).sum()
            if no_movement > 0:
                report['anomalies']['no_price_movement'] = no_movement
            
            # ĞŸÑ€Ğ¾Ğ²ĞµÑ€ĞºĞ° ÑĞºÑÑ‚Ñ€ĞµĞ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğ¹ Ñ†ĞµĞ½Ñ‹
            price_change = symbol_data['close'].pct_change()
            extreme_changes = (price_change.abs() > 0.2).sum()
            if extreme_changes > 0:
                report['anomalies']['extreme_price_changes'] = extreme_changes
            
            quality_report[symbol] = report
        
        self.logger.end_stage("data_validation", issues_found=sum(
            len(r['anomalies']) for r in quality_report.values()
        ))
        
        return quality_report
    
    def resample_data(self, df: pd.DataFrame, target_interval: str) -> pd.DataFrame:
        """Ğ˜Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğµ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ²Ğ°Ğ»Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…"""
        self.logger.info(f"Ğ ĞµÑĞµĞ¼Ğ¿Ğ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğº Ğ¸Ğ½Ñ‚ĞµÑ€Ğ²Ğ°Ğ»Ñƒ: {target_interval}")
        
        resampled_dfs = []
        
        for symbol in df['symbol'].unique():
            symbol_data = df[df['symbol'] == symbol].copy()
            symbol_data.set_index('datetime', inplace=True)
            
            agg_rules = {
                'open': 'first',
                'high': 'max',
                'low': 'min',
                'close': 'last',
                'volume': 'sum',
                'turnover': 'sum'
            }
            
            resampled = symbol_data.resample(target_interval).agg(agg_rules)
            resampled['symbol'] = symbol
            resampled.reset_index(inplace=True)
            
            resampled_dfs.append(resampled)
        
        return pd.concat(resampled_dfs, ignore_index=True)
    
    def _validate_symbols(self, symbols: List[str]):
        """Ğ’Ğ°Ğ»Ğ¸Ğ´Ğ°Ñ†Ğ¸Ñ ÑĞ¸Ğ¼Ğ²Ğ¾Ğ»Ğ¾Ğ²"""
        if not symbols:
            raise ValueError("Ğ¡Ğ¿Ğ¸ÑĞ¾Ğº ÑĞ¸Ğ¼Ğ²Ğ¾Ğ»Ğ¾Ğ² Ğ½Ğµ Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ±Ñ‹Ñ‚ÑŒ Ğ¿ÑƒÑÑ‚Ñ‹Ğ¼")
        
        for symbol in symbols:
            if not isinstance(symbol, str) or not symbol.replace('USDT', '').replace('1000', '').isalnum():
                raise ValueError(f"ĞĞµĞºĞ¾Ñ€Ñ€ĞµĞºÑ‚Ğ½Ñ‹Ğ¹ ÑĞ¸Ğ¼Ğ²Ğ¾Ğ»: {symbol}")
    
    def _validate_dates(self, start_date: str, end_date: str):
        """Ğ’Ğ°Ğ»Ğ¸Ğ´Ğ°Ñ†Ğ¸Ñ Ğ´Ğ°Ñ‚"""
        try:
            start = pd.to_datetime(start_date)
            end = pd.to_datetime(end_date)
            if start >= end:
                raise ValueError("ĞĞ°Ñ‡Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ğ´Ğ°Ñ‚Ğ° Ğ´Ğ¾Ğ»Ğ¶Ğ½Ğ° Ğ±Ñ‹Ñ‚ÑŒ Ñ€Ğ°Ğ½ÑŒÑˆĞµ ĞºĞ¾Ğ½ĞµÑ‡Ğ½Ğ¾Ğ¹")
        except Exception as e:
            raise ValueError(f"ĞĞµĞºĞ¾Ñ€Ñ€ĞµĞºÑ‚Ğ½Ñ‹Ğ¹ Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚ Ğ´Ğ°Ñ‚Ñ‹: {e}")
    
    def get_data_completeness(self, symbols: List[str], start_date: str, end_date: str) -> Dict:
        """ĞŸÑ€Ğ¾Ğ²ĞµÑ€ĞºĞ° Ğ¿Ğ¾Ğ»Ğ½Ğ¾Ñ‚Ñ‹ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…"""
        query = """
        SELECT 
            symbol,
            COUNT(*) as actual_records,
            MIN(datetime) as first_record,
            MAX(datetime) as last_record
        FROM raw_market_data 
        WHERE symbol = ANY(%(symbols)s)
        AND datetime BETWEEN %(start_date)s AND %(end_date)s
        AND market_type = 'futures'
        GROUP BY symbol
        """
        
        with self.engine.connect() as conn:
            result = conn.execute(query, {
                'symbols': symbols,
                'start_date': start_date,
                'end_date': end_date
            })
            
            completeness = {}
            for row in result:
                symbol, actual, first, last = row
                
                # Ğ’Ñ‹Ñ‡Ğ¸ÑĞ»ÑĞµĞ¼ Ğ¾Ğ¶Ğ¸Ğ´Ğ°ĞµĞ¼Ğ¾Ğµ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ·Ğ°Ğ¿Ğ¸ÑĞµĞ¹ (15-Ğ¼Ğ¸Ğ½ÑƒÑ‚Ğ½Ñ‹Ğµ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ²Ğ°Ğ»Ñ‹)
                time_diff = (last - first).total_seconds()
                expected_records = int(time_diff / (15 * 60)) + 1
                
                completeness[symbol] = {
                    'actual_records': actual,
                    'expected_records': expected_records,
                    'completeness_pct': (actual / expected_records) * 100 if expected_records > 0 else 0,
                    'first_record': first,
                    'last_record': last
                }
        
        return completeness