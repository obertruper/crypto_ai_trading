"""
Ð—Ð°Ð³Ñ€ÑƒÐ·Ñ‡Ð¸Ðº Ð´Ð°Ð½Ð½Ñ‹Ñ… Ð¸Ð· PostgreSQL
"""

import pandas as pd
import numpy as np
from typing import List, Optional, Tuple, Dict
from datetime import datetime
import psycopg2
from psycopg2 import pool
from psycopg2.extras import RealDictCursor
from sqlalchemy import create_engine, text
import pickle
from pathlib import Path
import hashlib
from tqdm import tqdm
from contextlib import contextmanager
import os

from utils.logger import get_logger

class CryptoDataLoader:
    """Ð—Ð°Ð³Ñ€ÑƒÐ·Ñ‡Ð¸Ðº Ð¸ÑÑ‚Ð¾Ñ€Ð¸Ñ‡ÐµÑÐºÐ¸Ñ… Ð´Ð°Ð½Ð½Ñ‹Ñ… ÐºÑ€Ð¸Ð¿Ñ‚Ð¾Ð²Ð°Ð»ÑŽÑ‚Ð½Ñ‹Ñ… Ñ„ÑŒÑŽÑ‡ÐµÑ€ÑÐ¾Ð²"""
    
    def __init__(self, config: Dict):
        self.config = config
        self.logger = get_logger("DataLoader")
        self.cache_dir = Path(config.get('performance', {}).get('cache_dir', 'cache'))
        self.cache_dir.mkdir(exist_ok=True)
        self.engine = self._create_engine()
        
    def _create_engine(self):
        """Ð¡Ð¾Ð·Ð´Ð°Ð½Ð¸Ðµ SQLAlchemy engine Ñ Ð¿Ð¾Ð´Ð´ÐµÑ€Ð¶ÐºÐ¾Ð¹ Ð¿ÐµÑ€ÐµÐ¼ÐµÐ½Ð½Ñ‹Ñ… Ð¾ÐºÑ€ÑƒÐ¶ÐµÐ½Ð¸Ñ"""
        db_config = self.config['database'].copy()
        
        # ÐžÐ±Ñ€Ð°Ð±Ð¾Ñ‚ÐºÐ° Ð¿ÐµÑ€ÐµÐ¼ÐµÐ½Ð½Ñ‹Ñ… Ð¾ÐºÑ€ÑƒÐ¶ÐµÐ½Ð¸Ñ
        for key, value in db_config.items():
            if isinstance(value, str) and value.startswith('${'):
                # Ð˜Ð·Ð²Ð»ÐµÑ‡ÐµÐ½Ð¸Ðµ Ð¸Ð¼ÐµÐ½Ð¸ Ð¿ÐµÑ€ÐµÐ¼ÐµÐ½Ð½Ð¾Ð¹ Ð¸ Ð·Ð½Ð°Ñ‡ÐµÐ½Ð¸Ñ Ð¿Ð¾ ÑƒÐ¼Ð¾Ð»Ñ‡Ð°Ð½Ð¸ÑŽ
                var_name = value.split(':')[0][2:]
                default_value = value.split(':')[1].rstrip('}')
                db_config[key] = os.getenv(var_name, default_value)
        
        # ÐŸÑ€ÐµÐ¾Ð±Ñ€Ð°Ð·Ð¾Ð²Ð°Ð½Ð¸Ðµ Ð¿Ð¾Ñ€Ñ‚Ð° Ð² int
        db_config['port'] = int(db_config['port'])
        
        # Ð¡Ð¾Ð·Ð´Ð°Ð½Ð¸Ðµ connection string
        connection_string = (
            f"postgresql://{db_config['user']}:{db_config['password']}@"
            f"{db_config['host']}:{db_config['port']}/{db_config['database']}"
        )
        
        self.logger.info(f"ÐŸÐ¾Ð´ÐºÐ»ÑŽÑ‡ÐµÐ½Ð¸Ðµ Ðº Ð‘Ð” {db_config['database']} Ð½Ð° {db_config['host']}:{db_config['port']}...")
        
        # Ð¡Ð¾Ð·Ð´Ð°Ð½Ð¸Ðµ engine Ñ Ð´Ð¾Ð¿Ð¾Ð»Ð½Ð¸Ñ‚ÐµÐ»ÑŒÐ½Ñ‹Ð¼Ð¸ Ð¿Ð°Ñ€Ð°Ð¼ÐµÑ‚Ñ€Ð°Ð¼Ð¸ Ð´Ð»Ñ ÑÑ‚Ð°Ð±Ð¸Ð»ÑŒÐ½Ð¾ÑÑ‚Ð¸
        engine = create_engine(
            connection_string,
            pool_size=db_config.get('pool_size', 10),
            max_overflow=db_config.get('max_overflow', 20),
            pool_pre_ping=True,  # ÐŸÑ€Ð¾Ð²ÐµÑ€ÐºÐ° ÑÐ¾ÐµÐ´Ð¸Ð½ÐµÐ½Ð¸Ñ Ð¿ÐµÑ€ÐµÐ´ Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ð½Ð¸ÐµÐ¼
            echo=False  # ÐžÑ‚ÐºÐ»ÑŽÑ‡Ð°ÐµÐ¼ SQL Ð»Ð¾Ð³Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¸Ðµ
        )
        
        # Ð¢ÐµÑÑ‚ Ð¿Ð¾Ð´ÐºÐ»ÑŽÑ‡ÐµÐ½Ð¸Ñ
        try:
            with engine.connect() as conn:
                result = conn.execute(text("SELECT 1"))
                result.fetchone()
            self.logger.info("âœ… ÐŸÐ¾Ð´ÐºÐ»ÑŽÑ‡ÐµÐ½Ð¸Ðµ Ðº Ð‘Ð” ÑƒÑÐ¿ÐµÑˆÐ½Ð¾ ÑƒÑÑ‚Ð°Ð½Ð¾Ð²Ð»ÐµÐ½Ð¾")
        except Exception as e:
            self.logger.error(f"âŒ ÐžÑˆÐ¸Ð±ÐºÐ° Ð¿Ð¾Ð´ÐºÐ»ÑŽÑ‡ÐµÐ½Ð¸Ñ Ðº Ð‘Ð”: {e}")
            raise
            
        return engine
    
    def _get_cache_key(self, symbols: List[str], start_date: str, end_date: str) -> str:
        """Ð“ÐµÐ½ÐµÑ€Ð°Ñ†Ð¸Ñ ÐºÐ»ÑŽÑ‡Ð° ÐºÑÑˆÐ°"""
        key_string = f"{','.join(sorted(symbols))}_{start_date}_{end_date}"
        return hashlib.md5(key_string.encode()).hexdigest()
    
    def _load_from_cache(self, cache_key: str) -> Optional[pd.DataFrame]:
        """Ð—Ð°Ð³Ñ€ÑƒÐ·ÐºÐ° Ð´Ð°Ð½Ð½Ñ‹Ñ… Ð¸Ð· ÐºÑÑˆÐ°"""
        cache_file = self.cache_dir / f"{cache_key}.pkl"
        
        if cache_file.exists() and self.config.get('performance', {}).get('cache_features', True):
            self.logger.info(f"Ð—Ð°Ð³Ñ€ÑƒÐ·ÐºÐ° Ð´Ð°Ð½Ð½Ñ‹Ñ… Ð¸Ð· ÐºÑÑˆÐ°: {cache_file}")
            try:
                with open(cache_file, 'rb') as f:
                    return pickle.load(f)
            except Exception as e:
                self.logger.warning(f"ÐžÑˆÐ¸Ð±ÐºÐ° Ð·Ð°Ð³Ñ€ÑƒÐ·ÐºÐ¸ ÐºÑÑˆÐ°: {e}")
                return None
        return None
    
    def _save_to_cache(self, data: pd.DataFrame, cache_key: str):
        """Ð¡Ð¾Ñ…Ñ€Ð°Ð½ÐµÐ½Ð¸Ðµ Ð´Ð°Ð½Ð½Ñ‹Ñ… Ð² ÐºÑÑˆ"""
        if self.config.get('performance', {}).get('cache_features', True):
            cache_file = self.cache_dir / f"{cache_key}.pkl"
            self.logger.info(f"Ð¡Ð¾Ñ…Ñ€Ð°Ð½ÐµÐ½Ð¸Ðµ Ð´Ð°Ð½Ð½Ñ‹Ñ… Ð² ÐºÑÑˆ: {cache_file}")
            
            try:
                with open(cache_file, 'wb') as f:
                    pickle.dump(data, f, protocol=pickle.HIGHEST_PROTOCOL)
            except Exception as e:
                self.logger.warning(f"ÐžÑˆÐ¸Ð±ÐºÐ° ÑÐ¾Ñ…Ñ€Ð°Ð½ÐµÐ½Ð¸Ñ ÐºÑÑˆÐ°: {e}")
    
    def load_data(self, 
                  symbols: Optional[List[str]] = None,
                  start_date: Optional[str] = None,
                  end_date: Optional[str] = None) -> pd.DataFrame:
        """Ð—Ð°Ð³Ñ€ÑƒÐ·ÐºÐ° Ð´Ð°Ð½Ð½Ñ‹Ñ… Ð¸Ð· Ð‘Ð” Ñ Ð²Ð°Ð»Ð¸Ð´Ð°Ñ†Ð¸ÐµÐ¹"""
        symbols = symbols or self.config['data']['symbols']
        start_date = start_date or self.config['data']['start_date']
        end_date = end_date or self.config['data']['end_date']
        
        # Ð’Ð°Ð»Ð¸Ð´Ð°Ñ†Ð¸Ñ Ð¿Ð°Ñ€Ð°Ð¼ÐµÑ‚Ñ€Ð¾Ð²
        self._validate_symbols(symbols)
        self._validate_dates(start_date, end_date)
        
        cache_key = self._get_cache_key(symbols, start_date, end_date)
        cached_data = self._load_from_cache(cache_key)
        if cached_data is not None:
            return cached_data
        
        self.logger.start_stage("data_loading", symbols_count=len(symbols))
        
        try:
            # SQL Ð·Ð°Ð¿Ñ€Ð¾Ñ Ð´Ð»Ñ pandas (Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐµÐ¼ %s ÑÐ¸Ð½Ñ‚Ð°ÐºÑÐ¸Ñ)
            query = """
            SELECT 
                id,
                symbol,
                timestamp,
                datetime,
                open,
                high,
                low,
                close,
                volume,
                turnover
            FROM raw_market_data
            WHERE 
                symbol = ANY(%(symbols)s)
                AND datetime >= %(start_date)s
                AND datetime <= %(end_date)s
                AND market_type = 'futures'
                AND interval_minutes = 15
            ORDER BY symbol, datetime
            """
            
            chunk_size = 100000
            chunks = []
            
            # Ð˜ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐµÐ¼ Ð¿Ñ€ÑÐ¼Ð¾Ðµ Ð¿Ð¾Ð´ÐºÐ»ÑŽÑ‡ÐµÐ½Ð¸Ðµ psycopg2 Ð´Ð»Ñ Ð·Ð°Ð³Ñ€ÑƒÐ·ÐºÐ¸ Ð´Ð°Ð½Ð½Ñ‹Ñ…
            db_config = self.config['database'].copy()
            # ÐžÐ±Ñ€Ð°Ð±Ð¾Ñ‚ÐºÐ° Ð¿ÐµÑ€ÐµÐ¼ÐµÐ½Ð½Ñ‹Ñ… Ð¾ÐºÑ€ÑƒÐ¶ÐµÐ½Ð¸Ñ
            for key, value in db_config.items():
                if isinstance(value, str) and value.startswith('${'):
                    var_name = value.split(':')[0][2:]
                    default_value = value.split(':')[1].rstrip('}')
                    db_config[key] = os.getenv(var_name, default_value)
            db_config['port'] = int(db_config['port'])
            
            # ÐŸÑ€ÑÐ¼Ð¾Ðµ Ð¿Ð¾Ð´ÐºÐ»ÑŽÑ‡ÐµÐ½Ð¸Ðµ Ñ‡ÐµÑ€ÐµÐ· psycopg2
            conn = psycopg2.connect(
                host=db_config['host'],
                port=db_config['port'],
                database=db_config['database'],
                user=db_config['user'],
                password=db_config['password']
            )
            
            try:
                # ÐŸÐ¾Ð´ÑÑ‡ÐµÑ‚ Ð¾Ð±Ñ‰ÐµÐ³Ð¾ ÐºÐ¾Ð»Ð¸Ñ‡ÐµÑÑ‚Ð²Ð° Ð·Ð°Ð¿Ð¸ÑÐµÐ¹
                count_query = """
                SELECT COUNT(*) 
                FROM raw_market_data 
                WHERE 
                    symbol = ANY(%s)
                    AND datetime >= %s
                    AND datetime <= %s
                    AND market_type = 'futures'
                    AND interval_minutes = 15
                """
                
                cursor = conn.cursor()
                cursor.execute(count_query, (symbols, start_date, end_date))
                total_records = cursor.fetchone()[0]
                cursor.close()
                
                self.logger.info(f"Ð—Ð°Ð³Ñ€ÑƒÐ·ÐºÐ° {total_records:,} Ð·Ð°Ð¿Ð¸ÑÐµÐ¹...")
                
                # Ð—Ð°Ð³Ñ€ÑƒÐ·ÐºÐ° Ð´Ð°Ð½Ð½Ñ‹Ñ… Ñ‡ÐµÑ€ÐµÐ· pandas
                with tqdm(total=total_records, desc="Ð—Ð°Ð³Ñ€ÑƒÐ·ÐºÐ° Ð´Ð°Ð½Ð½Ñ‹Ñ…") as pbar:
                    for chunk in pd.read_sql(
                        query,
                        conn,
                        params={
                            "symbols": symbols,
                            "start_date": start_date,
                            "end_date": end_date
                        },
                        chunksize=chunk_size
                    ):
                        chunks.append(chunk)
                        pbar.update(len(chunk))
            finally:
                conn.close()
            
            df = pd.concat(chunks, ignore_index=True)
            
            # ÐžÐ±Ñ€Ð°Ð±Ð¾Ñ‚ÐºÐ° Ñ‚Ð¸Ð¿Ð¾Ð² Ð´Ð°Ð½Ð½Ñ‹Ñ…
            df['datetime'] = pd.to_datetime(df['datetime'])
            numeric_columns = ['open', 'high', 'low', 'close', 'volume', 'turnover']
            df[numeric_columns] = df[numeric_columns].astype(np.float32)
            
            self._log_data_statistics(df)
            self._save_to_cache(df, cache_key)
            
            self.logger.end_stage("data_loading", records=len(df))
            
            return df
            
        except Exception as e:
            self.logger.log_error(e, "load_data")
            raise
    
    def _log_data_statistics(self, df: pd.DataFrame):
        """Ð›Ð¾Ð³Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¸Ðµ ÑÑ‚Ð°Ñ‚Ð¸ÑÑ‚Ð¸ÐºÐ¸ Ð¿Ð¾ Ð·Ð°Ð³Ñ€ÑƒÐ¶ÐµÐ½Ð½Ñ‹Ð¼ Ð´Ð°Ð½Ð½Ñ‹Ð¼"""
        self.logger.info("ðŸ“Š Ð¡Ñ‚Ð°Ñ‚Ð¸ÑÑ‚Ð¸ÐºÐ° Ð·Ð°Ð³Ñ€ÑƒÐ¶ÐµÐ½Ð½Ñ‹Ñ… Ð´Ð°Ð½Ð½Ñ‹Ñ…:")
        
        for symbol in df['symbol'].unique():
            symbol_data = df[df['symbol'] == symbol]
            date_range = (
                symbol_data['datetime'].min().strftime('%Y-%m-%d'),
                symbol_data['datetime'].max().strftime('%Y-%m-%d')
            )
            
            self.logger.log_data_info(
                symbol=symbol,
                records=len(symbol_data),
                date_range=date_range
            )
    
    def load_symbol_data(self, symbol: str, 
                        start_date: Optional[str] = None,
                        end_date: Optional[str] = None) -> pd.DataFrame:
        """Ð—Ð°Ð³Ñ€ÑƒÐ·ÐºÐ° Ð´Ð°Ð½Ð½Ñ‹Ñ… Ð´Ð»Ñ Ð¾Ð´Ð½Ð¾Ð³Ð¾ ÑÐ¸Ð¼Ð²Ð¾Ð»Ð°"""
        return self.load_data([symbol], start_date, end_date)
    
    def get_available_symbols(self) -> List[str]:
        """ÐŸÐ¾Ð»ÑƒÑ‡ÐµÐ½Ð¸Ðµ ÑÐ¿Ð¸ÑÐºÐ° Ð´Ð¾ÑÑ‚ÑƒÐ¿Ð½Ñ‹Ñ… ÑÐ¸Ð¼Ð²Ð¾Ð»Ð¾Ð² Ð² Ð‘Ð”"""
        query = """
        SELECT DISTINCT symbol 
        FROM raw_market_data 
        WHERE market_type = 'futures'
        ORDER BY symbol
        """
        
        with self.engine.connect() as conn:
            result = conn.execute(text(query))
            symbols = [row[0] for row in result]
        
        self.logger.info(f"ÐÐ°Ð¹Ð´ÐµÐ½Ð¾ {len(symbols)} Ð´Ð¾ÑÑ‚ÑƒÐ¿Ð½Ñ‹Ñ… ÑÐ¸Ð¼Ð²Ð¾Ð»Ð¾Ð²")
        return symbols
    
    def get_date_range(self, symbol: Optional[str] = None) -> Tuple[datetime, datetime]:
        """ÐŸÐ¾Ð»ÑƒÑ‡ÐµÐ½Ð¸Ðµ Ð´Ð¸Ð°Ð¿Ð°Ð·Ð¾Ð½Ð° Ð´Ð°Ñ‚ Ð´Ð»Ñ ÑÐ¸Ð¼Ð²Ð¾Ð»Ð° Ð¸Ð»Ð¸ Ð²ÑÐµÑ… Ð´Ð°Ð½Ð½Ñ‹Ñ…"""
        if symbol:
            query = """
            SELECT MIN(datetime), MAX(datetime)
            FROM raw_market_data
            WHERE symbol = :symbol AND market_type = 'futures'
            """
            params = {"symbol": symbol}
        else:
            query = """
            SELECT MIN(datetime), MAX(datetime)
            FROM raw_market_data
            WHERE market_type = 'futures'
            """
            params = {}
        
        with self.engine.connect() as conn:
            result = conn.execute(text(query), params).fetchone()
            
        return result[0], result[1]
    
    def validate_data_quality(self, df: pd.DataFrame) -> Dict[str, Dict]:
        """ÐŸÑ€Ð¾Ð²ÐµÑ€ÐºÐ° ÐºÐ°Ñ‡ÐµÑÑ‚Ð²Ð° Ð´Ð°Ð½Ð½Ñ‹Ñ…"""
        self.logger.start_stage("data_validation")
        
        quality_report = {}
        
        for symbol in df['symbol'].unique():
            symbol_data = df[df['symbol'] == symbol].copy()
            symbol_data = symbol_data.sort_values('datetime')
            
            report = {
                'total_records': len(symbol_data),
                'missing_values': symbol_data.isnull().sum().to_dict(),
                'duplicates': symbol_data.duplicated(subset=['datetime']).sum(),
                'gaps': 0,
                'anomalies': {}
            }
            
            # ÐŸÑ€Ð¾Ð²ÐµÑ€ÐºÐ° Ð¿Ñ€Ð¾Ð¿ÑƒÑÐºÐ¾Ð² Ð²Ð¾ Ð²Ñ€ÐµÐ¼ÐµÐ½Ð¸
            expected_freq = pd.Timedelta(minutes=15)
            time_diff = symbol_data['datetime'].diff()
            gaps = time_diff[time_diff > expected_freq * 1.5]
            report['gaps'] = len(gaps)
            
            if len(gaps) > 0:
                self.logger.warning(
                    f"ÐžÐ±Ð½Ð°Ñ€ÑƒÐ¶ÐµÐ½Ð¾ {len(gaps)} Ð¿Ñ€Ð¾Ð¿ÑƒÑÐºÐ¾Ð² Ð² Ð´Ð°Ð½Ð½Ñ‹Ñ… {symbol}"
                )
            
            # ÐŸÑ€Ð¾Ð²ÐµÑ€ÐºÐ° Ð½ÑƒÐ»ÐµÐ²Ð¾Ð³Ð¾ Ð¾Ð±ÑŠÐµÐ¼Ð°
            zero_volume = (symbol_data['volume'] == 0).sum()
            if zero_volume > 0:
                report['anomalies']['zero_volume'] = zero_volume
            
            # ÐŸÑ€Ð¾Ð²ÐµÑ€ÐºÐ° Ð¾Ñ‚ÑÑƒÑ‚ÑÑ‚Ð²Ð¸Ñ Ð´Ð²Ð¸Ð¶ÐµÐ½Ð¸Ñ Ñ†ÐµÐ½Ñ‹
            no_movement = (
                (symbol_data['open'] == symbol_data['high']) & 
                (symbol_data['high'] == symbol_data['low']) & 
                (symbol_data['low'] == symbol_data['close'])
            ).sum()
            if no_movement > 0:
                report['anomalies']['no_price_movement'] = no_movement
            
            # ÐŸÑ€Ð¾Ð²ÐµÑ€ÐºÐ° ÑÐºÑÑ‚Ñ€ÐµÐ¼Ð°Ð»ÑŒÐ½Ñ‹Ñ… Ð¸Ð·Ð¼ÐµÐ½ÐµÐ½Ð¸Ð¹ Ñ†ÐµÐ½Ñ‹
            price_change = symbol_data['close'].pct_change()
            extreme_changes = (price_change.abs() > 0.2).sum()
            if extreme_changes > 0:
                report['anomalies']['extreme_price_changes'] = extreme_changes
            
            quality_report[symbol] = report
        
        self.logger.end_stage("data_validation", issues_found=sum(
            len(r['anomalies']) for r in quality_report.values()
        ))
        
        return quality_report
    
    def resample_data(self, df: pd.DataFrame, target_interval: str) -> pd.DataFrame:
        """Ð˜Ð·Ð¼ÐµÐ½ÐµÐ½Ð¸Ðµ Ð²Ñ€ÐµÐ¼ÐµÐ½Ð½Ð¾Ð³Ð¾ Ð¸Ð½Ñ‚ÐµÑ€Ð²Ð°Ð»Ð° Ð´Ð°Ð½Ð½Ñ‹Ñ…"""
        self.logger.info(f"Ð ÐµÑÐµÐ¼Ð¿Ð»Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¸Ðµ Ð´Ð°Ð½Ð½Ñ‹Ñ… Ðº Ð¸Ð½Ñ‚ÐµÑ€Ð²Ð°Ð»Ñƒ: {target_interval}")
        
        resampled_dfs = []
        
        for symbol in df['symbol'].unique():
            symbol_data = df[df['symbol'] == symbol].copy()
            symbol_data.set_index('datetime', inplace=True)
            
            agg_rules = {
                'open': 'first',
                'high': 'max',
                'low': 'min',
                'close': 'last',
                'volume': 'sum',
                'turnover': 'sum'
            }
            
            resampled = symbol_data.resample(target_interval).agg(agg_rules)
            resampled['symbol'] = symbol
            resampled.reset_index(inplace=True)
            
            resampled_dfs.append(resampled)
        
        return pd.concat(resampled_dfs, ignore_index=True)
    
    def _validate_symbols(self, symbols: List[str]):
        """Ð’Ð°Ð»Ð¸Ð´Ð°Ñ†Ð¸Ñ ÑÐ¸Ð¼Ð²Ð¾Ð»Ð¾Ð²"""
        if not symbols:
            raise ValueError("Ð¡Ð¿Ð¸ÑÐ¾Ðº ÑÐ¸Ð¼Ð²Ð¾Ð»Ð¾Ð² Ð½Ðµ Ð¼Ð¾Ð¶ÐµÑ‚ Ð±Ñ‹Ñ‚ÑŒ Ð¿ÑƒÑÑ‚Ñ‹Ð¼")
        
        for symbol in symbols:
            if not isinstance(symbol, str) or not symbol.replace('USDT', '').replace('1000', '').isalnum():
                raise ValueError(f"ÐÐµÐºÐ¾Ñ€Ñ€ÐµÐºÑ‚Ð½Ñ‹Ð¹ ÑÐ¸Ð¼Ð²Ð¾Ð»: {symbol}")
    
    def _validate_dates(self, start_date: str, end_date: str):
        """Ð’Ð°Ð»Ð¸Ð´Ð°Ñ†Ð¸Ñ Ð´Ð°Ñ‚"""
        try:
            start = pd.to_datetime(start_date)
            end = pd.to_datetime(end_date)
            if start >= end:
                raise ValueError("ÐÐ°Ñ‡Ð°Ð»ÑŒÐ½Ð°Ñ Ð´Ð°Ñ‚Ð° Ð´Ð¾Ð»Ð¶Ð½Ð° Ð±Ñ‹Ñ‚ÑŒ Ñ€Ð°Ð½ÑŒÑˆÐµ ÐºÐ¾Ð½ÐµÑ‡Ð½Ð¾Ð¹")
        except Exception as e:
            raise ValueError(f"ÐÐµÐºÐ¾Ñ€Ñ€ÐµÐºÑ‚Ð½Ñ‹Ð¹ Ñ„Ð¾Ñ€Ð¼Ð°Ñ‚ Ð´Ð°Ñ‚Ñ‹: {e}")
    
    def get_data_completeness(self, symbols: List[str], start_date: str, end_date: str) -> Dict:
        """ÐŸÑ€Ð¾Ð²ÐµÑ€ÐºÐ° Ð¿Ð¾Ð»Ð½Ð¾Ñ‚Ñ‹ Ð´Ð°Ð½Ð½Ñ‹Ñ…"""
        query = """
        SELECT 
            symbol,
            COUNT(*) as actual_records,
            MIN(datetime) as first_record,
            MAX(datetime) as last_record
        FROM raw_market_data 
        WHERE symbol = ANY(:symbols)
        AND datetime BETWEEN :start_date AND :end_date
        AND market_type = 'futures'
        GROUP BY symbol
        """
        
        with self.engine.connect() as conn:
            result = conn.execute(text(query), {
                'symbols': symbols,
                'start_date': start_date,
                'end_date': end_date
            })
            
            completeness = {}
            for row in result:
                symbol, actual, first, last = row
                
                # Ð’Ñ‹Ñ‡Ð¸ÑÐ»ÑÐµÐ¼ Ð¾Ð¶Ð¸Ð´Ð°ÐµÐ¼Ð¾Ðµ ÐºÐ¾Ð»Ð¸Ñ‡ÐµÑÑ‚Ð²Ð¾ Ð·Ð°Ð¿Ð¸ÑÐµÐ¹ (15-Ð¼Ð¸Ð½ÑƒÑ‚Ð½Ñ‹Ðµ Ð¸Ð½Ñ‚ÐµÑ€Ð²Ð°Ð»Ñ‹)
                time_diff = (last - first).total_seconds()
                expected_records = int(time_diff / (15 * 60)) + 1
                
                completeness[symbol] = {
                    'actual_records': actual,
                    'expected_records': expected_records,
                    'completeness_pct': (actual / expected_records) * 100 if expected_records > 0 else 0,
                    'first_record': first,
                    'last_record': last
                }
        
        return completeness