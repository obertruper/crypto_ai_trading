"""
PrecomputedDataset –¥–ª—è –±—ã—Å—Ç—Ä–æ–π –∑–∞–≥—Ä—É–∑–∫–∏ –ø—Ä–µ–¥–≤—ã—á–∏—Å–ª–µ–Ω–Ω—ã—Ö –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –æ–∫–æ–Ω
"""

import torch
from torch.utils.data import Dataset
import numpy as np
from pathlib import Path
import pickle
from tqdm import tqdm
import gc
import h5py
from typing import List, Dict, Optional, Tuple
import pandas as pd

from utils.logger import get_logger
from data.dataset import TimeSeriesDataset
from torch.utils.data import WeightedRandomSampler


def custom_collate_fn(batch):
    """–ö–∞—Å—Ç–æ–º–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –ø—Ä–∞–≤–∏–ª—å–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏ –±–∞—Ç—á–µ–π —Å pin_memory
    
    –†–µ—à–∞–µ—Ç –ø—Ä–æ–±–ª–µ–º—É CUDA error –ø—Ä–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–∏ pin_memory —Å PyTorch 2.9.0.dev
    """
    # –†–∞–∑–¥–µ–ª—è–µ–º –±–∞—Ç—á –Ω–∞ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã
    X_batch = torch.stack([item[0] for item in batch])
    y_batch = torch.stack([item[1] for item in batch])
    
    # –°–æ–±–∏—Ä–∞–µ–º info —Å–ª–æ–≤–∞—Ä—å –±–µ–∑–æ–ø–∞—Å–Ω—ã–º —Å–ø–æ—Å–æ–±–æ–º
    info_batch = {
        'idx': torch.tensor([item[2]['idx'] for item in batch], dtype=torch.long)
    }
    
    return X_batch, y_batch, info_batch


def calculate_sample_weights(dataset: 'PrecomputedDataset', 
                           direction_indices: List[int] = [4, 5, 6, 7],
                           class_weights: List[float] = [2.5, 2.5, 0.3]) -> torch.Tensor:
    """
    –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ—Ç –≤–µ—Å–∞ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Å—ç–º–ø–ª–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –∫–ª–∞—Å—Å–æ–≤ direction
    
    Args:
        dataset: PrecomputedDataset
        direction_indices: –∏–Ω–¥–µ–∫—Å—ã direction –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –≤ targets (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é 4-7)
        class_weights: –≤–µ—Å–∞ –¥–ª—è –∫–ª–∞—Å—Å–æ–≤ [LONG, SHORT, FLAT]
        
    Returns:
        torch.Tensor —Å –≤–µ—Å–æ–º –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Å—ç–º–ø–ª–∞
    """
    logger = get_logger("SampleWeights")
    logger.info("üìä –†–∞—Å—á–µ—Ç –≤–µ—Å–æ–≤ —Å—ç–º–ø–ª–æ–≤ –¥–ª—è –±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∏ –∫–ª–∞—Å—Å–æ–≤...")
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º –≤—Å–µ —Ç–∞—Ä–≥–µ—Ç—ã –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
    all_targets = []
    cache_file = dataset._get_cache_path()
    
    if dataset.use_hdf5 and cache_file.exists():
        with h5py.File(cache_file, 'r') as f:
            targets = f['y'][:]  # (n_samples, 1, n_targets)
            if targets.ndim == 3:
                targets = targets.squeeze(1)  # (n_samples, n_targets)
    else:
        # Fallback –Ω–∞ –æ–±—ã—á–Ω—É—é –∑–∞–≥—Ä—É–∑–∫—É
        for i in range(len(dataset)):
            _, y, _ = dataset[i]
            all_targets.append(y)
        targets = torch.stack(all_targets).numpy()
    
    # –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ–º –≤–µ—Å–∞ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Å—ç–º–ø–ª–∞
    sample_weights = np.ones(len(targets))
    
    # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –∫–∞–∂–¥—ã–π direction –∏–Ω–¥–µ–∫—Å
    for idx in direction_indices:
        if idx < targets.shape[1]:
            directions = targets[:, idx].astype(int)
            
            # –ü–æ–¥—Å—á–µ—Ç –∫–ª–∞—Å—Å–æ–≤
            unique, counts = np.unique(directions, return_counts=True)
            class_dist = {int(cls): cnt for cls, cnt in zip(unique, counts)}
            
            # –ü—Ä–∏–º–µ–Ω—è–µ–º –≤–µ—Å–∞ –∫–ª–∞—Å—Å–æ–≤ –∫ –∫–∞–∂–¥–æ–º—É —Å—ç–º–ø–ª—É
            for i, direction in enumerate(directions):
                if 0 <= direction <= 2:  # LONG=0, SHORT=1, FLAT=2
                    sample_weights[i] *= class_weights[direction]
            
            # –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è
            total = len(directions)
            logger.info(f"   Direction idx {idx}: LONG={class_dist.get(0,0)/total:.1%}, "
                       f"SHORT={class_dist.get(1,0)/total:.1%}, FLAT={class_dist.get(2,0)/total:.1%}")
    
    # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –≤–µ—Å–∞
    sample_weights = sample_weights / sample_weights.mean()
    
    logger.info(f"‚úÖ –í–µ—Å–∞ —Ä–∞—Å—Å—á–∏—Ç–∞–Ω—ã: min={sample_weights.min():.2f}, "
                f"max={sample_weights.max():.2f}, mean={sample_weights.mean():.2f}")
    
    return torch.from_numpy(sample_weights).float()


class PrecomputedDataset(Dataset):
    """Dataset —Å –ø—Ä–µ–¥–≤—ã—á–∏—Å–ª–µ–Ω–Ω—ã–º–∏ –æ–∫–Ω–∞–º–∏ –¥–ª—è –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–π —Å–∫–æ—Ä–æ—Å—Ç–∏ –∑–∞–≥—Ä—É–∑–∫–∏"""
    
    def __init__(self, 
                 data: pd.DataFrame,
                 context_window: int = 168,
                 prediction_window: int = 4,
                 feature_cols: List[str] = None,
                 target_cols: List[str] = None,
                 stride: int = 1,
                 cache_dir: str = "cache/precomputed",
                 dataset_name: str = "train",
                 use_hdf5: bool = True,
                 normalize: bool = True,
                 scaler_path: Optional[str] = None,
                 fit_scaler: bool = False):
        """
        Args:
            data: DataFrame —Å –¥–∞–Ω–Ω—ã–º–∏
            context_window: —Ä–∞–∑–º–µ—Ä –≤—Ö–æ–¥–Ω–æ–≥–æ –æ–∫–Ω–∞
            prediction_window: —Ä–∞–∑–º–µ—Ä –æ–∫–Ω–∞ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
            feature_cols: —Å–ø–∏—Å–æ–∫ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
            target_cols: —Å–ø–∏—Å–æ–∫ —Ü–µ–ª–µ–≤—ã—Ö –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö
            stride: —à–∞–≥ –º–µ–∂–¥—É –æ–∫–Ω–∞–º–∏
            cache_dir: –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –¥–ª—è –∫—ç—à–∞
            dataset_name: –∏–º—è –¥–∞—Ç–∞—Å–µ—Ç–∞ (train/val/test)
            use_hdf5: –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å HDF5 –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è (—ç–∫–æ–Ω–æ–º–∏—è –ø–∞–º—è—Ç–∏)
        """
        self.logger = get_logger("PrecomputedDataset")
        self.context_window = context_window
        self.prediction_window = prediction_window
        self.stride = stride
        self.cache_dir = Path(cache_dir)
        self.cache_dir.mkdir(parents=True, exist_ok=True)
        self.dataset_name = dataset_name
        self.use_hdf5 = use_hdf5
        
        # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –∏ —Ü–µ–ª–µ–≤—ã—Ö –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö
        if feature_cols is None:
            self.feature_cols = [col for col in data.columns 
                               if col not in ['id', 'symbol', 'datetime', 'timestamp', 'sector']
                               and not col.startswith(('target_', 'future_', 'optimal_'))]
        else:
            self.feature_cols = feature_cols
            
        if target_cols is None:
            self.target_cols = [col for col in data.columns 
                              if col.startswith(('target_', 'future_return_', 'long_tp', 'short_tp', 
                                               'long_sl', 'short_sl', 'long_optimal', 'short_optimal',
                                               'best_direction'))]
        else:
            self.target_cols = target_cols
        
        # –°–æ–∑–¥–∞–µ–º –≤—Ä–µ–º–µ–Ω–Ω—ã–π –¥–∞—Ç–∞—Å–µ—Ç –¥–ª—è –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∏ –¥–∞–Ω–Ω—ã—Ö
        self.temp_dataset = TimeSeriesDataset(
            data=data,
            context_window=context_window,
            prediction_window=prediction_window,
            feature_cols=self.feature_cols,
            target_cols=self.target_cols,
            stride=stride,
            normalize=normalize,
            scaler_path=scaler_path,
            fit_scaler=fit_scaler
        )
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –∫—ç—à–∞
        cache_file = self._get_cache_path()
        
        if cache_file.exists():
            self.logger.info(f"‚úÖ –ó–∞–≥—Ä—É–∑–∫–∞ –ø—Ä–µ–¥–≤—ã—á–∏—Å–ª–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –∏–∑ {cache_file}")
            self._load_cache(cache_file)
        else:
            self.logger.info(f"üìä –ü—Ä–µ–¥–≤—ã—á–∏—Å–ª–µ–Ω–∏–µ –≤—Å–µ—Ö –æ–∫–æ–Ω –¥–ª—è {dataset_name}...")
            self._precompute_all_windows()
            self.logger.info(f"üíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ –∫—ç—à: {cache_file}")
            self._save_cache(cache_file)
        
        self.logger.info(f"‚úÖ PrecomputedDataset –≥–æ—Ç–æ–≤: {len(self)} –ø—Ä–∏–º–µ—Ä–æ–≤")
    
    def _get_cache_path(self) -> Path:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –ø—É—Ç–∏ –∫ —Ñ–∞–π–ª—É –∫—ç—à–∞"""
        cache_name = f"{self.dataset_name}_w{self.context_window}_s{self.stride}"
        if self.use_hdf5:
            return self.cache_dir / f"{cache_name}.h5"
        else:
            return self.cache_dir / f"{cache_name}.pkl"
    
    def _precompute_all_windows(self):
        """–ü—Ä–µ–¥–≤—ã—á–∏—Å–ª–µ–Ω–∏–µ –≤—Å–µ—Ö –æ–∫–æ–Ω"""
        n_samples = len(self.temp_dataset)
        
        if self.use_hdf5:
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º HDF5 –¥–ª—è —ç–∫–æ–Ω–æ–º–∏–∏ –ø–∞–º—è—Ç–∏
            cache_file = self._get_cache_path()
            
            # –ü–æ–ª—É—á–∞–µ–º —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏ –∏–∑ –ø–µ—Ä–≤–æ–≥–æ –ø—Ä–∏–º–µ—Ä–∞
            X_sample, y_sample, _ = self.temp_dataset[0]
            X_shape = (n_samples,) + X_sample.shape
            y_shape = (n_samples,) + y_sample.shape
            
            with h5py.File(cache_file, 'w') as f:
                # –°–æ–∑–¥–∞–µ–º –¥–∞—Ç–∞—Å–µ—Ç—ã
                X_dataset = f.create_dataset('X', shape=X_shape, dtype='float32')
                y_dataset = f.create_dataset('y', shape=y_shape, dtype='float32')
                
                # –ó–∞–ø–æ–ª–Ω—è–µ–º –¥–∞–Ω–Ω—ã–º–∏ —Å –ø—Ä–æ–≥—Ä–µ—Å—Å-–±–∞—Ä–æ–º
                for i in tqdm(range(n_samples), desc="–ü—Ä–µ–¥–≤—ã—á–∏—Å–ª–µ–Ω–∏–µ –æ–∫–æ–Ω"):
                    X, y, info = self.temp_dataset[i]
                    X_dataset[i] = X.numpy()
                    y_dataset[i] = y.numpy()
                    
                    # –ü–µ—Ä–∏–æ–¥–∏—á–µ—Å–∫–∞—è –æ—á–∏—Å—Ç–∫–∞ –ø–∞–º—è—Ç–∏
                    if i % 10000 == 0:
                        gc.collect()
            
            # –û—Ç–∫—Ä—ã–≤–∞–µ–º —Ñ–∞–π–ª –¥–ª—è —á—Ç–µ–Ω–∏—è
            self.h5_file = h5py.File(cache_file, 'r')
            self.X_data = self.h5_file['X']
            self.y_data = self.h5_file['y']
            
        else:
            # –ó–∞–≥—Ä—É–∂–∞–µ–º –≤—Å–µ –≤ –ø–∞–º—è—Ç—å (–±—ã—Å—Ç—Ä–µ–µ, –Ω–æ —Ç—Ä–µ–±—É–µ—Ç –±–æ–ª—å—à–µ RAM)
            self.logger.info("‚ö†Ô∏è –ó–∞–≥—Ä—É–∑–∫–∞ –≤—Å–µ—Ö –¥–∞–Ω–Ω—ã—Ö –≤ –ø–∞–º—è—Ç—å...")
            
            X_list = []
            y_list = []
            
            # –ü—Ä–µ–¥–≤—ã—á–∏—Å–ª—è–µ–º –≤—Å–µ –æ–∫–Ω–∞
            for i in tqdm(range(n_samples), desc="–ü—Ä–µ–¥–≤—ã—á–∏—Å–ª–µ–Ω–∏–µ –æ–∫–æ–Ω"):
                X, y, info = self.temp_dataset[i]
                X_list.append(X.numpy())
                y_list.append(y.numpy())
                
                # –ü–µ—Ä–∏–æ–¥–∏—á–µ—Å–∫–∞—è –æ—á–∏—Å—Ç–∫–∞ –ø–∞–º—è—Ç–∏
                if i % 10000 == 0:
                    gc.collect()
            
            # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ numpy –º–∞—Å—Å–∏–≤—ã
            self.X_data = np.stack(X_list)
            self.y_data = np.stack(y_list)
            
            # –û—á–∏—Å—Ç–∫–∞ –ø–∞–º—è—Ç–∏
            del X_list, y_list
            gc.collect()
    
    def _save_cache(self, cache_file: Path):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∫—ç—à–∞"""
        if not self.use_hdf5:
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º pickle
            cache_data = {
                'X': self.X_data,
                'y': self.y_data,
                'feature_cols': self.feature_cols,
                'target_cols': self.target_cols,
                'context_window': self.context_window,
                'prediction_window': self.prediction_window,
                'stride': self.stride
            }
            
            with open(cache_file, 'wb') as f:
                pickle.dump(cache_data, f, protocol=pickle.HIGHEST_PROTOCOL)
    
    def _load_cache(self, cache_file: Path):
        """–ó–∞–≥—Ä—É–∑–∫–∞ –∫—ç—à–∞"""
        if self.use_hdf5:
            # –û—Ç–∫—Ä—ã–≤–∞–µ–º HDF5 —Ñ–∞–π–ª
            self.h5_file = h5py.File(cache_file, 'r')
            self.X_data = self.h5_file['X']
            self.y_data = self.h5_file['y']
        else:
            # –ó–∞–≥—Ä—É–∂–∞–µ–º pickle
            with open(cache_file, 'rb') as f:
                cache_data = pickle.load(f)
            
            self.X_data = cache_data['X']
            self.y_data = cache_data['y']
    
    def __len__(self):
        return len(self.X_data)
    
    def __getitem__(self, idx):
        """–ë—ã—Å—Ç—Ä–æ–µ –ø–æ–ª—É—á–µ–Ω–∏–µ –ø—Ä–µ–¥–≤—ã—á–∏—Å–ª–µ–Ω–Ω–æ–≥–æ –ø—Ä–∏–º–µ—Ä–∞"""
        # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤ —Ç–µ–Ω–∑–æ—Ä—ã
        X = torch.FloatTensor(self.X_data[idx])
        y = torch.FloatTensor(self.y_data[idx])
        
        # –ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏
        info = {
            'idx': idx
        }
        
        return X, y, info
    
    def __del__(self):
        """–ó–∞–∫—Ä—ã—Ç–∏–µ HDF5 —Ñ–∞–π–ª–∞ –ø—Ä–∏ —É–¥–∞–ª–µ–Ω–∏–∏ –æ–±—ä–µ–∫—Ç–∞"""
        if hasattr(self, 'h5_file') and self.h5_file is not None:
            self.h5_file.close()


def create_precomputed_data_loaders(train_data: pd.DataFrame,
                                   val_data: pd.DataFrame,
                                   test_data: pd.DataFrame,
                                   config: Dict,
                                   feature_cols: List[str] = None,
                                   target_cols: List[str] = None) -> Tuple[torch.utils.data.DataLoader, 
                                                                           torch.utils.data.DataLoader, 
                                                                           torch.utils.data.DataLoader]:
    """–°–æ–∑–¥–∞–Ω–∏–µ DataLoader'–æ–≤ —Å –ø—Ä–µ–¥–≤—ã—á–∏—Å–ª–µ–Ω–Ω—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏ –¥–ª—è –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–π —Å–∫–æ—Ä–æ—Å—Ç–∏"""
    
    logger = get_logger("PrecomputedDataLoaders")
    
    batch_size = config['model']['batch_size']
    context_window = config['model']['context_window']
    pred_window = config['model']['pred_len']
    num_workers = config['performance']['num_workers']
    persistent_workers = config['performance'].get('persistent_workers', True) if num_workers > 0 else False
    prefetch_factor = config['performance'].get('prefetch_factor', 2)
    
    # –ü–æ–ª—É—á–∞–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –∏–∑ –∫–æ–Ω—Ñ–∏–≥–∞
    normalize = config.get('data', {}).get('normalize', True)
    scaler_path = config.get('data', {}).get('scaler_path', 'models_saved/data_scaler.pkl')
    pin_memory = config['performance'].get('dataloader_pin_memory', True)
    drop_last = config['performance'].get('dataloader_drop_last', True)
    
    # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã stride
    train_stride = config.get('data', {}).get('train_stride', 1)
    val_stride = config.get('data', {}).get('val_stride', 4)
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞–ª–∏—á–∏—è scaler
    from pathlib import Path
    scaler_exists = Path(scaler_path).exists()
    
    logger.info("üöÄ –°–æ–∑–¥–∞–Ω–∏–µ PrecomputedDataset –¥–ª—è –±—ã—Å—Ç—Ä–æ–π –∑–∞–≥—Ä—É–∑–∫–∏...")
    
    # –°–æ–∑–¥–∞–Ω–∏–µ –¥–∞—Ç–∞—Å–µ—Ç–æ–≤
    train_dataset = PrecomputedDataset(
        data=train_data,
        context_window=context_window,
        prediction_window=pred_window,
        feature_cols=feature_cols,
        target_cols=target_cols,
        stride=train_stride,
        dataset_name="train",
        use_hdf5=True,  # –ò—Å–ø–æ–ª—å–∑—É–µ–º HDF5 –¥–ª—è –±–æ–ª—å—à–∏—Ö –¥–∞–Ω–Ω—ã—Ö
        normalize=normalize,
        scaler_path=scaler_path,
        fit_scaler=not scaler_exists
    )
    
    val_dataset = PrecomputedDataset(
        data=val_data,
        context_window=context_window,
        prediction_window=pred_window,
        feature_cols=feature_cols,
        target_cols=target_cols,
        stride=val_stride,
        dataset_name="val",
        use_hdf5=True,
        normalize=normalize,
        scaler_path=scaler_path,
        fit_scaler=False
    )
    
    test_dataset = PrecomputedDataset(
        data=test_data,
        context_window=context_window,
        prediction_window=pred_window,
        feature_cols=feature_cols,
        target_cols=target_cols,
        stride=4,  # –§–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã–π stride –¥–ª—è —Ç–µ—Å—Ç–∞
        dataset_name="test",
        use_hdf5=True,
        normalize=normalize,
        scaler_path=scaler_path,
        fit_scaler=False
    )
    
    logger.info(f"üìä –†–∞–∑–º–µ—Ä—ã –ø—Ä–µ–¥–≤—ã—á–∏—Å–ª–µ–Ω–Ω—ã—Ö –¥–∞—Ç–∞—Å–µ—Ç–æ–≤:")
    logger.info(f"   - Train: {len(train_dataset):,} –æ–∫–æ–Ω")
    logger.info(f"   - Val: {len(val_dataset):,} –æ–∫–æ–Ω")
    logger.info(f"   - Test: {len(test_dataset):,} –æ–∫–æ–Ω")
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω—É–∂–Ω–æ –ª–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å WeightedRandomSampler
    use_weighted_sampling = config.get('loss', {}).get('use_weighted_sampling', True)
    
    # –°–æ–∑–¥–∞–Ω–∏–µ DataLoader'–æ–≤
    if use_weighted_sampling:
        logger.info("‚öñÔ∏è –ò—Å–ø–æ–ª—å–∑—É–µ–º WeightedRandomSampler –¥–ª—è –±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∏ –∫–ª–∞—Å—Å–æ–≤...")
        
        # –ü–æ–ª—É—á–∞–µ–º –≤–µ—Å–∞ –∫–ª–∞—Å—Å–æ–≤ –∏–∑ –∫–æ–Ω—Ñ–∏–≥–∞
        class_weights = config.get('loss', {}).get('class_weights', [2.5, 2.5, 0.3])
        
        # –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ–º –≤–µ—Å–∞ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Å—ç–º–ø–ª–∞
        sample_weights = calculate_sample_weights(train_dataset, class_weights=class_weights)
        
        # –°–æ–∑–¥–∞–µ–º sampler
        sampler = WeightedRandomSampler(
            weights=sample_weights,
            num_samples=len(sample_weights),
            replacement=True
        )
        
        train_loader = torch.utils.data.DataLoader(
            train_dataset,
            batch_size=batch_size,
            sampler=sampler,  # –ò—Å–ø–æ–ª—å–∑—É–µ–º sampler –≤–º–µ—Å—Ç–æ shuffle
            num_workers=num_workers,
            pin_memory=pin_memory,
            drop_last=drop_last,
            persistent_workers=persistent_workers,
            prefetch_factor=prefetch_factor if num_workers > 0 else None,
            collate_fn=custom_collate_fn
        )
    else:
        train_loader = torch.utils.data.DataLoader(
            train_dataset,
            batch_size=batch_size,
            shuffle=True,
            num_workers=num_workers,
            pin_memory=pin_memory,
            drop_last=drop_last,
            persistent_workers=persistent_workers,
            prefetch_factor=prefetch_factor if num_workers > 0 else None,
            collate_fn=custom_collate_fn
        )
    
    val_loader = torch.utils.data.DataLoader(
        val_dataset,
        batch_size=batch_size,
        shuffle=False,
        num_workers=num_workers,
        pin_memory=pin_memory,
        drop_last=drop_last,
        persistent_workers=persistent_workers,
        prefetch_factor=prefetch_factor if num_workers > 0 else None,
        collate_fn=custom_collate_fn  # –ò—Å–ø–æ–ª—å–∑—É–µ–º –∫–∞—Å—Ç–æ–º–Ω—É—é —Ñ—É–Ω–∫—Ü–∏—é –¥–ª—è pin_memory
    )
    
    test_loader = torch.utils.data.DataLoader(
        test_dataset,
        batch_size=batch_size,
        shuffle=False,
        num_workers=num_workers,
        pin_memory=pin_memory,
        drop_last=False,
        persistent_workers=persistent_workers,
        prefetch_factor=prefetch_factor if num_workers > 0 else None,
        collate_fn=custom_collate_fn  # –ò—Å–ø–æ–ª—å–∑—É–µ–º –∫–∞—Å—Ç–æ–º–Ω—É—é —Ñ—É–Ω–∫—Ü–∏—é –¥–ª—è pin_memory
    )
    
    logger.info("‚úÖ PrecomputedDataLoader'—ã —Å–æ–∑–¥–∞–Ω—ã —É—Å–ø–µ—à–Ω–æ!")
    
    return train_loader, val_loader, test_loader