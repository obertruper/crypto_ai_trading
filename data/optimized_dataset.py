"""
–û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π Dataset –¥–ª—è –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–π –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ GPU
"""

import torch
from torch.utils.data import Dataset
import pandas as pd
import numpy as np
from typing import Dict, List, Optional, Tuple
import pickle
from pathlib import Path

from utils.logger import get_logger

class OptimizedTimeSeriesDataset(Dataset):
    """–û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π Dataset —Å –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω—ã–º –≤—ã—á–∏—Å–ª–µ–Ω–∏–µ–º –∫–≤–∞–Ω—Ç–∏–ª–µ–π"""
    
    def __init__(self, 
                 data: pd.DataFrame,
                 context_window: int = 168,
                 prediction_window: int = 4,
                 feature_cols: List[str] = None,
                 target_cols: List[str] = None,
                 stride: int = 1,
                 normalize: bool = True,
                 scaler_path: Optional[str] = None,
                 fit_scaler: bool = False,
                 precompute_quantiles: bool = True):
        
        self.logger = get_logger("OptimizedDataset")
        self.data = data.sort_values(['symbol', 'datetime']).reset_index(drop=True)
        self.context_window = context_window
        self.prediction_window = prediction_window
        self.target_window = prediction_window
        self.stride = stride
        
        # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
        if feature_cols is None:
            self.feature_cols = [col for col in data.columns 
                               if col not in ['id', 'symbol', 'datetime', 'timestamp', 'sector']
                               and not col.startswith(('target_', 'future_', 'optimal_'))]
        else:
            self.feature_cols = feature_cols
            
        if target_cols is None:
            self.target_cols = [col for col in data.columns 
                              if col.startswith(('target_', 'future_return_', 'long_tp', 'short_tp', 
                                               'long_sl', 'short_sl', 'long_optimal', 'short_optimal',
                                               'best_direction', 'signal_strength'))]
        else:
            self.target_cols = target_cols
        
        # –°–æ–∑–¥–∞–Ω–∏–µ –∏–Ω–¥–µ–∫—Å–æ–≤
        self._create_indices()
        
        # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏
        self.normalize = normalize
        self.scaler = None
        self.volume_based_cols = []
        self.price_based_cols = []
        self.ratio_cols = []
        
        # –û–ü–¢–ò–ú–ò–ó–ê–¶–ò–Ø: –ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö –≤ numpy –∑–∞—Ä–∞–Ω–µ–µ
        self.logger.info("üìä –ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö –≤ numpy arrays...")
        self.feature_data = self.data[self.feature_cols].values.astype(np.float32)
        self.target_data = self.data[self.target_cols].values.astype(np.float32) if self.target_cols else None
        
        # –û–ü–¢–ò–ú–ò–ó–ê–¶–ò–Ø: –ü—Ä–µ–¥–≤—ã—á–∏—Å–ª–µ–Ω–∏–µ –∫–≤–∞–Ω—Ç–∏–ª–µ–π
        self.quantiles = {}
        if normalize and precompute_quantiles:
            self.logger.info("üöÄ –ü—Ä–µ–¥–≤—ã—á–∏—Å–ª–µ–Ω–∏–µ –∫–≤–∞–Ω—Ç–∏–ª–µ–π –¥–ª—è –≤—Å–µ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞...")
            self._precompute_quantiles()
        
        if self.normalize:
            self._setup_normalization(scaler_path, fit_scaler)
        
        # –ö—ç—à –¥–ª—è —á–∞—Å—Ç–æ –∏—Å–ø–æ–ª—å–∑—É–µ–º—ã—Ö –¥–∞–Ω–Ω—ã—Ö
        self.cache = {}
        self.cache_size = 1000  # –ö—ç—à–∏—Ä–æ–≤–∞—Ç—å –ø–æ—Å–ª–µ–¥–Ω–∏–µ 1000 —ç–ª–µ–º–µ–Ω—Ç–æ–≤
        
        self.logger.info(f"‚úÖ OptimizedDataset —Å–æ–∑–¥–∞–Ω: {len(self)} –ø—Ä–∏–º–µ—Ä–æ–≤")
    
    def _precompute_quantiles(self):
        """–ü—Ä–µ–¥–≤—ã—á–∏—Å–ª–µ–Ω–∏–µ –∫–≤–∞–Ω—Ç–∏–ª–µ–π –¥–ª—è –≤—Å–µ—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤"""
        # –°–Ω–∞—á–∞–ª–∞ –æ–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–∏–ø—ã –∫–æ–ª–æ–Ω–æ–∫
        self._identify_column_types()
        
        for i, col in enumerate(self.feature_cols):
            if col not in self.ratio_cols:  # –ù–µ –≤—ã—á–∏—Å–ª—è–µ–º –¥–ª—è ratio –∫–æ–ª–æ–Ω–æ–∫
                col_data = self.data[col].values
                # –ü—Ä–∏–º–µ–Ω—è–µ–º log —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏—é –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
                if col in self.volume_based_cols:
                    col_data = np.log1p(np.clip(col_data, 0, None))
                
                # –í—ã—á–∏—Å–ª—è–µ–º –∫–≤–∞–Ω—Ç–∏–ª–∏ –æ–¥–∏–Ω —Ä–∞–∑ –¥–ª—è –≤—Å–µ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞
                self.quantiles[col] = {
                    'q99': np.percentile(col_data, 99),
                    'q01': np.percentile(col_data, 1)
                }
    
    def _identify_column_types(self):
        """–û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ç–∏–ø–æ–≤ –∫–æ–ª–æ–Ω–æ–∫ –¥–ª—è —Å–ø–µ—Ü–∏–∞–ª—å–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏"""
        volume_keywords = ['volume', 'quote_volume', 'buy_volume', 'sell_volume', 'taker_volume']
        price_keywords = ['price', 'high', 'low', 'open', 'close', 'vwap']
        ratio_keywords = ['ratio', 'pct', 'percent', 'position', '_norm', 'consensus']
        
        for col in self.feature_cols:
            col_lower = col.lower()
            if any(keyword in col_lower for keyword in volume_keywords):
                self.volume_based_cols.append(col)
            elif any(keyword in col_lower for keyword in price_keywords):
                self.price_based_cols.append(col)
            elif any(keyword in col_lower for keyword in ratio_keywords):
                self.ratio_cols.append(col)
    
    def _create_indices(self):
        """–°–æ–∑–¥–∞–Ω–∏–µ –∏–Ω–¥–µ–∫—Å–æ–≤ –¥–ª—è –≤–∞–ª–∏–¥–Ω—ã—Ö –æ–∫–æ–Ω"""
        self.indices = []
        
        for symbol in self.data['symbol'].unique():
            symbol_data = self.data[self.data['symbol'] == symbol]
            symbol_indices = symbol_data.index.tolist()
            
            for i in range(0, len(symbol_indices) - self.context_window - self.prediction_window + 1, self.stride):
                window_indices = symbol_indices[i:i + self.context_window + self.prediction_window]
                
                if all(window_indices[j+1] - window_indices[j] == 1 for j in range(len(window_indices)-1)):
                    self.indices.append({
                        'symbol': symbol,
                        'start_idx': window_indices[0],
                        'context_end_idx': window_indices[self.context_window - 1],
                        'target_end_idx': window_indices[-1]
                    })
    
    def _setup_normalization(self, scaler_path, fit_scaler):
        """–ù–∞—Å—Ç—Ä–æ–π–∫–∞ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏"""
        from sklearn.preprocessing import RobustScaler
        
        # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ç–∏–ø–æ–≤ –∫–æ–ª–æ–Ω–æ–∫ –µ—Å–ª–∏ –µ—â–µ –Ω–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω—ã
        if not self.volume_based_cols and not self.price_based_cols and not self.ratio_cols:
            self._identify_column_types()
        
        if scaler_path and Path(scaler_path).exists() and not fit_scaler:
            with open(scaler_path, 'rb') as f:
                self.scaler = pickle.load(f)
        else:
            self.scaler = RobustScaler()
            if fit_scaler:
                # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è fit
                fit_data = self.feature_data.copy()
                
                # Log —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏—è –¥–ª—è –æ–±—ä–µ–º–Ω—ã—Ö –∫–æ–ª–æ–Ω–æ–∫
                for i, col in enumerate(self.feature_cols):
                    if col in self.volume_based_cols:
                        fit_data[:, i] = np.log1p(np.clip(fit_data[:, i], 0, None))
                
                # Fit scaler
                self.scaler.fit(fit_data)
                
                if scaler_path:
                    Path(scaler_path).parent.mkdir(exist_ok=True)
                    with open(scaler_path, 'wb') as f:
                        pickle.dump(self.scaler, f)
    
    def __len__(self):
        return len(self.indices)
    
    def __getitem__(self, idx):
        """–û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ –ø–æ–ª—É—á–µ–Ω–∏–µ —ç–ª–µ–º–µ–Ω—Ç–∞"""
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –∫—ç—à–∞
        if idx in self.cache:
            return self.cache[idx]
        
        index_info = self.indices[idx]
        
        # –ü–æ–ª—É—á–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö —á–µ—Ä–µ–∑ numpy slicing (–±—ã—Å—Ç—Ä–µ–µ —á–µ–º pandas)
        start_idx = index_info['start_idx']
        end_idx = index_info['context_end_idx'] + 1
        
        # –ë—ã—Å—Ç—Ä–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö
        feature_values = self.feature_data[start_idx:end_idx].copy()
        
        # –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏
        if self.normalize and self.scaler is not None:
            # Log —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏—è –¥–ª—è –æ–±—ä–µ–º–Ω—ã—Ö –∫–æ–ª–æ–Ω–æ–∫
            for i, col in enumerate(self.feature_cols):
                if col in self.volume_based_cols:
                    feature_values[:, i] = np.log1p(np.clip(feature_values[:, i], 0, None))
            
            # –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –ø—Ä–µ–¥–≤—ã—á–∏—Å–ª–µ–Ω–Ω—ã—Ö –∫–≤–∞–Ω—Ç–∏–ª–µ–π
            if self.quantiles:
                for i, col in enumerate(self.feature_cols):
                    if col in self.quantiles:
                        q_data = self.quantiles[col]
                        feature_values[:, i] = np.clip(feature_values[:, i], 
                                                      q_data['q01'], q_data['q99'])
            
            # –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ scaler
            feature_values = self.scaler.transform(feature_values)
            feature_values = np.clip(feature_values, -10, 10)
        
        # –ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è –≤ —Ç–µ–Ω–∑–æ—Ä
        X = torch.from_numpy(feature_values).float()
        
        # –û–±—Ä–∞–±–æ—Ç–∫–∞ —Ü–µ–ª–µ–≤—ã—Ö –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö
        if self.target_data is not None:
            target_idx = index_info['context_end_idx']
            y_values = self.target_data[target_idx].copy()
            
            # –°–ø–µ—Ü–∏–∞–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–ª—è best_direction
            best_dir_idx = None
            for i, col in enumerate(self.target_cols):
                if col == 'best_direction':
                    best_dir_idx = i
                    # –ü—Ä–µ–¥–ø–æ–ª–∞–≥–∞–µ–º, —á—Ç–æ –∑–Ω–∞—á–µ–Ω–∏—è —É–∂–µ —á–∏—Å–ª–æ–≤—ã–µ (0, 1, 2)
                    break
            
            y = torch.from_numpy(y_values).float().unsqueeze(0)
        else:
            y = torch.empty(0)
        
        # –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è (–º–∏–Ω–∏–º–∞–ª—å–Ω–∞—è –¥–ª—è —Å–∫–æ—Ä–æ—Å—Ç–∏)
        info = {
            'symbol': index_info['symbol'],
            'idx': idx
        }
        
        # –ö—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ
        if len(self.cache) >= self.cache_size:
            # –£–¥–∞–ª—è–µ–º —Å—Ç–∞—Ä—ã–µ —ç–ª–µ–º–µ–Ω—Ç—ã
            old_keys = list(self.cache.keys())[:100]
            for k in old_keys:
                del self.cache[k]
        
        self.cache[idx] = (X, y, info)
        
        return X, y, info


def create_optimized_dataloaders(train_data, val_data, test_data, config, logger):
    """–°–æ–∑–¥–∞–Ω–∏–µ –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö DataLoader'–æ–≤"""
    from torch.utils.data import DataLoader
    
    batch_size = config['model']['batch_size']
    num_workers = 4  # –û–ø—Ç–∏–º–∞–ª—å–Ω–æ –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ –¥–∞–Ω–Ω—ã—Ö
    
    # –°–æ–∑–¥–∞–Ω–∏–µ –¥–∞—Ç–∞—Å–µ—Ç–æ–≤
    train_dataset = OptimizedTimeSeriesDataset(
        train_data,
        context_window=config['model']['context_window'],
        feature_cols=config.get('feature_cols'),
        target_cols=config.get('target_cols'),
        normalize=True,
        fit_scaler=True,
        scaler_path='cache/scaler.pkl'
    )
    
    val_dataset = OptimizedTimeSeriesDataset(
        val_data,
        context_window=config['model']['context_window'],
        feature_cols=train_dataset.feature_cols,
        target_cols=train_dataset.target_cols,
        normalize=True,
        scaler_path='cache/scaler.pkl'
    )
    
    test_dataset = OptimizedTimeSeriesDataset(
        test_data,
        context_window=config['model']['context_window'],
        feature_cols=train_dataset.feature_cols,
        target_cols=train_dataset.target_cols,
        normalize=True,
        scaler_path='cache/scaler.pkl'
    )
    
    # –°–æ–∑–¥–∞–Ω–∏–µ DataLoader'–æ–≤ —Å –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è–º–∏
    train_loader = DataLoader(
        train_dataset,
        batch_size=batch_size,
        shuffle=True,
        num_workers=num_workers,
        pin_memory=True,  # –í–∞–∂–Ω–æ –¥–ª—è GPU!
        persistent_workers=True,  # –ü–µ—Ä–µ–∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –≤–æ—Ä–∫–µ—Ä–æ–≤
        prefetch_factor=2,  # –ü—Ä–µ–¥–∑–∞–≥—Ä—É–∑–∫–∞
        drop_last=True
    )
    
    val_loader = DataLoader(
        val_dataset,
        batch_size=batch_size,
        shuffle=False,
        num_workers=num_workers,
        pin_memory=True,
        persistent_workers=True,
        prefetch_factor=2
    )
    
    test_loader = DataLoader(
        test_dataset,
        batch_size=batch_size,
        shuffle=False,
        num_workers=num_workers,
        pin_memory=True,
        persistent_workers=True,
        prefetch_factor=2
    )
    
    logger.info(f"‚úÖ –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ DataLoader'—ã —Å–æ–∑–¥–∞–Ω—ã:")
    logger.info(f"   - num_workers: {num_workers}")
    logger.info(f"   - pin_memory: True")
    logger.info(f"   - prefetch_factor: 2")
    logger.info(f"   - persistent_workers: True")
    
    return train_loader, val_loader, test_loader