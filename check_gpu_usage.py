#!/usr/bin/env python3
"""
–ü—Ä–æ–≤–µ—Ä–∫–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è GPU –≤–æ –≤—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è
"""

import torch
import yaml
from pathlib import Path
import traceback

from utils.logger import get_logger
from models.patchtst_unified import create_unified_model
from main import load_cached_data_if_exists, create_unified_data_loaders

def check_gpu_setup():
    """–ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ GPU"""
    logger = get_logger("GPUCheck")
    
    logger.info("üîç –ü—Ä–æ–≤–µ—Ä–∫–∞ GPU...")
    logger.info(f"CUDA –¥–æ—Å—Ç—É–ø–Ω–∞: {torch.cuda.is_available()}")
    
    if torch.cuda.is_available():
        logger.info(f"–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ GPU: {torch.cuda.device_count()}")
        logger.info(f"–¢–µ–∫—É—â–∏–π GPU: {torch.cuda.current_device()}")
        logger.info(f"GPU –∏–º—è: {torch.cuda.get_device_name(0)}")
        logger.info(f"GPU –ø–∞–º—è—Ç—å: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB")
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –≤—ã–¥–µ–ª–µ–Ω–Ω–æ–π –ø–∞–º—è—Ç–∏
        logger.info(f"–í—ã–¥–µ–ª–µ–Ω–æ –ø–∞–º—è—Ç–∏: {torch.cuda.memory_allocated() / 1024**3:.2f} GB")
        logger.info(f"–ó–∞—Ä–µ–∑–µ—Ä–≤–∏—Ä–æ–≤–∞–Ω–æ –ø–∞–º—è—Ç–∏: {torch.cuda.memory_reserved() / 1024**3:.2f} GB")
    else:
        logger.error("‚ùå CUDA –Ω–µ –¥–æ—Å—Ç—É–ø–Ω–∞!")
        return False
    
    return True

def check_model_on_gpu():
    """–ü—Ä–æ–≤–µ—Ä–∫–∞ —á—Ç–æ –º–æ–¥–µ–ª—å –Ω–∞ GPU"""
    logger = get_logger("GPUCheck")
    
    try:
        # –ó–∞–≥—Ä—É–∑–∫–∞ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
        with open('config/config.yaml', 'r') as f:
            config = yaml.safe_load(f)
        
        # –°–æ–∑–¥–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏
        logger.info("üèóÔ∏è –°–æ–∑–¥–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏...")
        config['model']['name'] = 'UnifiedPatchTST'
        config['model']['output_size'] = 36
        model = create_unified_model(config)
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞ –º–æ–¥–µ–ª–∏
        device = next(model.parameters()).device
        logger.info(f"üìç –ú–æ–¥–µ–ª—å –Ω–∞ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–µ: {device}")
        
        if device.type != 'cuda':
            logger.warning("‚ö†Ô∏è –ú–æ–¥–µ–ª—å –ù–ï –Ω–∞ GPU! –ü–µ—Ä–µ–º–µ—â–∞–µ–º...")
            model = model.cuda()
            device = next(model.parameters()).device
            logger.info(f"‚úÖ –ú–æ–¥–µ–ª—å –ø–µ—Ä–µ–º–µ—â–µ–Ω–∞ –Ω–∞: {device}")
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –≤—Å–µ—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
        all_on_gpu = all(p.is_cuda for p in model.parameters())
        logger.info(f"–í—Å–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –Ω–∞ GPU: {all_on_gpu}")
        
        return True
        
    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø—Ä–æ–≤–µ—Ä–∫–µ –º–æ–¥–µ–ª–∏: {e}")
        traceback.print_exc()
        return False

def check_data_loading():
    """–ü—Ä–æ–≤–µ—Ä–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –¥–∞–Ω–Ω—ã—Ö –Ω–∞ GPU"""
    logger = get_logger("GPUCheck")
    
    try:
        # –ó–∞–≥—Ä—É–∑–∫–∞ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
        with open('config/config.yaml', 'r') as f:
            config = yaml.safe_load(f)
        
        # –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö
        logger.info("üì• –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö...")
        train_data, val_data, test_data, feature_cols, target_cols = load_cached_data_if_exists(logger)
        
        if train_data is None:
            logger.error("‚ùå –î–∞–Ω–Ω—ã–µ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã!")
            return False
        
        # –°–æ–∑–¥–∞–Ω–∏–µ DataLoader
        logger.info("üîÑ –°–æ–∑–¥–∞–Ω–∏–µ DataLoader...")
        train_loader, val_loader, test_loader, config_updated = create_unified_data_loaders(
            train_data, val_data, test_data, feature_cols, target_cols, config, logger
        )
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø–µ—Ä–≤–æ–≥–æ –±–∞—Ç—á–∞
        logger.info("üîç –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø–µ—Ä–≤–æ–≥–æ –±–∞—Ç—á–∞...")
        batch = next(iter(train_loader))
        X, y, info = batch
        
        logger.info(f"üìä –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏ –±–∞—Ç—á–∞:")
        logger.info(f"   X: {X.shape}, —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {X.device}")
        logger.info(f"   y: {y.shape}, —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {y.device}")
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø–µ—Ä–µ–º–µ—â–µ–Ω–∏—è –Ω–∞ GPU
        logger.info("üöÄ –ü–µ—Ä–µ–º–µ—â–µ–Ω–∏–µ –Ω–∞ GPU...")
        X_gpu = X.cuda()
        y_gpu = y.cuda()
        
        logger.info(f"   X –Ω–∞ GPU: {X_gpu.device}")
        logger.info(f"   y –Ω–∞ GPU: {y_gpu.device}")
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø–∞–º—è—Ç–∏ –ø–æ—Å–ª–µ –∑–∞–≥—Ä—É–∑–∫–∏ –±–∞—Ç—á–∞
        logger.info(f"üíæ –ü–∞–º—è—Ç—å –ø–æ—Å–ª–µ –∑–∞–≥—Ä—É–∑–∫–∏ –±–∞—Ç—á–∞:")
        logger.info(f"   –í—ã–¥–µ–ª–µ–Ω–æ: {torch.cuda.memory_allocated() / 1024**3:.2f} GB")
        logger.info(f"   –ó–∞—Ä–µ–∑–µ—Ä–≤–∏—Ä–æ–≤–∞–Ω–æ: {torch.cuda.memory_reserved() / 1024**3:.2f} GB")
        
        return True
        
    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø—Ä–æ–≤–µ—Ä–∫–µ –¥–∞–Ω–Ω—ã—Ö: {e}")
        traceback.print_exc()
        return False

def run_mini_training():
    """–ó–∞–ø—É—Å–∫ –º–∏–Ω–∏-–æ–±—É—á–µ–Ω–∏—è –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ GPU"""
    logger = get_logger("GPUCheck")
    
    try:
        from training.trainer import Trainer
        
        # –ó–∞–≥—Ä—É–∑–∫–∞ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
        with open('config/config.yaml', 'r') as f:
            config = yaml.safe_load(f)
        
        # –°–æ–∑–¥–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏
        config['model']['name'] = 'UnifiedPatchTST'
        config['model']['output_size'] = 36
        config['model']['epochs'] = 1  # –¢–æ–ª—å–∫–æ 1 —ç–ø–æ—Ö–∞ –¥–ª—è —Ç–µ—Å—Ç–∞
        model = create_unified_model(config)
        
        # –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö
        train_data, val_data, test_data, feature_cols, target_cols = load_cached_data_if_exists(logger)
        train_loader, val_loader, test_loader, config_updated = create_unified_data_loaders(
            train_data, val_data, test_data, feature_cols, target_cols, config, logger
        )
        
        # –°–æ–∑–¥–∞–Ω–∏–µ —Ç—Ä–µ–π–Ω–µ—Ä–∞
        trainer = Trainer(model, config_updated)
        
        logger.info(f"üéØ –£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ —Ç—Ä–µ–π–Ω–µ—Ä–∞: {trainer.device}")
        logger.info(f"üî• –ú–æ–¥–µ–ª—å –Ω–∞ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–µ: {next(trainer.model.parameters()).device}")
        
        # –û–±—É—á–µ–Ω–∏–µ –Ω–∞ 10 –±–∞—Ç—á–∞—Ö
        logger.info("üöÄ –ó–∞–ø—É—Å–∫ —Ç–µ—Å—Ç–æ–≤–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è...")
        
        model.train()
        for i, (X, y, info) in enumerate(train_loader):
            if i >= 10:
                break
            
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ —É—Å—Ç—Ä–æ–π—Å—Ç–≤
            logger.info(f"\nüìç –ë–∞—Ç—á {i}:")
            logger.info(f"   X —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ –¥–æ: {X.device}")
            logger.info(f"   y —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ –¥–æ: {y.device}")
            
            # –ü–µ—Ä–µ–º–µ—â–µ–Ω–∏–µ –Ω–∞ GPU
            X = X.to(trainer.device)
            y = y.to(trainer.device)
            
            logger.info(f"   X —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ –ø–æ—Å–ª–µ: {X.device}")
            logger.info(f"   y —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ –ø–æ—Å–ª–µ: {y.device}")
            
            # Forward pass
            outputs = model(X)
            logger.info(f"   Outputs —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {outputs.device}")
            
            # –ü–∞–º—è—Ç—å GPU
            logger.info(f"   GPU –ø–∞–º—è—Ç—å: {torch.cuda.memory_allocated() / 1024**3:.2f} GB")
        
        logger.info("\n‚úÖ –¢–µ—Å—Ç GPU –∑–∞–≤–µ—Ä—à–µ–Ω —É—Å–ø–µ—à–Ω–æ!")
        return True
        
    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ —Ç–µ—Å—Ç–æ–≤–æ–º –æ–±—É—á–µ–Ω–∏–∏: {e}")
        traceback.print_exc()
        return False

def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –ø—Ä–æ–≤–µ—Ä–∫–∏"""
    logger = get_logger("GPUCheck")
    
    logger.info("="*80)
    logger.info("üîç –ü–†–û–í–ï–†–ö–ê –ò–°–ü–û–õ–¨–ó–û–í–ê–ù–ò–Ø GPU")
    logger.info("="*80)
    
    # 1. –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏ GPU
    if not check_gpu_setup():
        return
    
    # 2. –ü—Ä–æ–≤–µ—Ä–∫–∞ –º–æ–¥–µ–ª–∏ –Ω–∞ GPU
    logger.info("\n" + "="*50)
    if not check_model_on_gpu():
        return
    
    # 3. –ü—Ä–æ–≤–µ—Ä–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –¥–∞–Ω–Ω—ã—Ö
    logger.info("\n" + "="*50)
    if not check_data_loading():
        return
    
    # 4. –ó–∞–ø—É—Å–∫ –º–∏–Ω–∏-–æ–±—É—á–µ–Ω–∏—è
    logger.info("\n" + "="*50)
    if not run_mini_training():
        return
    
    logger.info("\n" + "="*80)
    logger.info("‚úÖ –í–°–ï –ü–†–û–í–ï–†–ö–ò –ü–†–û–ô–î–ï–ù–´!")
    logger.info("="*80)

if __name__ == "__main__":
    main()