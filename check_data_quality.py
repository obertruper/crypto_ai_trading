#!/usr/bin/env python3
"""
–ü—Ä–æ–≤–µ—Ä–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –≤—ã—è–≤–ª–µ–Ω–∏—è –ø—Ä–æ–±–ª–µ–º
"""

import pandas as pd
import numpy as np
from pathlib import Path
import matplotlib.pyplot as plt
import seaborn as sns
from datetime import datetime
import logging
from typing import Dict, List, Tuple
import warnings
warnings.filterwarnings('ignore')

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(level=logging.INFO, format='%(asctime)s | %(name)s | %(levelname)s | %(message)s')
logger = logging.getLogger('DataQuality')

class DataQualityChecker:
    """–ü—Ä–æ–≤–µ—Ä–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞ –¥–∞–Ω–Ω—ã—Ö"""
    
    def __init__(self, data_dir: str = "data/processed"):
        self.data_dir = Path(data_dir)
        self.issues = []
        self.stats = {}
        
    def load_data(self):
        """–ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö"""
        logger.info("üì• –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö...")
        
        self.train_data = pd.read_parquet(self.data_dir / "train_data.parquet")
        self.val_data = pd.read_parquet(self.data_dir / "val_data.parquet")
        self.test_data = pd.read_parquet(self.data_dir / "test_data.parquet")
        
        logger.info(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ: train={len(self.train_data):,}, val={len(self.val_data):,}, test={len(self.test_data):,}")
        
    def check_missing_values(self):
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π"""
        logger.info("üîç –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π...")
        
        for name, data in [("train", self.train_data), ("val", self.val_data), ("test", self.test_data)]:
            missing = data.isnull().sum()
            missing_pct = (missing / len(data)) * 100
            
            if missing.any():
                logger.warning(f"‚ö†Ô∏è {name}: –Ω–∞–π–¥–µ–Ω–æ {missing.sum()} –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π")
                top_missing = missing[missing > 0].sort_values(ascending=False).head(10)
                for col, count in top_missing.items():
                    logger.warning(f"    {col}: {count} ({missing_pct[col]:.2f}%)")
                    
                self.issues.append(f"{name}_missing_values")
            else:
                logger.info(f"‚úÖ {name}: –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π –Ω–µ—Ç")
                
    def check_infinite_values(self):
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –±–µ—Å–∫–æ–Ω–µ—á–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π"""
        logger.info("üîç –ü—Ä–æ–≤–µ—Ä–∫–∞ –±–µ—Å–∫–æ–Ω–µ—á–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π...")
        
        for name, data in [("train", self.train_data), ("val", self.val_data), ("test", self.test_data)]:
            numeric_cols = data.select_dtypes(include=[np.number]).columns
            
            inf_mask = np.isinf(data[numeric_cols])
            inf_count = inf_mask.sum()
            
            if inf_count.any():
                logger.warning(f"‚ö†Ô∏è {name}: –Ω–∞–π–¥–µ–Ω–æ {inf_count.sum()} –±–µ—Å–∫–æ–Ω–µ—á–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π")
                top_inf = inf_count[inf_count > 0].sort_values(ascending=False).head(10)
                for col, count in top_inf.items():
                    logger.warning(f"    {col}: {count}")
                    
                self.issues.append(f"{name}_infinite_values")
            else:
                logger.info(f"‚úÖ {name}: –±–µ—Å–∫–æ–Ω–µ—á–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π –Ω–µ—Ç")
                
    def check_data_distributions(self):
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–π –¥–∞–Ω–Ω—ã—Ö"""
        logger.info("üîç –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–π –¥–∞–Ω–Ω—ã—Ö...")
        
        # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –≥—Ä—É–ø–ø –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
        target_cols = [col for col in self.train_data.columns if any(
            x in col for x in ['future_return', 'direction_', 'will_reach', 'tp_time', 'sl_time']
        )]
        
        feature_cols = [col for col in self.train_data.columns if col not in target_cols and 
                       col not in ['datetime', 'symbol', 'open', 'high', 'low', 'close', 'volume']]
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –ø—Ä–∏–∑–Ω–∞–∫–∞–º
        for name, data in [("train", self.train_data), ("val", self.val_data), ("test", self.test_data)]:
            numeric_data = data[feature_cols].select_dtypes(include=[np.number])
            
            stats = {
                'mean': numeric_data.mean(),
                'std': numeric_data.std(),
                'min': numeric_data.min(),
                'max': numeric_data.max(),
                'skew': numeric_data.skew(),
                'kurtosis': numeric_data.kurtosis()
            }
            
            self.stats[name] = stats
            
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ —ç–∫—Å—Ç—Ä–µ–º–∞–ª—å–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è
            extreme_skew = stats['skew'][abs(stats['skew']) > 3]
            if len(extreme_skew) > 0:
                logger.warning(f"‚ö†Ô∏è {name}: {len(extreme_skew)} –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ —Å —ç–∫—Å—Ç—Ä–µ–º–∞–ª—å–Ω–æ–π –∞—Å–∏–º–º–µ—Ç—Ä–∏–µ–π")
                for col in extreme_skew.index[:5]:
                    logger.warning(f"    {col}: skew={stats['skew'][col]:.2f}")
                    
            extreme_kurt = stats['kurtosis'][abs(stats['kurtosis']) > 10]
            if len(extreme_kurt) > 0:
                logger.warning(f"‚ö†Ô∏è {name}: {len(extreme_kurt)} –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ —Å —ç–∫—Å—Ç—Ä–µ–º–∞–ª—å–Ω—ã–º —ç–∫—Å—Ü–µ—Å—Å–æ–º")
                
    def check_target_balance(self):
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –±–∞–ª–∞–Ω—Å–∞ —Ü–µ–ª–µ–≤—ã—Ö –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö"""
        logger.info("üîç –ü—Ä–æ–≤–µ—Ä–∫–∞ –±–∞–ª–∞–Ω—Å–∞ —Ü–µ–ª–µ–≤—ã—Ö –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö...")
        
        # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ü–µ–ª–µ–≤—ã—Ö –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö v4.0
        regression_targets = ['future_return_15m', 'future_return_1h', 
                            'future_return_4h', 'future_return_12h']
        
        categorical_targets = ['direction_15m', 'direction_1h', 
                             'direction_4h', 'direction_12h']
        
        binary_targets = [col for col in self.train_data.columns if 'will_reach' in col]
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–µ–≥—Ä–µ—Å—Å–∏–æ–Ω–Ω—ã—Ö —Ç–∞—Ä–≥–µ—Ç–æ–≤
        logger.info("üìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ —Ä–µ–≥—Ä–µ—Å—Å–∏–æ–Ω–Ω—ã—Ö —Ç–∞—Ä–≥–µ—Ç–æ–≤:")
        for target in regression_targets:
            if target in self.train_data.columns:
                train_mean = self.train_data[target].mean()
                val_mean = self.val_data[target].mean()
                test_mean = self.test_data[target].mean()
                
                logger.info(f"  {target}: train={train_mean:.6f}, val={val_mean:.6f}, test={test_mean:.6f}")
                
                # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ —Å–¥–≤–∏–≥ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è
                if abs(train_mean - val_mean) > 0.001 or abs(train_mean - test_mean) > 0.001:
                    logger.warning(f"    ‚ö†Ô∏è –û–±–Ω–∞—Ä—É–∂–µ–Ω —Å–¥–≤–∏–≥ –≤ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–∏!")
                    self.issues.append(f"distribution_shift_{target}")
                    
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã—Ö —Ç–∞—Ä–≥–µ—Ç–æ–≤
        logger.info("üìä –ë–∞–ª–∞–Ω—Å –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã—Ö —Ç–∞—Ä–≥–µ—Ç–æ–≤:")
        for target in categorical_targets:
            if target in self.train_data.columns:
                train_dist = self.train_data[target].value_counts(normalize=True)
                logger.info(f"  {target}:")
                for cat, pct in train_dist.items():
                    logger.info(f"    {cat}: {pct:.2%}")
                    
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –±–∏–Ω–∞—Ä–Ω—ã—Ö —Ç–∞—Ä–≥–µ—Ç–æ–≤
        logger.info("üìä –ë–∞–ª–∞–Ω—Å –±–∏–Ω–∞—Ä–Ω—ã—Ö —Ç–∞—Ä–≥–µ—Ç–æ–≤:")
        for target in binary_targets[:5]:  # –ü–µ—Ä–≤—ã–µ 5 –¥–ª—è –ø—Ä–∏–º–µ—Ä–∞
            if target in self.train_data.columns:
                positive_pct = self.train_data[target].mean()
                logger.info(f"  {target}: {positive_pct:.2%} positive")
                
                if positive_pct < 0.05 or positive_pct > 0.95:
                    logger.warning(f"    ‚ö†Ô∏è –°–∏–ª—å–Ω—ã–π –¥–∏—Å–±–∞–ª–∞–Ω—Å –∫–ª–∞—Å—Å–æ–≤!")
                    self.issues.append(f"class_imbalance_{target}")
                    
    def check_feature_correlations(self):
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–π –º–µ–∂–¥—É –ø—Ä–∏–∑–Ω–∞–∫–∞–º–∏"""
        logger.info("üîç –ü—Ä–æ–≤–µ—Ä–∫–∞ –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–π –º–µ–∂–¥—É –ø—Ä–∏–∑–Ω–∞–∫–∞–º–∏...")
        
        # –í—ã–±–∏—Ä–∞–µ–º —á–∏—Å–ª–æ–≤—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
        numeric_cols = self.train_data.select_dtypes(include=[np.number]).columns
        feature_cols = [col for col in numeric_cols if not any(
            x in col for x in ['future_return', 'direction_', 'will_reach', 'datetime']
        )]
        
        # –í—ã—á–∏—Å–ª—è–µ–º –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–æ–Ω–Ω—É—é –º–∞—Ç—Ä–∏—Ü—É
        corr_matrix = self.train_data[feature_cols[:50]].corr()  # –ü–µ—Ä–≤—ã–µ 50 –¥–ª—è —Å–∫–æ—Ä–æ—Å—Ç–∏
        
        # –ù–∞—Ö–æ–¥–∏–º –≤—ã—Å–æ–∫–∏–µ –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–∏
        high_corr = []
        for i in range(len(corr_matrix.columns)):
            for j in range(i+1, len(corr_matrix.columns)):
                if abs(corr_matrix.iloc[i, j]) > 0.95:
                    high_corr.append((
                        corr_matrix.columns[i],
                        corr_matrix.columns[j],
                        corr_matrix.iloc[i, j]
                    ))
                    
        if high_corr:
            logger.warning(f"‚ö†Ô∏è –ù–∞–π–¥–µ–Ω–æ {len(high_corr)} –ø–∞—Ä –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ —Å –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–µ–π > 0.95:")
            for col1, col2, corr in high_corr[:10]:
                logger.warning(f"    {col1} <-> {col2}: {corr:.3f}")
            self.issues.append("high_feature_correlation")
        else:
            logger.info("‚úÖ –í—ã—Å–æ–∫–∏—Ö –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–π –º–µ–∂–¥—É –ø—Ä–∏–∑–Ω–∞–∫–∞–º–∏ –Ω–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–æ")
            
    def check_temporal_consistency(self):
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –≤—Ä–µ–º–µ–Ω–Ω–æ–π –∫–æ–Ω—Å–∏—Å—Ç–µ–Ω—Ç–Ω–æ—Å—Ç–∏"""
        logger.info("üîç –ü—Ä–æ–≤–µ—Ä–∫–∞ –≤—Ä–µ–º–µ–Ω–Ω–æ–π –∫–æ–Ω—Å–∏—Å—Ç–µ–Ω—Ç–Ω–æ—Å—Ç–∏...")
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø–æ—Ä—è–¥–∫–∞ –¥–∞—Ç
        for name, data in [("train", self.train_data), ("val", self.val_data), ("test", self.test_data)]:
            if 'datetime' in data.columns:
                # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ datetime –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
                if data['datetime'].dtype == 'object':
                    data['datetime'] = pd.to_datetime(data['datetime'])
                    
                # –ü—Ä–æ–≤–µ—Ä–∫–∞ –º–æ–Ω–æ—Ç–æ–Ω–Ω–æ—Å—Ç–∏
                is_sorted = data['datetime'].is_monotonic_increasing
                if not is_sorted:
                    logger.warning(f"‚ö†Ô∏è {name}: –¥–∞–Ω–Ω—ã–µ –Ω–µ –æ—Ç—Å–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω—ã –ø–æ –≤—Ä–µ–º–µ–Ω–∏!")
                    self.issues.append(f"{name}_not_sorted")
                    
                # –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø—Ä–æ–ø—É—Å–∫–æ–≤ –≤–æ –≤—Ä–µ–º–µ–Ω–∏
                time_diffs = data.groupby('symbol')['datetime'].diff()
                expected_diff = pd.Timedelta(minutes=15)  # –î–ª—è 15-–º–∏–Ω—É—Ç–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
                
                gaps = time_diffs[time_diffs > expected_diff * 2]
                if len(gaps) > 0:
                    logger.warning(f"‚ö†Ô∏è {name}: –Ω–∞–π–¥–µ–Ω–æ {len(gaps)} –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –ø—Ä–æ–ø—É—Å–∫–æ–≤")
                    
    def check_data_leakage(self):
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ —É—Ç–µ—á–∫–∏ –¥–∞–Ω–Ω—ã—Ö –º–µ–∂–¥—É –≤—ã–±–æ—Ä–∫–∞–º–∏"""
        logger.info("üîç –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ —É—Ç–µ—á–∫–∏ –¥–∞–Ω–Ω—ã—Ö...")
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø–µ—Ä–µ—Å–µ—á–µ–Ω–∏—è –ø–æ –≤—Ä–µ–º–µ–Ω–∏
        if 'datetime' in self.train_data.columns:
            train_max = self.train_data['datetime'].max()
            val_min = self.val_data['datetime'].min()
            val_max = self.val_data['datetime'].max()
            test_min = self.test_data['datetime'].min()
            
            if train_max >= val_min:
                logger.error("‚ùå –û–±–Ω–∞—Ä—É–∂–µ–Ω–æ –ø–µ—Ä–µ—Å–µ—á–µ–Ω–∏–µ train –∏ val –ø–æ –≤—Ä–µ–º–µ–Ω–∏!")
                self.issues.append("train_val_overlap")
                
            if val_max >= test_min:
                logger.error("‚ùå –û–±–Ω–∞—Ä—É–∂–µ–Ω–æ –ø–µ—Ä–µ—Å–µ—á–µ–Ω–∏–µ val –∏ test –ø–æ –≤—Ä–µ–º–µ–Ω–∏!")
                self.issues.append("val_test_overlap")
                
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ –≤—Ä–µ–º–µ–Ω–Ω–æ–≥–æ gap
            train_val_gap = (val_min - train_max).days
            val_test_gap = (test_min - val_max).days
            
            logger.info(f"üìÖ –í—Ä–µ–º–µ–Ω–Ω—ã–µ –ø—Ä–æ–º–µ–∂—É—Ç–∫–∏:")
            logger.info(f"  Train ‚Üí Val: {train_val_gap} –¥–Ω–µ–π")
            logger.info(f"  Val ‚Üí Test: {val_test_gap} –¥–Ω–µ–π")
            
            if train_val_gap < 1 or val_test_gap < 1:
                logger.warning("‚ö†Ô∏è –°–ª–∏—à–∫–æ–º –º–∞–ª–µ–Ω—å–∫–∏–π –≤—Ä–µ–º–µ–Ω–Ω–æ–π gap –º–µ–∂–¥—É –≤—ã–±–æ—Ä–∫–∞–º–∏!")
                
    def generate_report(self, save_path: str = "data_quality_report.txt"):
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç—á–µ—Ç–∞"""
        logger.info("üìÑ –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç—á–µ—Ç–∞...")
        
        with open(save_path, 'w', encoding='utf-8') as f:
            f.write("=" * 80 + "\n")
            f.write("–û–¢–ß–ï–¢ –û –ö–ê–ß–ï–°–¢–í–ï –î–ê–ù–ù–´–•\n")
            f.write(f"–î–∞—Ç–∞: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write("=" * 80 + "\n\n")
            
            f.write(f"üìä –†–∞–∑–º–µ—Ä—ã –¥–∞—Ç–∞—Å–µ—Ç–æ–≤:\n")
            f.write(f"  Train: {len(self.train_data):,} –∑–∞–ø–∏—Å–µ–π\n")
            f.write(f"  Val: {len(self.val_data):,} –∑–∞–ø–∏—Å–µ–π\n")
            f.write(f"  Test: {len(self.test_data):,} –∑–∞–ø–∏—Å–µ–π\n\n")
            
            if self.issues:
                f.write(f"‚ö†Ô∏è –û–±–Ω–∞—Ä—É–∂–µ–Ω–æ –ø—Ä–æ–±–ª–µ–º: {len(self.issues)}\n")
                for issue in self.issues:
                    f.write(f"  - {issue}\n")
            else:
                f.write("‚úÖ –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∏—Ö –ø—Ä–æ–±–ª–µ–º –Ω–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–æ\n")
                
            f.write("\n" + "=" * 80 + "\n")
            f.write("–†–ï–ö–û–ú–ï–ù–î–ê–¶–ò–ò:\n")
            f.write("=" * 80 + "\n")
            
            if "high_feature_correlation" in self.issues:
                f.write("‚Ä¢ –£–¥–∞–ª–∏—Ç—å –≤—ã—Å–æ–∫–æ –∫–æ—Ä—Ä–µ–ª–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏\n")
            if any("class_imbalance" in issue for issue in self.issues):
                f.write("‚Ä¢ –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –≤–∑–≤–µ—à–µ–Ω–Ω—É—é loss —Ñ—É–Ω–∫—Ü–∏—é –¥–ª—è –Ω–µ—Å–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –∫–ª–∞—Å—Å–æ–≤\n")
            if any("distribution_shift" in issue for issue in self.issues):
                f.write("‚Ä¢ –ü—Ä–æ–≤–µ—Ä–∏—Ç—å –ø—Ä–æ—Ü–µ—Å—Å —Ä–∞–∑–¥–µ–ª–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö –Ω–∞ –≤—ã–±–æ—Ä–∫–∏\n")
            if any("missing_values" in issue for issue in self.issues):
                f.write("‚Ä¢ –û–±—Ä–∞–±–æ—Ç–∞—Ç—å –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è\n")
                
        logger.info(f"‚úÖ –û—Ç—á–µ—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤ {save_path}")
        

def main():
    """–ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è"""
    checker = DataQualityChecker()
    
    # –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö
    checker.load_data()
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∏
    checker.check_missing_values()
    checker.check_infinite_values()
    checker.check_data_distributions()
    checker.check_target_balance()
    checker.check_feature_correlations()
    checker.check_temporal_consistency()
    checker.check_data_leakage()
    
    # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç—á–µ—Ç–∞
    checker.generate_report()
    
    logger.info("‚úÖ –ü—Ä–æ–≤–µ—Ä–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞ –¥–∞–Ω–Ω—ã—Ö –∑–∞–≤–µ—Ä—à–µ–Ω–∞!")
    

if __name__ == "__main__":
    main()