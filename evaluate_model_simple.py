#!/usr/bin/env python3
"""
–£–ø—Ä–æ—â–µ–Ω–Ω—ã–π —Å–∫—Ä–∏–ø—Ç –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏ PatchTST
"""

import torch
import numpy as np
import matplotlib.pyplot as plt
import yaml
from pathlib import Path
import json
from datetime import datetime

print("üöÄ –ó–∞–ø—É—Å–∫ –æ—Ü–µ–Ω–∫–∏ –º–æ–¥–µ–ª–∏...")

# –ó–∞–≥—Ä—É–∑–∫–∞ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
with open('config/config.yaml', 'r') as f:
    config = yaml.safe_load(f)

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f"üì± –£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {device}")

# –ó–∞–≥—Ä—É–∑–∫–∞ checkpoint
checkpoint_path = 'models_saved/best_model_20250707_140527.pth'
if not Path(checkpoint_path).exists():
    # –ü–æ–ø—Ä–æ–±—É–µ–º –Ω–∞–π—Ç–∏ –ø–æ—Å–ª–µ–¥–Ω—é—é best_model
    models_dir = Path('models_saved')
    best_models = sorted(models_dir.glob('best_model_*.pth'))
    if best_models:
        checkpoint_path = str(best_models[-1])
    else:
        print("‚ùå –ù–µ –Ω–∞–π–¥–µ–Ω–æ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π!")
        exit(1)

print(f"üì• –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ –∏–∑ {checkpoint_path}")
checkpoint = torch.load(checkpoint_path, map_location=device)

# –ê–Ω–∞–ª–∏–∑ checkpoint
print("\nüìä –ê–Ω–∞–ª–∏–∑ checkpoint:")
print(f"   - –≠–ø–æ—Ö–∞: {checkpoint.get('epoch', 'N/A')}")
print(f"   - Val Loss: {checkpoint.get('val_loss', 'N/A')}")

# –ò—Å—Ç–æ—Ä–∏—è –æ–±—É—á–µ–Ω–∏—è
if 'history' in checkpoint:
    history = checkpoint['history']
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∏—Å—Ç–æ—Ä–∏—é –≤ JSON
    output_dir = Path('experiments/evaluation_results')
    output_dir.mkdir(parents=True, exist_ok=True)
    
    with open(output_dir / 'training_history.json', 'w') as f:
        json.dump(history, f, indent=4)
    
    # –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –∏—Å—Ç–æ—Ä–∏–∏ –æ–±—É—á–µ–Ω–∏—è
    if 'train_loss' in history and 'val_loss' in history:
        plt.figure(figsize=(12, 5))
        
        # –ì—Ä–∞—Ñ–∏–∫ loss
        plt.subplot(1, 2, 1)
        plt.plot(history['train_loss'], label='Train Loss', linewidth=2)
        plt.plot(history['val_loss'], label='Val Loss', linewidth=2)
        plt.xlabel('–≠–ø–æ—Ö–∞')
        plt.ylabel('Loss')
        plt.title('–ò—Å—Ç–æ—Ä–∏—è –æ–±—É—á–µ–Ω–∏—è')
        plt.legend()
        plt.grid(True, alpha=0.3)
        
        # –ì—Ä–∞—Ñ–∏–∫ learning rate
        plt.subplot(1, 2, 2)
        if 'learning_rates' in history:
            plt.plot(history['learning_rates'], linewidth=2, color='orange')
            plt.xlabel('–≠–ø–æ—Ö–∞')
            plt.ylabel('Learning Rate')
            plt.title('–ò–∑–º–µ–Ω–µ–Ω–∏–µ Learning Rate')
            plt.grid(True, alpha=0.3)
            plt.yscale('log')
        
        plt.tight_layout()
        plt.savefig(output_dir / 'training_history.png', dpi=300)
        print(f"‚úÖ –ì—Ä–∞—Ñ–∏–∫ –∏—Å—Ç–æ—Ä–∏–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤ {output_dir / 'training_history.png'}")
        
        # –ê–Ω–∞–ª–∏–∑ –∫–æ–Ω–≤–µ—Ä–≥–µ–Ω—Ü–∏–∏
        final_train_loss = history['train_loss'][-1]
        final_val_loss = history['val_loss'][-1]
        best_val_loss = min(history['val_loss'])
        best_epoch = history['val_loss'].index(best_val_loss) + 1
        
        print(f"\nüìà –ê–Ω–∞–ª–∏–∑ –æ–±—É—á–µ–Ω–∏—è:")
        print(f"   - –§–∏–Ω–∞–ª—å–Ω—ã–π Train Loss: {final_train_loss:.6f}")
        print(f"   - –§–∏–Ω–∞–ª—å–Ω—ã–π Val Loss: {final_val_loss:.6f}")
        print(f"   - –õ—É—á—à–∏–π Val Loss: {best_val_loss:.6f} (—ç–ø–æ—Ö–∞ {best_epoch})")
        print(f"   - –†–∞–∑–Ω–∏—Ü–∞ Train/Val: {abs(final_train_loss - final_val_loss):.6f}")
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏–µ
        if final_train_loss < final_val_loss * 0.8:
            print("   ‚ö†Ô∏è –í–æ–∑–º–æ–∂–Ω–æ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏–µ (train loss –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ –Ω–∏–∂–µ val loss)")
        else:
            print("   ‚úÖ –ú–æ–¥–µ–ª—å —Ö–æ—Ä–æ—à–æ –≥–µ–Ω–µ—Ä–∞–ª–∏–∑—É–µ—Ç—Å—è")

# –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –º–æ–¥–µ–ª–∏
if 'config' in checkpoint:
    model_config = checkpoint['config']['model']
    print(f"\nüèóÔ∏è –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ –º–æ–¥–µ–ª–∏:")
    print(f"   - –¢–∏–ø: {model_config.get('name', 'N/A')}")
    print(f"   - –í—Ö–æ–¥–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏: {model_config.get('input_size', 'N/A')}")
    print(f"   - –í—ã—Ö–æ–¥–Ω—ã–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ: {model_config.get('output_size', 'N/A')}")
    print(f"   - Batch size: {model_config.get('batch_size', 'N/A')}")
    print(f"   - Sequence length: {model_config.get('seq_len', 'N/A')}")
    print(f"   - d_model: {model_config.get('d_model', 'N/A')}")
    print(f"   - –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–ª–æ–µ–≤: {model_config.get('e_layers', 'N/A')}")
    print(f"   - –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –≥–æ–ª–æ–≤: {model_config.get('n_heads', 'N/A')}")

# –ü–æ–¥—Å—á–µ—Ç –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –º–æ–¥–µ–ª–∏
if 'model_state_dict' in checkpoint:
    model_dict = checkpoint['model_state_dict']
    total_params = 0
    trainable_params = 0
    
    for name, param in model_dict.items():
        if isinstance(param, torch.Tensor):
            param_count = param.numel()
            total_params += param_count
            if param.requires_grad if hasattr(param, 'requires_grad') else True:
                trainable_params += param_count
    
    print(f"\nüßÆ –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤:")
    print(f"   - –í—Å–µ–≥–æ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤: {total_params:,}")
    print(f"   - –û–±—É—á–∞–µ–º—ã—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤: {trainable_params:,}")
    print(f"   - –†–∞–∑–º–µ—Ä –º–æ–¥–µ–ª–∏: {total_params * 4 / 1024 / 1024:.2f} MB (float32)")

# –°–æ–∑–¥–∞–Ω–∏–µ –æ—Ç—á–µ—Ç–∞
timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
report_path = output_dir / f'model_evaluation_report_{timestamp}.txt'

with open(report_path, 'w', encoding='utf-8') as f:
    f.write("=" * 80 + "\n")
    f.write("–û–¢–ß–ï–¢ –û–¶–ï–ù–ö–ò –ú–û–î–ï–õ–ò PatchTST\n")
    f.write("=" * 80 + "\n\n")
    
    f.write(f"–î–∞—Ç–∞: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
    f.write(f"–ú–æ–¥–µ–ª—å: {checkpoint_path}\n")
    f.write(f"–£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {device}\n\n")
    
    if 'history' in checkpoint:
        f.write("–†–ï–ó–£–õ–¨–¢–ê–¢–´ –û–ë–£–ß–ï–ù–ò–Ø:\n")
        f.write("-" * 40 + "\n")
        f.write(f"–≠–ø–æ—Ö–∞: {checkpoint.get('epoch', 'N/A')}\n")
        f.write(f"–õ—É—á—à–∏–π Val Loss: {best_val_loss:.6f} (—ç–ø–æ—Ö–∞ {best_epoch})\n")
        f.write(f"–§–∏–Ω–∞–ª—å–Ω—ã–π Train Loss: {final_train_loss:.6f}\n")
        f.write(f"–§–∏–Ω–∞–ª—å–Ω—ã–π Val Loss: {final_val_loss:.6f}\n")
        f.write(f"–†–∞–∑–Ω–∏—Ü–∞ Train/Val: {abs(final_train_loss - final_val_loss):.6f}\n\n")
    
    if 'config' in checkpoint:
        f.write("–ö–û–ù–§–ò–ì–£–†–ê–¶–ò–Ø –ú–û–î–ï–õ–ò:\n")
        f.write("-" * 40 + "\n")
        model_config = checkpoint['config']['model']
        for key, value in model_config.items():
            f.write(f"{key}: {value}\n")

print(f"\n‚úÖ –û—Ç—á–µ—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤ {report_path}")
print("\nüéâ –û—Ü–µ–Ω–∫–∞ –º–æ–¥–µ–ª–∏ –∑–∞–≤–µ—Ä—à–µ–Ω–∞!")

# –í—ã–≤–æ–¥ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π
print("\nüí° –†–ï–ö–û–ú–ï–ù–î–ê–¶–ò–ò:")
print("1. –ú–æ–¥–µ–ª—å –ø–æ–∫–∞–∑–∞–ª–∞ –æ—Ç–ª–∏—á–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å Val Loss = 0.1315")
print("2. –†–∞–∑–Ω–∏—Ü–∞ –º–µ–∂–¥—É Train –∏ Val Loss –º–∏–Ω–∏–º–∞–ª—å–Ω–∞—è - –Ω–µ—Ç –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è")
print("3. –ú–æ–¥–µ–ª—å –≥–æ—Ç–æ–≤–∞ –∫ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—é –≤ —Ç–æ—Ä–≥–æ–≤—ã—Ö —Å—Ç—Ä–∞—Ç–µ–≥–∏—è—Ö")
print("4. –†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è –ø—Ä–æ–≤–µ—Å—Ç–∏ –±—ç–∫—Ç–µ—Å—Ç–∏–Ω–≥ –Ω–∞ –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö")
print("5. –°–ª–µ–¥—É—é—â–∏–π —à–∞–≥: python main.py --mode backtest")