#!/usr/bin/env python3
"""
COMPLETE CRYPTO AI TRADING SYSTEM
=================================

–ü–æ–ª–Ω—ã–π –ø—Ä–æ–µ–∫—Ç –¥–ª—è –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏—è –∏ —Ç–æ—Ä–≥–æ–≤–ª–∏ –∫—Ä–∏–ø—Ç–æ–≤–∞–ª—é—Ç–Ω—ã–º–∏ —Ñ—å—é—á–µ—Ä—Å–∞–º–∏
—Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º PatchTST (Patch Time Series Transformer)

–î–ª—è —Ä–∞–∑–≤–µ—Ä—Ç—ã–≤–∞–Ω–∏—è:
1. –°–æ–∑–¥–∞–π—Ç–µ —Å—Ç—Ä—É–∫—Ç—É—Ä—É –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–π —Å–æ–≥–ª–∞—Å–Ω–æ —Å–µ–∫—Ü–∏–∏ "PROJECT STRUCTURE"
2. –°–∫–æ–ø–∏—Ä—É–π—Ç–µ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏–µ —Å–µ–∫—Ü–∏–∏ –∫–æ–¥–∞ –≤ —Ñ–∞–π–ª—ã
3. –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –∏–∑ requirements.txt
4. –ù–∞—Å—Ç—Ä–æ–π—Ç–µ PostgreSQL –∏ config.yaml
5. –ó–∞–ø—É—Å—Ç–∏—Ç–µ: python main.py --mode full

–ê–≤—Ç–æ—Ä: AI Trading System
–í–µ—Ä—Å–∏—è: 1.0.0
"""

# ==============================================================================
# PROJECT STRUCTURE
# ==============================================================================
"""
crypto_ai_trading/
‚îÇ
‚îú‚îÄ‚îÄ config/
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ config.yaml              
‚îÇ   ‚îî‚îÄ‚îÄ logging_config.py        
‚îÇ
‚îú‚îÄ‚îÄ data/
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ data_loader.py          
‚îÇ   ‚îú‚îÄ‚îÄ preprocessor.py         
‚îÇ   ‚îú‚îÄ‚îÄ feature_engineering.py  
‚îÇ   ‚îî‚îÄ‚îÄ dataset.py              
‚îÇ
‚îú‚îÄ‚îÄ models/
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ patchtst.py             
‚îÇ   ‚îú‚îÄ‚îÄ ensemble.py             
‚îÇ   ‚îî‚îÄ‚îÄ losses.py               
‚îÇ
‚îú‚îÄ‚îÄ trading/
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ risk_manager.py         
‚îÇ   ‚îú‚îÄ‚îÄ position_sizer.py       
‚îÇ   ‚îú‚îÄ‚îÄ signals.py              
‚îÇ   ‚îî‚îÄ‚îÄ backtester.py           
‚îÇ
‚îú‚îÄ‚îÄ training/
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ trainer.py              
‚îÇ   ‚îú‚îÄ‚îÄ optimizer.py            
‚îÇ   ‚îî‚îÄ‚îÄ validator.py            
‚îÇ
‚îú‚îÄ‚îÄ utils/
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ metrics.py              
‚îÇ   ‚îú‚îÄ‚îÄ visualization.py        
‚îÇ   ‚îî‚îÄ‚îÄ logger.py               
‚îÇ
‚îú‚îÄ‚îÄ notebooks/
‚îÇ   ‚îú‚îÄ‚îÄ 01_data_exploration.ipynb
‚îÇ   ‚îú‚îÄ‚îÄ 02_feature_analysis.ipynb
‚îÇ   ‚îî‚îÄ‚îÄ 03_model_evaluation.ipynb
‚îÇ
‚îú‚îÄ‚îÄ experiments/
‚îÇ   ‚îî‚îÄ‚îÄ logs/                   
‚îÇ
‚îú‚îÄ‚îÄ models_saved/               
‚îú‚îÄ‚îÄ results/                    
‚îÇ
‚îú‚îÄ‚îÄ requirements.txt
‚îú‚îÄ‚îÄ setup.py
‚îú‚îÄ‚îÄ README.md
‚îî‚îÄ‚îÄ main.py                     
"""

# ==============================================================================
# FILE: requirements.txt
# ==============================================================================
"""
# Core ML
torch>=2.0.0
numpy>=1.24.0
pandas>=2.0.0
scikit-learn>=1.3.0

# Database
psycopg2-binary>=2.9.5
sqlalchemy>=2.0.0

# Technical Analysis
ta>=0.10.2
pandas-ta>=0.3.14b

# Backtesting
vectorbt>=0.25.0
ccxt>=4.0.0

# Visualization
matplotlib>=3.7.0
seaborn>=0.12.0
plotly>=5.14.0

# Configuration
pyyaml>=6.0
python-dotenv>=1.0.0

# Logging & Monitoring
tensorboard>=2.13.0
wandb>=0.15.0
tqdm>=4.65.0
colorlog>=6.7.0
python-json-logger>=2.0.0

# Model specific
einops>=0.6.1
transformers>=4.30.0

# Data handling
h5py>=3.8.0
pyarrow>=11.0.0

# Development
jupyter>=1.0.0
black>=23.0.0
pytest>=7.3.0
"""

# ==============================================================================
# FILE: config/config.yaml
# ==============================================================================
CONFIG_YAML = """
# –û—Å–Ω–æ–≤–Ω–∞—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –ø—Ä–æ–µ–∫—Ç–∞ AI —Ç–æ—Ä–≥–æ–≤–æ–π —Å–∏—Å—Ç–µ–º—ã

# –ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ –±–∞–∑–µ –¥–∞–Ω–Ω—ã—Ö
database:
  host: "localhost"
  port: 5432
  database: "crypto_futures"
  user: "your_user"
  password: "your_password"
  table: "raw_market_data"

# –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–∞–Ω–Ω—ã—Ö
data:
  # –°–ø–∏—Å–æ–∫ —Ç–æ—Ä–≥–æ–≤—ã—Ö –ø–∞—Ä –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
  symbols: 
    - "BTCUSDT"
    - "ETHUSDT"
    - "BNBUSDT"
    - "SOLUSDT"
    - "XRPUSDT"
    - "DOGEUSDT"
    - "ADAUSDT"
    - "AVAXUSDT"
    - "DOTUSDT"
    - "LINKUSDT"
    - "LTCUSDT"
    - "ATOMUSDT"
    - "UNIUSDT"
    - "MATICUSDT"
    - "ALGOUSDT"
    - "XLMUSDT"
    - "VETUSDT"
    - "NEARUSDT"
    - "FILUSDT"
    - "ICPUSDT"
    - "TRBUSDT"
    - "AAVEUSDT"
    - "SANDUSDT"
    - "AXSUSDT"
    - "MANAUSDT"
    - "GALAUSDT"
    - "APEUSDT"
    - "GMTUSDT"
    - "CAKEUSDT"
    - "1INCHUSDT"
    - "ENSUSDT"
    - "PEOPLEUSDT"
    - "ANTUSDT"
    - "ROSEUSDT"
    - "DYDXUSDT"
    - "1000SHIBUSDT"
    - "OPUSDT"
    - "APTUSDT"
    - "ARBUSDT"
    - "SUIUSDT"
    - "TIAUSDT"
    - "TONUSDT"
    - "TAOUSDT"
    - "JTOUSDT"
    - "OMUSDT"
    - "HBARUSDT"
    - "WIFUSDT"
    - "POPCATUSDT"
    - "PNUTUSDT"
    - "1000PEPEUSDT"
    - "ZEREBROUSDT"
  
  # –í—Ä–µ–º–µ–Ω–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
  start_date: "2022-06-08"
  end_date: "2025-06-16"
  interval_minutes: 15
  
  # –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö
  train_ratio: 0.7
  val_ratio: 0.15
  test_ratio: 0.15
  
  # Walk-forward –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
  walk_forward:
    training_window: 180  # –¥–Ω–µ–π
    validation_window: 30
    test_window: 30
    step_size: 7  # –¥–Ω–µ–π

# –ò–Ω–∂–µ–Ω–µ—Ä–∏—è –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
features:
  # –¢–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã
  technical:
    - name: "sma"
      periods: [10, 20, 50]
    - name: "ema"
      periods: [12, 26]
    - name: "rsi"
      period: 14
    - name: "macd"
      fast: 12
      slow: 26
      signal: 9
    - name: "bollinger_bands"
      period: 20
      std_dev: 2
    - name: "atr"
      period: 14
    - name: "volume_profile"
      bins: 20
  
  # –ú–∏–∫—Ä–æ—Å—Ç—Ä—É–∫—Ç—É—Ä–∞ —Ä—ã–Ω–∫–∞
  microstructure:
    - "bid_ask_spread"
    - "order_book_imbalance"
    - "volume_weighted_price"
    - "trade_flow_toxicity"
  
  # –ö—Ä–æ—Å—Å-–∞–∫—Ç–∏–≤–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
  cross_asset:
    - "btc_dominance_effect"
    - "sector_correlation"
    - "lead_lag_signals"
  
  # –í—Ä–µ–º–µ–Ω–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
  temporal:
    - "hour_of_day"
    - "day_of_week"
    - "month_of_year"
    - "is_weekend"

# –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –º–æ–¥–µ–ª–∏ PatchTST
model:
  name: "PatchTST"
  
  # –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞
  input_size: 100  # –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
  patch_len: 16    # –¥–ª–∏–Ω–∞ –ø–∞—Ç—á–∞
  stride: 8        # —à–∞–≥ –ø–∞—Ç—á–∞
  context_window: 168  # –≤—Ö–æ–¥–Ω–æ–µ –æ–∫–Ω–æ (42 —á–∞—Å–∞)
  
  # Transformer –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
  d_model: 128
  n_heads: 8
  e_layers: 3      # encoder layers
  d_ff: 512        # feedforward dimension
  dropout: 0.1
  activation: "gelu"
  
  # –ü—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏–µ
  pred_len: 4      # –ø—Ä–æ–≥–Ω–æ–∑ –Ω–∞ 4 —à–∞–≥–∞ (1 —á–∞—Å)
  individual: false # channel-independent
  
  # –û–±—É—á–µ–Ω–∏–µ
  learning_rate: 0.001
  batch_size: 32
  epochs: 100
  early_stopping_patience: 10
  gradient_clip: 1.0

# –£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ —Ä–∏—Å–∫–∞–º–∏
risk_management:
  # –û—Å–Ω–æ–≤–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
  risk_reward_ratio: 3.0
  stop_loss_pct: 1.0      # 1% —Å—Ç–æ–ø-–ª–æ—Å—Å
  take_profit_targets:    # –¶–µ–ª–∏ –ø—Ä–∏–±—ã–ª–∏
    - 2.0   # 2% (—Ä–∏—Å–∫ 1:2)
    - 3.0   # 3% (—Ä–∏—Å–∫ 1:3)
    - 5.0   # 5% (—Ä–∏—Å–∫ 1:5)
  
  # –†–∞–∑–º–µ—Ä –ø–æ–∑–∏—Ü–∏–∏
  position_sizing:
    method: "volatility_based"  # –Ω–∞ –æ—Å–Ω–æ–≤–µ ATR
    max_position_pct: 10.0      # –º–∞–∫—Å 10% –ø–æ—Ä—Ç—Ñ–µ–ª—è
    risk_per_trade_pct: 1.0     # —Ä–∏—Å–∫ 1% –Ω–∞ —Å–¥–µ–ª–∫—É
  
  # –î–∏–≤–µ—Ä—Å–∏—Ñ–∏–∫–∞—Ü–∏—è
  max_concurrent_positions: 10
  max_positions_per_symbol: 1
  
  # –ö–æ—Ä—Ä–µ–∫—Ç–∏—Ä–æ–≤–∫–∞ –¥–ª—è —Ä–∞–∑–Ω—ã—Ö —Ç–∏–ø–æ–≤ –º–æ–Ω–µ—Ç
  volatility_adjustment:
    major_coins: ["BTCUSDT", "ETHUSDT", "BNBUSDT"]
    major_risk_multiplier: 1.0
    altcoin_risk_multiplier: 0.7
    meme_coin_risk_multiplier: 0.5

# –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –±—ç–∫—Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
backtesting:
  initial_capital: 100000  # USD
  commission: 0.001        # 0.1%
  slippage: 0.0005        # 0.05%
  
  # –ú–µ—Ç—Ä–∏–∫–∏ –¥–ª—è –æ—Ü–µ–Ω–∫–∏
  metrics:
    - "total_return"
    - "sharpe_ratio"
    - "max_drawdown"
    - "win_rate"
    - "profit_factor"
    - "expectancy"
    - "calmar_ratio"

# –ê–Ω—Å–∞–º–±–ª—å –º–æ–¥–µ–ª–µ–π
ensemble:
  enabled: true
  models_count: 5
  voting_method: "weighted_average"  # –∏–ª–∏ "unanimous"
  
  # –í–µ—Å–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
  weight_metric: "sharpe_ratio"
  rebalance_frequency: "weekly"

# –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –∏ –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥
logging:
  level: "INFO"
  handlers:
    - "console"
    - "file"
  
  # –ü—É—Ç—å –¥–ª—è –ª–æ–≥–æ–≤
  log_dir: "experiments/logs"
  
  # TensorBoard
  tensorboard:
    enabled: true
    log_dir: "experiments/tensorboard"
  
  # Weights & Biases
  wandb:
    enabled: false
    project: "crypto-ai-trading"
    entity: "your-entity"

# –ü—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å
performance:
  num_workers: 4          # –¥–ª—è DataLoader
  device: "cuda"          # –∏–ª–∏ "cpu"
  mixed_precision: true   # FP16 –æ–±—É—á–µ–Ω–∏–µ
  
  # –ö—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ
  cache_features: true
  cache_dir: "cache/"

# –í–∞–ª–∏–¥–∞—Ü–∏—è
validation:
  # –°—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏–µ —Ç–µ—Å—Ç—ã
  statistical_tests:
    - "sharpe_ratio_test"
    - "information_ratio"
    - "monte_carlo_permutation"
  
  # –ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–µ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è
  min_sharpe_ratio: 1.5
  min_win_rate: 0.25      # –¥–ª—è —Ä–∏—Å–∫–∞ 1:3
  max_drawdown: 0.20      # 20%
"""

# ==============================================================================
# FILE: utils/logger.py
# ==============================================================================
LOGGER_PY = '''
"""
–ü—Ä–æ–¥–≤–∏–Ω—É—Ç–∞—è —Å–∏—Å—Ç–µ–º–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è –¥–ª—è AI —Ç–æ—Ä–≥–æ–≤–æ–π —Å–∏—Å—Ç–µ–º—ã
"""

import logging
import logging.handlers
import os
import sys
from datetime import datetime
from pathlib import Path
from typing import Optional, Dict, Any
import json
import colorlog
from pythonjsonlogger import jsonlogger

class TradingLogger:
    """–¶–µ–Ω—Ç—Ä–∞–ª–∏–∑–æ–≤–∞–Ω–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è –¥–ª—è —Ç–æ—Ä–≥–æ–≤–æ–π —Å–∏—Å—Ç–µ–º—ã"""
    
    def __init__(self, name: str, config_path: str = "config/config.yaml"):
        self.name = name
        self.config = self._load_config(config_path)
        self.logger = self._setup_logger()
        self.stage_timers = {}
        
    def _load_config(self, config_path: str) -> Dict[str, Any]:
        """–ó–∞–≥—Ä—É–∑–∫–∞ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏"""
        import yaml
        with open(config_path, 'r') as f:
            return yaml.safe_load(f)
    
    def _setup_logger(self) -> logging.Logger:
        """–ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–≥–µ—Ä–∞ —Å –Ω–µ—Å–∫–æ–ª—å–∫–∏–º–∏ –æ–±—Ä–∞–±–æ—Ç—á–∏–∫–∞–º–∏"""
        logger = logging.getLogger(self.name)
        logger.setLevel(self.config['logging']['level'])
        logger.handlers = []
        
        log_dir = Path(self.config['logging']['log_dir'])
        log_dir.mkdir(parents=True, exist_ok=True)
        
        if 'console' in self.config['logging']['handlers']:
            console_handler = self._create_console_handler()
            logger.addHandler(console_handler)
        
        if 'file' in self.config['logging']['handlers']:
            file_handler = self._create_file_handler(log_dir)
            logger.addHandler(file_handler)
        
        json_handler = self._create_json_handler(log_dir)
        logger.addHandler(json_handler)
        
        return logger
    
    def _create_console_handler(self) -> logging.Handler:
        """–°–æ–∑–¥–∞–Ω–∏–µ —Ü–≤–µ—Ç–Ω–æ–≥–æ –∫–æ–Ω—Å–æ–ª—å–Ω–æ–≥–æ –æ–±—Ä–∞–±–æ—Ç—á–∏–∫–∞"""
        console_handler = colorlog.StreamHandler(sys.stdout)
        
        console_format = colorlog.ColoredFormatter(
            '%(log_color)s%(asctime)s | %(name)-20s | %(levelname)-8s | %(message)s',
            datefmt='%Y-%m-%d %H:%M:%S',
            log_colors={
                'DEBUG': 'cyan',
                'INFO': 'green',
                'WARNING': 'yellow',
                'ERROR': 'red',
                'CRITICAL': 'red,bg_white',
            }
        )
        
        console_handler.setFormatter(console_format)
        return console_handler
    
    def _create_file_handler(self, log_dir: Path) -> logging.Handler:
        """–°–æ–∑–¥–∞–Ω–∏–µ —Ñ–∞–π–ª–æ–≤–æ–≥–æ –æ–±—Ä–∞–±–æ—Ç—á–∏–∫–∞ —Å —Ä–æ—Ç–∞—Ü–∏–µ–π"""
        log_file = log_dir / f"{self.name}_{datetime.now().strftime('%Y%m%d')}.log"
        
        file_handler = logging.handlers.RotatingFileHandler(
            log_file,
            maxBytes=10 * 1024 * 1024,  # 10MB
            backupCount=10,
            encoding='utf-8'
        )
        
        file_format = logging.Formatter(
            '%(asctime)s | %(name)s | %(levelname)s | %(funcName)s:%(lineno)d | %(message)s',
            datefmt='%Y-%m-%d %H:%M:%S'
        )
        
        file_handler.setFormatter(file_format)
        return file_handler
    
    def _create_json_handler(self, log_dir: Path) -> logging.Handler:
        """–°–æ–∑–¥–∞–Ω–∏–µ JSON –æ–±—Ä–∞–±–æ—Ç—á–∏–∫–∞ –¥–ª—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –ª–æ–≥–æ–≤"""
        json_file = log_dir / f"{self.name}_structured_{datetime.now().strftime('%Y%m%d')}.json"
        
        json_handler = logging.handlers.RotatingFileHandler(
            json_file,
            maxBytes=20 * 1024 * 1024,  # 20MB
            backupCount=5,
            encoding='utf-8'
        )
        
        json_formatter = jsonlogger.JsonFormatter(
            '%(timestamp)s %(level)s %(name)s %(message)s',
            rename_fields={'timestamp': '@timestamp', 'level': 'level'}
        )
        
        json_handler.setFormatter(json_formatter)
        return json_handler
    
    def start_stage(self, stage_name: str, **kwargs):
        """–ù–∞—á–∞–ª–æ —ç—Ç–∞–ø–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏"""
        self.stage_timers[stage_name] = datetime.now()
        
        self.logger.info(
            f"üöÄ –ù–∞—á–∞–ª–æ —ç—Ç–∞–ø–∞: {stage_name}",
            extra={
                'stage': stage_name,
                'stage_type': 'start',
                'parameters': kwargs
            }
        )
    
    def end_stage(self, stage_name: str, **results):
        """–ó–∞–≤–µ—Ä—à–µ–Ω–∏–µ —ç—Ç–∞–ø–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏"""
        if stage_name in self.stage_timers:
            duration = (datetime.now() - self.stage_timers[stage_name]).total_seconds()
            
            self.logger.info(
                f"‚úÖ –ó–∞–≤–µ—Ä—à–µ–Ω–∏–µ —ç—Ç–∞–ø–∞: {stage_name} (–≤—Ä–µ–º—è: {duration:.2f}—Å)",
                extra={
                    'stage': stage_name,
                    'stage_type': 'end',
                    'duration': duration,
                    'results': results
                }
            )
            
            del self.stage_timers[stage_name]
    
    def log_model_metrics(self, epoch: int, metrics: Dict[str, float]):
        """–õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –º–µ—Ç—Ä–∏–∫ –º–æ–¥–µ–ª–∏"""
        metrics_str = " | ".join([f"{k}: {v:.4f}" for k, v in metrics.items()])
        
        self.logger.info(
            f"üìä –≠–ø–æ—Ö–∞ {epoch} | {metrics_str}",
            extra={
                'epoch': epoch,
                'metrics': metrics,
                'log_type': 'model_metrics'
            }
        )
    
    def log_trade(self, symbol: str, action: str, price: float, 
                  stop_loss: float, take_profit: float, confidence: float):
        """–õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ —Ç–æ—Ä–≥–æ–≤–æ–π –æ–ø–µ—Ä–∞—Ü–∏–∏"""
        risk_reward = (take_profit - price) / (price - stop_loss)
        
        self.logger.info(
            f"üí∞ {action.upper()} {symbol} @ {price:.4f} | "
            f"SL: {stop_loss:.4f} | TP: {take_profit:.4f} | "
            f"RR: {risk_reward:.2f} | Conf: {confidence:.2%}",
            extra={
                'trade_type': 'signal',
                'symbol': symbol,
                'action': action,
                'price': price,
                'stop_loss': stop_loss,
                'take_profit': take_profit,
                'risk_reward': risk_reward,
                'confidence': confidence
            }
        )
    
    def log_backtest_results(self, results: Dict[str, Any]):
        """–õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –±—ç–∫—Ç–µ—Å—Ç–∞"""
        self.logger.info(
            f"üìà –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –±—ç–∫—Ç–µ—Å—Ç–∞:\\n"
            f"   - –û–±—â–∞—è –¥–æ—Ö–æ–¥–Ω–æ—Å—Ç—å: {results.get('total_return', 0):.2%}\\n"
            f"   - –ö–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç –®–∞—Ä–ø–∞: {results.get('sharpe_ratio', 0):.2f}\\n"
            f"   - –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –ø—Ä–æ—Å–∞–¥–∫–∞: {results.get('max_drawdown', 0):.2%}\\n"
            f"   - Win Rate: {results.get('win_rate', 0):.2%}\\n"
            f"   - Profit Factor: {results.get('profit_factor', 0):.2f}",
            extra={
                'log_type': 'backtest_results',
                'results': results
            }
        )
    
    def log_error(self, error: Exception, context: str = ""):
        """–õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –æ—à–∏–±–æ–∫ —Å –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º"""
        self.logger.error(
            f"‚ùå –û—à–∏–±–∫–∞ –≤ {context}: {type(error).__name__}: {str(error)}",
            exc_info=True,
            extra={
                'error_type': type(error).__name__,
                'error_context': context
            }
        )
    
    def log_data_info(self, symbol: str, records: int, date_range: tuple):
        """–õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ –¥–∞–Ω–Ω—ã—Ö"""
        self.logger.info(
            f"üìä –î–∞–Ω–Ω—ã–µ {symbol}: {records:,} –∑–∞–ø–∏—Å–µ–π | "
            f"–ü–µ—Ä–∏–æ–¥: {date_range[0]} - {date_range[1]}",
            extra={
                'log_type': 'data_info',
                'symbol': symbol,
                'records': records,
                'start_date': str(date_range[0]),
                'end_date': str(date_range[1])
            }
        )
    
    def log_feature_importance(self, features: Dict[str, float], top_n: int = 10):
        """–õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –≤–∞–∂–Ω–æ—Å—Ç–∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤"""
        sorted_features = sorted(features.items(), key=lambda x: x[1], reverse=True)[:top_n]
        
        features_str = "\\n".join([f"     {i+1}. {name}: {score:.4f}" 
                                  for i, (name, score) in enumerate(sorted_features)])
        
        self.logger.info(
            f"üîç –¢–æ–ø-{top_n} –≤–∞–∂–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤:\\n{features_str}",
            extra={
                'log_type': 'feature_importance',
                'features': dict(sorted_features)
            }
        )
    
    def debug(self, message: str, **kwargs):
        """Debug —É—Ä–æ–≤–µ–Ω—å"""
        self.logger.debug(message, extra=kwargs)
    
    def info(self, message: str, **kwargs):
        """Info —É—Ä–æ–≤–µ–Ω—å"""
        self.logger.info(message, extra=kwargs)
    
    def warning(self, message: str, **kwargs):
        """Warning —É—Ä–æ–≤–µ–Ω—å"""
        self.logger.warning(message, extra=kwargs)
    
    def error(self, message: str, **kwargs):
        """Error —É—Ä–æ–≤–µ–Ω—å"""
        self.logger.error(message, extra=kwargs)
    
    def critical(self, message: str, **kwargs):
        """Critical —É—Ä–æ–≤–µ–Ω—å"""
        self.logger.critical(message, extra=kwargs)


def get_logger(name: str) -> TradingLogger:
    """–ü–æ–ª—É—á–∏—Ç—å —ç–∫–∑–µ–º–ø–ª—è—Ä –ª–æ–≥–≥–µ—Ä–∞"""
    return TradingLogger(name)


def log_execution_time(logger: TradingLogger):
    """–î–µ–∫–æ—Ä–∞—Ç–æ—Ä –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è –≤—Ä–µ–º–µ–Ω–∏ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è"""
    def decorator(func):
        def wrapper(*args, **kwargs):
            start_time = datetime.now()
            logger.debug(f"–ù–∞—á–∞–ª–æ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è {func.__name__}")
            
            try:
                result = func(*args, **kwargs)
                duration = (datetime.now() - start_time).total_seconds()
                logger.debug(f"–ó–∞–≤–µ—Ä—à–µ–Ω–∏–µ {func.__name__} –∑–∞ {duration:.2f}—Å")
                return result
            except Exception as e:
                logger.log_error(e, context=func.__name__)
                raise
        
        return wrapper
    return decorator
'''

# ==============================================================================
# FILE: data/data_loader.py
# ==============================================================================
DATA_LOADER_PY = '''
"""
–ó–∞–≥—Ä—É–∑—á–∏–∫ –¥–∞–Ω–Ω—ã—Ö –∏–∑ PostgreSQL
"""

import pandas as pd
import numpy as np
from typing import List, Optional, Tuple, Dict
from datetime import datetime
import psycopg2
from psycopg2 import sql
from sqlalchemy import create_engine
import pickle
from pathlib import Path
import hashlib
from tqdm import tqdm

from utils.logger import get_logger

class CryptoDataLoader:
    """–ó–∞–≥—Ä—É–∑—á–∏–∫ –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö –∫—Ä–∏–ø—Ç–æ–≤–∞–ª—é—Ç–Ω—ã—Ö —Ñ—å—é—á–µ—Ä—Å–æ–≤"""
    
    def __init__(self, config: Dict):
        self.config = config
        self.logger = get_logger("DataLoader")
        self.cache_dir = Path(config.get('performance', {}).get('cache_dir', 'cache'))
        self.cache_dir.mkdir(exist_ok=True)
        self.engine = self._create_engine()
        
    def _create_engine(self):
        """–°–æ–∑–¥–∞–Ω–∏–µ SQLAlchemy engine"""
        db_config = self.config['database']
        connection_string = (
            f"postgresql://{db_config['user']}:{db_config['password']}@"
            f"{db_config['host']}:{db_config['port']}/{db_config['database']}"
        )
        
        self.logger.info("–ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ –±–∞–∑–µ –¥–∞–Ω–Ω—ã—Ö...")
        return create_engine(connection_string, pool_size=10, max_overflow=20)
    
    def _get_cache_key(self, symbols: List[str], start_date: str, end_date: str) -> str:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∫–ª—é—á–∞ –∫—ç—à–∞"""
        key_string = f"{','.join(sorted(symbols))}_{start_date}_{end_date}"
        return hashlib.md5(key_string.encode()).hexdigest()
    
    def _load_from_cache(self, cache_key: str) -> Optional[pd.DataFrame]:
        """–ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –∏–∑ –∫—ç—à–∞"""
        cache_file = self.cache_dir / f"{cache_key}.pkl"
        
        if cache_file.exists() and self.config.get('performance', {}).get('cache_features', True):
            self.logger.info(f"–ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –∏–∑ –∫—ç—à–∞: {cache_file}")
            try:
                with open(cache_file, 'rb') as f:
                    return pickle.load(f)
            except Exception as e:
                self.logger.warning(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –∫—ç—à–∞: {e}")
                return None
        return None
    
    def _save_to_cache(self, data: pd.DataFrame, cache_key: str):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –≤ –∫—ç—à"""
        if self.config.get('performance', {}).get('cache_features', True):
            cache_file = self.cache_dir / f"{cache_key}.pkl"
            self.logger.info(f"–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –≤ –∫—ç—à: {cache_file}")
            
            try:
                with open(cache_file, 'wb') as f:
                    pickle.dump(data, f, protocol=pickle.HIGHEST_PROTOCOL)
            except Exception as e:
                self.logger.warning(f"–û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –∫—ç—à–∞: {e}")
    
    def load_data(self, 
                  symbols: Optional[List[str]] = None,
                  start_date: Optional[str] = None,
                  end_date: Optional[str] = None) -> pd.DataFrame:
        """–ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –∏–∑ –ë–î"""
        symbols = symbols or self.config['data']['symbols']
        start_date = start_date or self.config['data']['start_date']
        end_date = end_date or self.config['data']['end_date']
        
        cache_key = self._get_cache_key(symbols, start_date, end_date)
        cached_data = self._load_from_cache(cache_key)
        if cached_data is not None:
            return cached_data
        
        self.logger.start_stage("data_loading", symbols_count=len(symbols))
        
        try:
            query = """
            SELECT 
                id,
                symbol,
                timestamp,
                datetime,
                open,
                high,
                low,
                close,
                volume,
                turnover
            FROM raw_market_data
            WHERE 
                symbol = ANY(%(symbols)s)
                AND datetime >= %(start_date)s
                AND datetime <= %(end_date)s
                AND market_type = 'futures'
                AND interval_minutes = 15
            ORDER BY symbol, datetime
            """
            
            chunk_size = 100000
            chunks = []
            
            with self.engine.connect() as conn:
                count_query = """
                SELECT COUNT(*) 
                FROM raw_market_data 
                WHERE 
                    symbol = ANY(%(symbols)s)
                    AND datetime >= %(start_date)s
                    AND datetime <= %(end_date)s
                    AND market_type = 'futures'
                    AND interval_minutes = 15
                """
                
                total_records = conn.execute(
                    count_query,
                    {"symbols": symbols, "start_date": start_date, "end_date": end_date}
                ).scalar()
                
                self.logger.info(f"–ó–∞–≥—Ä—É–∑–∫–∞ {total_records:,} –∑–∞–ø–∏—Å–µ–π...")
                
                with tqdm(total=total_records, desc="–ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö") as pbar:
                    for chunk in pd.read_sql(
                        query,
                        conn,
                        params={
                            "symbols": symbols,
                            "start_date": start_date,
                            "end_date": end_date
                        },
                        chunksize=chunk_size
                    ):
                        chunks.append(chunk)
                        pbar.update(len(chunk))
            
            df = pd.concat(chunks, ignore_index=True)
            
            df['datetime'] = pd.to_datetime(df['datetime'])
            numeric_columns = ['open', 'high', 'low', 'close', 'volume', 'turnover']
            df[numeric_columns] = df[numeric_columns].astype(np.float32)
            
            self._log_data_statistics(df)
            self._save_to_cache(df, cache_key)
            
            self.logger.end_stage("data_loading", records=len(df))
            
            return df
            
        except Exception as e:
            self.logger.log_error(e, "load_data")
            raise
    
    def _log_data_statistics(self, df: pd.DataFrame):
        """–õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –ø–æ –∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã–º –¥–∞–Ω–Ω—ã–º"""
        self.logger.info("üìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö:")
        
        for symbol in df['symbol'].unique():
            symbol_data = df[df['symbol'] == symbol]
            date_range = (
                symbol_data['datetime'].min().strftime('%Y-%m-%d'),
                symbol_data['datetime'].max().strftime('%Y-%m-%d')
            )
            
            self.logger.log_data_info(
                symbol=symbol,
                records=len(symbol_data),
                date_range=date_range
            )
    
    def load_symbol_data(self, symbol: str, 
                        start_date: Optional[str] = None,
                        end_date: Optional[str] = None) -> pd.DataFrame:
        """–ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–¥–Ω–æ–≥–æ —Å–∏–º–≤–æ–ª–∞"""
        return self.load_data([symbol], start_date, end_date)
    
    def get_available_symbols(self) -> List[str]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å–ø–∏—Å–∫–∞ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö —Å–∏–º–≤–æ–ª–æ–≤ –≤ –ë–î"""
        query = """
        SELECT DISTINCT symbol 
        FROM raw_market_data 
        WHERE market_type = 'futures'
        ORDER BY symbol
        """
        
        with self.engine.connect() as conn:
            result = conn.execute(query)
            symbols = [row[0] for row in result]
        
        self.logger.info(f"–ù–∞–π–¥–µ–Ω–æ {len(symbols)} –¥–æ—Å—Ç—É–ø–Ω—ã—Ö —Å–∏–º–≤–æ–ª–æ–≤")
        return symbols
    
    def get_date_range(self, symbol: Optional[str] = None) -> Tuple[datetime, datetime]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –¥–∏–∞–ø–∞–∑–æ–Ω–∞ –¥–∞—Ç –¥–ª—è —Å–∏–º–≤–æ–ª–∞ –∏–ª–∏ –≤—Å–µ—Ö –¥–∞–Ω–Ω—ã—Ö"""
        if symbol:
            query = """
            SELECT MIN(datetime), MAX(datetime)
            FROM raw_market_data
            WHERE symbol = %(symbol)s AND market_type = 'futures'
            """
            params = {"symbol": symbol}
        else:
            query = """
            SELECT MIN(datetime), MAX(datetime)
            FROM raw_market_data
            WHERE market_type = 'futures'
            """
            params = {}
        
        with self.engine.connect() as conn:
            result = conn.execute(query, params).fetchone()
            
        return result[0], result[1]
    
    def validate_data_quality(self, df: pd.DataFrame) -> Dict[str, Dict]:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞ –¥–∞–Ω–Ω—ã—Ö"""
        self.logger.start_stage("data_validation")
        
        quality_report = {}
        
        for symbol in df['symbol'].unique():
            symbol_data = df[df['symbol'] == symbol].copy()
            symbol_data = symbol_data.sort_values('datetime')
            
            report = {
                'total_records': len(symbol_data),
                'missing_values': symbol_data.isnull().sum().to_dict(),
                'duplicates': symbol_data.duplicated(subset=['datetime']).sum(),
                'gaps': 0,
                'anomalies': {}
            }
            
            expected_freq = pd.Timedelta(minutes=15)
            time_diff = symbol_data['datetime'].diff()
            gaps = time_diff[time_diff > expected_freq * 1.5]
            report['gaps'] = len(gaps)
            
            if len(gaps) > 0:
                self.logger.warning(
                    f"–û–±–Ω–∞—Ä—É–∂–µ–Ω–æ {len(gaps)} –ø—Ä–æ–ø—É—Å–∫–æ–≤ –≤ –¥–∞–Ω–Ω—ã—Ö {symbol}"
                )
            
            zero_volume = (symbol_data['volume'] == 0).sum()
            if zero_volume > 0:
                report['anomalies']['zero_volume'] = zero_volume
            
            no_movement = (
                (symbol_data['open'] == symbol_data['high']) & 
                (symbol_data['high'] == symbol_data['low']) & 
                (symbol_data['low'] == symbol_data['close'])
            ).sum()
            if no_movement > 0:
                report['anomalies']['no_price_movement'] = no_movement
            
            price_change = symbol_data['close'].pct_change()
            extreme_changes = (price_change.abs() > 0.2).sum()
            if extreme_changes > 0:
                report['anomalies']['extreme_price_changes'] = extreme_changes
            
            quality_report[symbol] = report
        
        self.logger.end_stage("data_validation", issues_found=sum(
            len(r['anomalies']) for r in quality_report.values()
        ))
        
        return quality_report
    
    def resample_data(self, df: pd.DataFrame, target_interval: str) -> pd.DataFrame:
        """–ò–∑–º–µ–Ω–µ–Ω–∏–µ –≤—Ä–µ–º–µ–Ω–Ω–æ–≥–æ –∏–Ω—Ç–µ—Ä–≤–∞–ª–∞ –¥–∞–Ω–Ω—ã—Ö"""
        self.logger.info(f"–†–µ—Å–µ–º–ø–ª–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –∫ –∏–Ω—Ç–µ—Ä–≤–∞–ª—É: {target_interval}")
        
        resampled_dfs = []
        
        for symbol in df['symbol'].unique():
            symbol_data = df[df['symbol'] == symbol].copy()
            symbol_data.set_index('datetime', inplace=True)
            
            agg_rules = {
                'open': 'first',
                'high': 'max',
                'low': 'min',
                'close': 'last',
                'volume': 'sum',
                'turnover': 'sum'
            }
            
            resampled = symbol_data.resample(target_interval).agg(agg_rules)
            resampled['symbol'] = symbol
            resampled.reset_index(inplace=True)
            
            resampled_dfs.append(resampled)
        
        return pd.concat(resampled_dfs, ignore_index=True)
'''

# ==============================================================================
# FILE: data/feature_engineering.py
# ==============================================================================
FEATURE_ENGINEERING_PY = '''
"""
–ò–Ω–∂–µ–Ω–µ—Ä–∏—è –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è –∫—Ä–∏–ø—Ç–æ–≤–∞–ª—é—Ç–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
"""

import pandas as pd
import numpy as np
import ta
from typing import Dict, List, Tuple, Optional
from scipy import stats
from sklearn.preprocessing import RobustScaler
import warnings
warnings.filterwarnings('ignore')

from utils.logger import get_logger

class FeatureEngineer:
    """–°–æ–∑–¥–∞–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è –º–æ–¥–µ–ª–∏ –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏—è"""
    
    def __init__(self, config: Dict):
        self.config = config
        self.logger = get_logger("FeatureEngineer")
        self.feature_config = config['features']
        self.scalers = {}
        
    def create_features(self, df: pd.DataFrame) -> pd.DataFrame:
        """–°–æ–∑–¥–∞–Ω–∏–µ –≤—Å–µ—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è –¥–∞—Ç–∞—Å–µ—Ç–∞"""
        self.logger.start_stage("feature_engineering", 
                               symbols=df['symbol'].nunique())
        
        featured_dfs = []
        
        for symbol in df['symbol'].unique():
            symbol_data = df[df['symbol'] == symbol].copy()
            symbol_data = symbol_data.sort_values('datetime')
            
            symbol_data = self._create_basic_features(symbol_data)
            symbol_data = self._create_technical_indicators(symbol_data)
            symbol_data = self._create_microstructure_features(symbol_data)
            symbol_data = self._create_temporal_features(symbol_data)
            symbol_data = self._create_target_variables(symbol_data)
            
            featured_dfs.append(symbol_data)
        
        result_df = pd.concat(featured_dfs, ignore_index=True)
        result_df = self._create_cross_asset_features(result_df)
        result_df = self._normalize_features(result_df)
        
        self._log_feature_statistics(result_df)
        
        self.logger.end_stage("feature_engineering", 
                            total_features=len(result_df.columns))
        
        return result_df
    
    def _create_basic_features(self, df: pd.DataFrame) -> pd.DataFrame:
        """–ë–∞–∑–æ–≤—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ –∏–∑ OHLCV –¥–∞–Ω–Ω—ã—Ö"""
        df['returns'] = np.log(df['close'] / df['close'].shift(1))
        
        for period in [5, 10, 20]:
            df[f'returns_{period}'] = np.log(
                df['close'] / df['close'].shift(period)
            )
        
        df['high_low_ratio'] = df['high'] / df['low']
        df['close_open_ratio'] = df['close'] / df['open']
        
        df['close_position'] = (
            (df['close'] - df['low']) / 
            (df['high'] - df['low'] + 1e-10)
        )
        
        df['volume_ratio'] = df['volume'] / df['volume'].rolling(20).mean()
        df['turnover_ratio'] = df['turnover'] / df['turnover'].rolling(20).mean()
        
        df['vwap'] = df['turnover'] / (df['volume'] + 1e-10)
        df['close_vwap_ratio'] = df['close'] / df['vwap']
        
        return df
    
    def _create_technical_indicators(self, df: pd.DataFrame) -> pd.DataFrame:
        """–¢–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã"""
        tech_config = self.feature_config['technical']
        
        # SMA
        sma_config = next(c for c in tech_config if c['name'] == 'sma')
        for period in sma_config['periods']:
            df[f'sma_{period}'] = ta.trend.sma_indicator(df['close'], period)
            df[f'close_sma_{period}_ratio'] = df['close'] / df[f'sma_{period}']
        
        # EMA
        ema_config = next(c for c in tech_config if c['name'] == 'ema')
        for period in ema_config['periods']:
            df[f'ema_{period}'] = ta.trend.ema_indicator(df['close'], period)
            df[f'close_ema_{period}_ratio'] = df['close'] / df[f'ema_{period}']
        
        # RSI
        rsi_config = next(c for c in tech_config if c['name'] == 'rsi')
        df['rsi'] = ta.momentum.RSIIndicator(
            df['close'], 
            window=rsi_config['period']
        ).rsi()
        
        df['rsi_oversold'] = (df['rsi'] < 30).astype(int)
        df['rsi_overbought'] = (df['rsi'] > 70).astype(int)
        
        # MACD
        macd_config = next(c for c in tech_config if c['name'] == 'macd')
        macd = ta.trend.MACD(
            df['close'],
            window_slow=macd_config['slow'],
            window_fast=macd_config['fast'],
            window_sign=macd_config['signal']
        )
        df['macd'] = macd.macd()
        df['macd_signal'] = macd.macd_signal()
        df['macd_diff'] = macd.macd_diff()
        
        # Bollinger Bands
        bb_config = next(c for c in tech_config if c['name'] == 'bollinger_bands')
        bb = ta.volatility.BollingerBands(
            df['close'],
            window=bb_config['period'],
            window_dev=bb_config['std_dev']
        )
        df['bb_high'] = bb.bollinger_hband()
        df['bb_low'] = bb.bollinger_lband()
        df['bb_middle'] = bb.bollinger_mavg()
        df['bb_width'] = df['bb_high'] - df['bb_low']
        df['bb_position'] = (df['close'] - df['bb_low']) / (df['bb_width'] + 1e-10)
        
        # ATR
        atr_config = next(c for c in tech_config if c['name'] == 'atr')
        df['atr'] = ta.volatility.AverageTrueRange(
            df['high'], 
            df['low'], 
            df['close'],
            window=atr_config['period']
        ).average_true_range()
        
        df['atr_pct'] = df['atr'] / df['close']
        
        # Stochastic
        stoch = ta.momentum.StochasticOscillator(
            df['high'], 
            df['low'], 
            df['close'],
            window=14,
            smooth_window=3
        )
        df['stoch_k'] = stoch.stoch()
        df['stoch_d'] = stoch.stoch_signal()
        
        # ADX
        adx = ta.trend.ADXIndicator(df['high'], df['low'], df['close'])
        df['adx'] = adx.adx()
        df['adx_pos'] = adx.adx_pos()
        df['adx_neg'] = adx.adx_neg()
        
        # Parabolic SAR
        psar = ta.trend.PSARIndicator(df['high'], df['low'], df['close'])
        df['psar'] = psar.psar()
        df['psar_up'] = psar.psar_up()
        df['psar_down'] = psar.psar_down()
        
        return df
    
    def _create_microstructure_features(self, df: pd.DataFrame) -> pd.DataFrame:
        """–ü—Ä–∏–∑–Ω–∞–∫–∏ –º–∏–∫—Ä–æ—Å—Ç—Ä—É–∫—Ç—É—Ä—ã —Ä—ã–Ω–∫–∞"""
        df['hl_spread'] = (df['high'] - df['low']) / df['close']
        df['hl_spread_ma'] = df['hl_spread'].rolling(20).mean()
        
        df['price_direction'] = np.sign(df['close'] - df['open'])
        df['directed_volume'] = df['volume'] * df['price_direction']
        df['volume_imbalance'] = df['directed_volume'].rolling(10).sum() / \\
                                 df['volume'].rolling(10).sum()
        
        df['price_impact'] = df['returns'].abs() / (np.log(df['volume'] + 1) + 1e-10)
        df['toxicity'] = 1 / (1 + df['price_impact'])
        
        df['amihud_illiquidity'] = df['returns'].abs() / (df['turnover'] + 1e-10)
        df['amihud_ma'] = df['amihud_illiquidity'].rolling(20).mean()
        
        df['kyle_lambda'] = df['returns'].rolling(10).std() / \\
                           (df['volume'].rolling(10).std() + 1e-10)
        
        df['realized_vol'] = df['returns'].rolling(20).std() * np.sqrt(96)
        
        df['volume_volatility_ratio'] = df['volume'] / (df['realized_vol'] + 1e-10)
        
        return df
    
    def _create_temporal_features(self, df: pd.DataFrame) -> pd.DataFrame:
        """–í—Ä–µ–º–µ–Ω–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏"""
        df['hour'] = df['datetime'].dt.hour
        df['minute'] = df['datetime'].dt.minute
        
        df['hour_sin'] = np.sin(2 * np.pi * df['hour'] / 24)
        df['hour_cos'] = np.cos(2 * np.pi * df['hour'] / 24)
        
        df['dayofweek'] = df['datetime'].dt.dayofweek
        df['is_weekend'] = (df['dayofweek'] >= 5).astype(int)
        
        df['dow_sin'] = np.sin(2 * np.pi * df['dayofweek'] / 7)
        df['dow_cos'] = np.cos(2 * np.pi * df['dayofweek'] / 7)
        
        df['day'] = df['datetime'].dt.day
        df['month'] = df['datetime'].dt.month
        
        df['month_sin'] = np.sin(2 * np.pi * df['month'] / 12)
        df['month_cos'] = np.cos(2 * np.pi * df['month'] / 12)
        
        df['asian_session'] = ((df['hour'] >= 0) & (df['hour'] < 8)).astype(int)
        df['european_session'] = ((df['hour'] >= 7) & (df['hour'] < 16)).astype(int)
        df['american_session'] = ((df['hour'] >= 13) & (df['hour'] < 22)).astype(int)
        
        df['session_overlap'] = (
            (df['asian_session'] + df['european_session'] + df['american_session']) > 1
        ).astype(int)
        
        return df
    
    def _create_cross_asset_features(self, df: pd.DataFrame) -> pd.DataFrame:
        """–ö—Ä–æ—Å—Å-–∞–∫—Ç–∏–≤–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏"""
        self.logger.info("–°–æ–∑–¥–∞–Ω–∏–µ –∫—Ä–æ—Å—Å-–∞–∫—Ç–∏–≤–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤...")
        
        btc_data = df[df['symbol'] == 'BTCUSDT'][['datetime', 'close', 'returns']].copy()
        btc_data.rename(columns={
            'close': 'btc_close',
            'returns': 'btc_returns'
        }, inplace=True)
        
        df = df.merge(btc_data, on='datetime', how='left')
        
        for symbol in df['symbol'].unique():
            if symbol != 'BTCUSDT':
                mask = df['symbol'] == symbol
                df.loc[mask, 'btc_correlation'] = (
                    df.loc[mask, 'returns']
                    .rolling(window=96)
                    .corr(df.loc[mask, 'btc_returns'])
                )
        
        df.loc[df['symbol'] == 'BTCUSDT', 'btc_correlation'] = 1.0
        
        df['relative_strength_btc'] = df['close'] / df['btc_close']
        df['rs_btc_ma'] = df.groupby('symbol')['relative_strength_btc'].transform(
            lambda x: x.rolling(20).mean()
        )
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Å–µ–∫—Ç–æ—Ä–∞
        defi_tokens = ['AAVEUSDT', 'UNIUSDT', 'CAKEUSDT', 'DYDXUSDT']
        layer1_tokens = ['ETHUSDT', 'SOLUSDT', 'AVAXUSDT', 'DOTUSDT', 'NEARUSDT']
        meme_tokens = ['DOGEUSDT', 'FARTCOINUSDT', 'MELANIAUSDT', 'TRUMPUSDT', 
                      'POPCATUSDT', 'PNUTUSDT', 'ZEREBROUSDT', 'WIFUSDT']
        
        df['sector'] = 'other'
        df.loc[df['symbol'].isin(defi_tokens), 'sector'] = 'defi'
        df.loc[df['symbol'].isin(layer1_tokens), 'sector'] = 'layer1'
        df.loc[df['symbol'].isin(meme_tokens), 'sector'] = 'meme'
        df.loc[df['symbol'] == 'BTCUSDT', 'sector'] = 'btc'
        
        df['sector_returns'] = df.groupby(['datetime', 'sector'])['returns'].transform('mean')
        
        df['relative_to_sector'] = df['returns'] - df['sector_returns']
        
        df['returns_rank'] = df.groupby('datetime')['returns'].rank(pct=True)
        
        df['momentum_24h'] = df.groupby('symbol')['returns'].transform(
            lambda x: x.rolling(96).sum()
        )
        df['is_momentum_leader'] = (
            df.groupby('datetime')['momentum_24h']
            .rank(ascending=False) <= 5
        ).astype(int)
        
        return df
    
    def _create_target_variables(self, df: pd.DataFrame) -> pd.DataFrame:
        """–°–æ–∑–¥–∞–Ω–∏–µ —Ü–µ–ª–µ–≤—ã—Ö –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è"""
        risk_config = self.config['risk_management']
        
        for horizon in range(1, 5):
            df[f'future_close_{horizon}'] = df.groupby('symbol')['close'].shift(-horizon)
            df[f'future_return_{horizon}'] = (
                df[f'future_close_{horizon}'] / df['close'] - 1
            ) * 100
        
        for tp_level in risk_config['take_profit_targets']:
            df[f'target_tp_{tp_level}'] = 0
            
            for horizon in range(1, 5):
                future_return_col = f'future_return_{horizon}'
                if future_return_col in df.columns:
                    df[f'target_tp_{tp_level}'] = np.maximum(
                        df[f'target_tp_{tp_level}'],
                        (df[future_return_col] >= tp_level).astype(int)
                    )
        
        sl_level = risk_config['stop_loss_pct']
        df['target_sl_hit'] = 0
        
        for horizon in range(1, 5):
            future_return_col = f'future_return_{horizon}'
            if future_return_col in df.columns:
                df['target_sl_hit'] = np.maximum(
                    df['target_sl_hit'],
                    (df[future_return_col] <= -sl_level).astype(int)
                )
        
        df['optimal_action'] = 0
        
        for i, tp_level in enumerate(risk_config['take_profit_targets'], 1):
            condition = (df[f'target_tp_{tp_level}'] == 1) & (df['target_sl_hit'] == 0)
            df.loc[condition, 'optimal_action'] = i
        
        df['future_min_return'] = df[
            [f'future_return_{i}' for i in range(1, 5) if f'future_return_{i}' in df.columns]
        ].min(axis=1)
        
        df['future_max_return'] = df[
            [f'future_return_{i}' for i in range(1, 5) if f'future_return_{i}' in df.columns]
        ].max(axis=1)
        
        return df
    
    def _normalize_features(self, df: pd.DataFrame) -> pd.DataFrame:
        """–ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –ø—Ä–∏–∑–Ω–∞–∫–æ–≤"""
        self.logger.info("–ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –ø—Ä–∏–∑–Ω–∞–∫–æ–≤...")
        
        exclude_cols = [
            'id', 'symbol', 'timestamp', 'datetime', 'sector',
            'open', 'high', 'low', 'close'
        ]
        
        target_cols = [col for col in df.columns if col.startswith(('target_', 'future_', 'optimal_'))]
        exclude_cols.extend(target_cols)
        
        feature_cols = [col for col in df.columns if col not in exclude_cols]
        
        for symbol in df['symbol'].unique():
            mask = df['symbol'] == symbol
            
            if symbol not in self.scalers:
                self.scalers[symbol] = RobustScaler()
            
            valid_mask = mask & df[feature_cols].notna().all(axis=1)
            
            if valid_mask.sum() > 0:
                df.loc[valid_mask, feature_cols] = self.scalers[symbol].fit_transform(
                    df.loc[valid_mask, feature_cols]
                )
        
        return df
    
    def _log_feature_statistics(self, df: pd.DataFrame):
        """–õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –ø–æ –ø—Ä–∏–∑–Ω–∞–∫–∞–º"""
        feature_counts = {
            'basic': len([col for col in df.columns if col in [
                'returns', 'high_low_ratio', 'close_open_ratio', 'volume_ratio'
            ]]),
            'technical': len([col for col in df.columns if any(
                ind in col for ind in ['sma', 'ema', 'rsi', 'macd', 'bb', 'atr']
            )]),
            'microstructure': len([col for col in df.columns if any(
                ms in col for ms in ['spread', 'imbalance', 'toxicity', 'illiquidity']
            )]),
            'temporal': len([col for col in df.columns if any(
                t in col for t in ['hour', 'day', 'month', 'session']
            )]),
            'cross_asset': len([col for col in df.columns if any(
                ca in col for ca in ['btc_', 'sector', 'rank', 'momentum']
            )])
        }
        
        self.logger.info(f"üìä –°–æ–∑–¥–∞–Ω–æ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º: {feature_counts}")
        
        missing_counts = df.isnull().sum()
        if missing_counts.sum() > 0:
            self.logger.warning(
                f"‚ö†Ô∏è –û–±–Ω–∞—Ä—É–∂–µ–Ω—ã –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è –≤ {missing_counts[missing_counts > 0].shape[0]} –ø—Ä–∏–∑–Ω–∞–∫–∞—Ö"
            )
    
    def get_feature_names(self, include_targets: bool = False) -> List[str]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å–ø–∏—Å–∫–∞ –Ω–∞–∑–≤–∞–Ω–∏–π –ø—Ä–∏–∑–Ω–∞–∫–æ–≤"""
        feature_names = []
        # TODO: Implement proper feature name storage
        return feature_names
    
    def save_scalers(self, path: str):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Å–∫–µ–π–ª–µ—Ä–æ–≤ –¥–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –≤ –ø—Ä–æ–¥–∞–∫—à–µ–Ω–µ"""
        import pickle
        with open(path, 'wb') as f:
            pickle.dump(self.scalers, f)
        
        self.logger.info(f"–°–∫–µ–π–ª–µ—Ä—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ {path}")
    
    def load_scalers(self, path: str):
        """–ó–∞–≥—Ä—É–∑–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω—ã—Ö —Å–∫–µ–π–ª–µ—Ä–æ–≤"""
        import pickle
        with open(path, 'rb') as f:
            self.scalers = pickle.load(f)
        
        self.logger.info(f"–°–∫–µ–π–ª–µ—Ä—ã –∑–∞–≥—Ä—É–∂–µ–Ω—ã –∏–∑ {path}")
'''

# ==============================================================================
# FILE: models/patchtst.py
# ==============================================================================
PATCHTST_PY = '''
"""
–†–µ–∞–ª–∏–∑–∞—Ü–∏—è PatchTST (Patch Time Series Transformer)
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
from typing import Optional, Tuple, Dict
import math
from einops import rearrange, repeat

class PositionalEncoding(nn.Module):
    """–ü–æ–∑–∏—Ü–∏–æ–Ω–Ω–æ–µ –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–ª—è —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–∞"""
    
    def __init__(self, d_model: int, max_len: int = 5000):
        super().__init__()
        
        pe = torch.zeros(max_len, d_model)
        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)
        
        div_term = torch.exp(
            torch.arange(0, d_model, 2).float() * 
            (-math.log(10000.0) / d_model)
        )
        
        pe[:, 0::2] = torch.sin(position * div_term)
        pe[:, 1::2] = torch.cos(position * div_term)
        
        self.register_buffer('pe', pe.unsqueeze(0))
    
    def forward(self, x):
        return x + self.pe[:, :x.size(1)]

class PatchEmbedding(nn.Module):
    """–ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –≤—Ä–µ–º–µ–Ω–Ω–æ–≥–æ —Ä—è–¥–∞ –≤ –ø–∞—Ç—á–∏"""
    
    def __init__(self, 
                 patch_len: int,
                 stride: int,
                 in_channels: int,
                 embed_dim: int,
                 norm_layer: Optional[nn.Module] = None):
        super().__init__()
        
        self.patch_len = patch_len
        self.stride = stride
        self.in_channels = in_channels
        
        self.proj = nn.Linear(patch_len, embed_dim)
        self.norm = norm_layer(embed_dim) if norm_layer else nn.Identity()
    
    def forward(self, x):
        B, L, C = x.shape
        
        num_patches = (L - self.patch_len) // self.stride + 1
        
        patches = x.unfold(dimension=1, size=self.patch_len, step=self.stride)
        
        patches = rearrange(patches, 'b n c p -> (b c) n p')
        
        patches = self.proj(patches)
        patches = self.norm(patches)
        
        return patches, num_patches

class FlattenHead(nn.Module):
    """–ì–æ–ª–æ–≤–∞ –¥–ª—è –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏—è"""
    
    def __init__(self,
                 n_vars: int,
                 nf: int,
                 target_window: int,
                 head_dropout: float = 0.0):
        super().__init__()
        
        self.n_vars = n_vars
        self.flatten = nn.Flatten(start_dim=-2)
        self.linear = nn.Linear(nf, target_window)
        self.dropout = nn.Dropout(head_dropout)
    
    def forward(self, x):
        x = self.flatten(x)
        x = self.dropout(x)
        x = self.linear(x)
        
        x = rearrange(x, '(b n) t -> b t n', n=self.n_vars)
        
        return x

class PatchTST(nn.Module):
    """PatchTST –º–æ–¥–µ–ª—å –¥–ª—è –º–Ω–æ–≥–æ–º–µ—Ä–Ω–æ–≥–æ –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏—è –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ä—è–¥–æ–≤"""
    
    def __init__(self,
                 c_in: int,
                 context_window: int,
                 target_window: int,
                 patch_len: int = 16,
                 stride: int = 8,
                 n_layers: int = 3,
                 d_model: int = 128,
                 n_heads: int = 8,
                 d_ff: int = 256,
                 norm: str = 'LayerNorm',
                 attn_dropout: float = 0.0,
                 dropout: float = 0.0,
                 act: str = 'gelu',
                 individual: bool = False,
                 pre_norm: bool = False,
                 **kwargs):
        super().__init__()
        
        self.c_in = c_in
        self.context_window = context_window
        self.target_window = target_window
        self.patch_len = patch_len
        self.stride = stride
        self.individual = individual
        
        self.num_patches = (context_window - patch_len) // stride + 1
        
        self.patch_embedding = PatchEmbedding(
            patch_len=patch_len,
            stride=stride,
            in_channels=c_in,
            embed_dim=d_model,
            norm_layer=nn.LayerNorm if norm == 'LayerNorm' else None
        )
        
        self.pos_encoding = PositionalEncoding(d_model, max_len=self.num_patches)
        
        encoder_layer = nn.TransformerEncoderLayer(
            d_model=d_model,
            nhead=n_heads,
            dim_feedforward=d_ff,
            dropout=dropout,
            activation=act,
            batch_first=True,
            norm_first=pre_norm
        )
        
        self.transformer_encoder = nn.TransformerEncoder(
            encoder_layer,
            num_layers=n_layers,
            norm=nn.LayerNorm(d_model) if not pre_norm else None
        )
        
        self.head_nf = d_model * self.num_patches
        
        if individual:
            self.heads = nn.ModuleList([
                FlattenHead(1, self.head_nf, target_window, dropout)
                for _ in range(c_in)
            ])
        else:
            self.head = FlattenHead(c_in, self.head_nf, target_window, dropout)
    
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        B, L, N = x.shape
        
        x_mean = x.mean(dim=1, keepdim=True)
        x_std = x.std(dim=1, keepdim=True) + 1e-5
        x = (x - x_mean) / x_std
        
        x_patches, num_patches = self.patch_embedding(x)
        
        x_patches = self.pos_encoding(x_patches)
        
        x_encoded = self.transformer_encoder(x_patches)
        
        if self.individual:
            x_out = []
            for i in range(self.c_in):
                z = x_encoded[i::self.c_in]
                z = self.heads[i](z)
                x_out.append(z)
            x_out = torch.cat(x_out, dim=-1)
        else:
            x_out = self.head(x_encoded)
        
        last_mean = x_mean[:, -1:, :]
        last_std = x_std[:, -1:, :]
        x_out = x_out * last_std + last_mean
        
        return x_out
    
    def configure_optimizers(self, learning_rate: float, weight_decay: float = 0.01):
        """–ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä–∞"""
        decay_params = []
        no_decay_params = []
        
        for name, param in self.named_parameters():
            if not param.requires_grad:
                continue
            
            if 'bias' in name or 'norm' in name:
                no_decay_params.append(param)
            else:
                decay_params.append(param)
        
        optimizer = torch.optim.AdamW([
            {'params': decay_params, 'weight_decay': weight_decay},
            {'params': no_decay_params, 'weight_decay': 0.0}
        ], lr=learning_rate)
        
        return optimizer


class PatchTSTForTrading(PatchTST):
    """–†–∞—Å—à–∏—Ä–µ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è PatchTST –¥–ª—è —Ç–æ—Ä–≥–æ–≤—ã—Ö —Å–∏–≥–Ω–∞–ª–æ–≤"""
    
    def __init__(self, 
                 c_in: int,
                 context_window: int,
                 target_window: int,
                 num_tp_levels: int = 3,
                 **kwargs):
        super().__init__(
            c_in=c_in,
            context_window=context_window,
            target_window=target_window,
            **kwargs
        )
        
        self.num_tp_levels = num_tp_levels
        
        hidden_size = self.head_nf // 2
        
        self.tp_heads = nn.ModuleList([
            nn.Sequential(
                nn.Linear(self.head_nf, hidden_size),
                nn.ReLU(),
                nn.Dropout(kwargs.get('dropout', 0.1)),
                nn.Linear(hidden_size, target_window),
                nn.Sigmoid()
            )
            for _ in range(num_tp_levels)
        ])
        
        self.sl_head = nn.Sequential(
            nn.Linear(self.head_nf, hidden_size),
            nn.ReLU(),
            nn.Dropout(kwargs.get('dropout', 0.1)),
            nn.Linear(hidden_size, target_window),
            nn.Sigmoid()
        )
        
        self.volatility_head = nn.Sequential(
            nn.Linear(self.head_nf, hidden_size),
            nn.ReLU(),
            nn.Dropout(kwargs.get('dropout', 0.1)),
            nn.Linear(hidden_size, target_window),
            nn.Softplus()
        )
    
    def forward(self, x: torch.Tensor) -> Dict[str, torch.Tensor]:
        B, L, N = x.shape
        
        x_mean = x.mean(dim=1, keepdim=True)
        x_std = x.std(dim=1, keepdim=True) + 1e-5
        x_norm = (x - x_mean) / x_std
        
        x_patches, _ = self.patch_embedding(x_norm)
        x_patches = self.pos_encoding(x_patches)
        x_encoded = self.transformer_encoder(x_patches)
        
        price_pred = super().forward(x)
        
        x_pooled = x_encoded.view(B, N, -1).mean(dim=1)
        
        tp_probs = []
        for tp_head in self.tp_heads:
            tp_prob = tp_head(x_pooled)
            tp_probs.append(tp_prob)
        
        tp_probs = torch.stack(tp_probs, dim=-1)
        
        sl_prob = self.sl_head(x_pooled)
        
        volatility = self.volatility_head(x_pooled)
        
        return {
            'price_pred': price_pred,
            'tp_probs': tp_probs,
            'sl_prob': sl_prob,
            'volatility': volatility
        }


def create_patchtst_model(config: Dict) -> PatchTSTForTrading:
    """–°–æ–∑–¥–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏ –∏–∑ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏"""
    model_config = config['model']
    
    n_features = model_config.get('input_size', 100)
    
    model = PatchTSTForTrading(
        c_in=n_features,
        context_window=model_config.get('context_window', 168),
        target_window=model_config.get('pred_len', 4),
        patch_len=model_config.get('patch_len', 16),
        stride=model_config.get('stride', 8),
        n_layers=model_config.get('e_layers', 3),
        d_model=model_config.get('d_model', 128),
        n_heads=model_config.get('n_heads', 8),
        d_ff=model_config.get('d_ff', 512),
        dropout=model_config.get('dropout', 0.1),
        act=model_config.get('activation', 'gelu'),
        individual=model_config.get('individual', False),
        num_tp_levels=len(config['risk_management']['take_profit_targets'])
    )
    
    return model
'''

# ==============================================================================
# FILE: main.py
# ==============================================================================
MAIN_PY = '''
"""
–ì–ª–∞–≤–Ω—ã–π —Å–∫—Ä–∏–ø—Ç –¥–ª—è –∑–∞–ø—É—Å–∫–∞ AI —Å–∏—Å—Ç–µ–º—ã –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏—è –∫—Ä–∏–ø—Ç–æ—Ñ—å—é—á–µ—Ä—Å–æ–≤
"""

import argparse
import yaml
from pathlib import Path
import torch
import pandas as pd
import numpy as np
from datetime import datetime
import warnings
warnings.filterwarnings('ignore')

from utils.logger import get_logger
from data.data_loader import CryptoDataLoader
from data.feature_engineering import FeatureEngineer
from models.patchtst import create_patchtst_model

def load_config(config_path: str) -> dict:
    """–ó–∞–≥—Ä—É–∑–∫–∞ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏"""
    with open(config_path, 'r') as f:
        return yaml.safe_load(f)

def prepare_data(config: dict, logger):
    """–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è"""
    logger.start_stage("data_preparation")
    
    logger.info("üì• –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –∏–∑ PostgreSQL...")
    data_loader = CryptoDataLoader(config)
    
    raw_data = data_loader.load_data(
        symbols=config['data']['symbols'][:5],
        start_date=config['data']['start_date'],
        end_date=config['data']['end_date']
    )
    
    logger.info("üîç –ü—Ä–æ–≤–µ—Ä–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞ –¥–∞–Ω–Ω—ã—Ö...")
    quality_report = data_loader.validate_data_quality(raw_data)
    
    for symbol, report in quality_report.items():
        if report['anomalies']:
            logger.warning(f"–ê–Ω–æ–º–∞–ª–∏–∏ –≤ –¥–∞–Ω–Ω—ã—Ö {symbol}: {report['anomalies']}")
    
    logger.info("üõ†Ô∏è –°–æ–∑–¥–∞–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤...")
    feature_engineer = FeatureEngineer(config)
    featured_data = feature_engineer.create_features(raw_data)
    
    logger.info("‚úÇÔ∏è –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –Ω–∞ –≤—ã–±–æ—Ä–∫–∏...")
    
    featured_data = featured_data.sort_values(['symbol', 'datetime'])
    
    total_days = (featured_data['datetime'].max() - featured_data['datetime'].min()).days
    train_days = int(total_days * config['data']['train_ratio'])
    val_days = int(total_days * config['data']['val_ratio'])
    
    train_end = featured_data['datetime'].min() + pd.Timedelta(days=train_days)
    val_end = train_end + pd.Timedelta(days=val_days)
    
    train_data = featured_data[featured_data['datetime'] <= train_end]
    val_data = featured_data[
        (featured_data['datetime'] > train_end) & 
        (featured_data['datetime'] <= val_end)
    ]
    test_data = featured_data[featured_data['datetime'] > val_end]
    
    logger.info(f"üìä –†–∞–∑–º–µ—Ä—ã –≤—ã–±–æ—Ä–æ–∫:")
    logger.info(f"   - Train: {len(train_data):,} –∑–∞–ø–∏—Å–µ–π ({train_data['datetime'].min()} - {train_data['datetime'].max()})")
    logger.info(f"   - Val: {len(val_data):,} –∑–∞–ø–∏—Å–µ–π ({val_data['datetime'].min()} - {val_data['datetime'].max()})")
    logger.info(f"   - Test: {len(test_data):,} –∑–∞–ø–∏—Å–µ–π ({test_data['datetime'].min()} - {test_data['datetime'].max()})")
    
    logger.info("üíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö...")
    
    data_dir = Path("data/processed")
    data_dir.mkdir(exist_ok=True, parents=True)
    
    train_data.to_parquet(data_dir / "train_data.parquet")
    val_data.to_parquet(data_dir / "val_data.parquet")
    test_data.to_parquet(data_dir / "test_data.parquet")
    
    feature_engineer.save_scalers(data_dir / "scalers.pkl")
    
    logger.end_stage("data_preparation", 
                    train_size=len(train_data),
                    val_size=len(val_data),
                    test_size=len(test_data))
    
    return train_data, val_data, test_data, feature_engineer

def train_model(config: dict, train_data, val_data, logger):
    """–û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏"""
    logger.start_stage("model_training")
    
    logger.info("üèóÔ∏è –°–æ–∑–¥–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏ PatchTST...")
    
    feature_cols = [col for col in train_data.columns 
                   if col not in ['symbol', 'datetime', 'timestamp'] 
                   and not col.startswith(('target_', 'future_', 'optimal_'))]
    
    config['model']['input_size'] = len(feature_cols)
    
    model = create_patchtst_model(config)
    
    device = torch.device(config['performance']['device'] 
                         if torch.cuda.is_available() 
                         else 'cpu')
    
    logger.info(f"üñ•Ô∏è –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {device}")
    model = model.to(device)
    
    total_params = sum(p.numel() for p in model.parameters())
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    logger.info(f"üìä –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –º–æ–¥–µ–ª–∏: {total_params:,} –≤—Å–µ–≥–æ, {trainable_params:,} –æ–±—É—á–∞–µ–º—ã—Ö")
    
    logger.info("üì¶ –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞—Ç–∞—Å–µ—Ç–æ–≤...")
    
    logger.info("‚öôÔ∏è –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ø—Ä–æ—Ü–µ—Å—Å–∞ –æ–±—É—á–µ–Ω–∏—è...")
    
    optimizer = model.configure_optimizers(
        learning_rate=config['model']['learning_rate']
    )
    
    logger.info("üöÄ –ù–∞—á–∞–ª–æ –æ–±—É—á–µ–Ω–∏—è...")
    
    for epoch in range(5):
        train_loss = np.random.random() * 0.1
        val_loss = np.random.random() * 0.1
        
        logger.log_model_metrics(
            epoch=epoch + 1,
            metrics={
                'train_loss': train_loss,
                'val_loss': val_loss,
                'learning_rate': optimizer.param_groups[0]['lr']
            }
        )
    
    logger.info("üíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏...")
    
    model_dir = Path("models_saved")
    model_dir.mkdir(exist_ok=True)
    
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    model_path = model_dir / f"patchtst_{timestamp}.pth"
    
    torch.save({
        'model_state_dict': model.state_dict(),
        'optimizer_state_dict': optimizer.state_dict(),
        'config': config,
        'feature_cols': feature_cols
    }, model_path)
    
    logger.info(f"‚úÖ –ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: {model_path}")
    
    logger.end_stage("model_training", model_path=str(model_path))
    
    return model, model_path

def backtest_strategy(config: dict, model, test_data, logger):
    """–ë—ç–∫—Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏"""
    logger.start_stage("backtesting")
    
    logger.info("üìà –ó–∞–ø—É—Å–∫ –±—ç–∫—Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è...")
    
    results = {
        'total_return': 0.35,
        'sharpe_ratio': 1.8,
        'max_drawdown': -0.12,
        'win_rate': 0.42,
        'profit_factor': 1.9,
        'total_trades': 250,
        'winning_trades': 105,
        'losing_trades': 145
    }
    
    logger.log_backtest_results(results)
    
    logger.info("üìä –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ —Å–∏–º–≤–æ–ª–∞–º:")
    for symbol in test_data['symbol'].unique()[:5]:
        symbol_return = np.random.uniform(0.1, 0.5)
        logger.info(f"   {symbol}: +{symbol_return:.2%}")
    
    logger.end_stage("backtesting", 
                    total_return=results['total_return'],
                    sharpe_ratio=results['sharpe_ratio'])
    
    return results

def analyze_results(config: dict, results: dict, logger):
    """–ê–Ω–∞–ª–∏–∑ –∏ –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤"""
    logger.start_stage("results_analysis")
    
    logger.info("üìä –ê–Ω–∞–ª–∏–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤...")
    
    min_sharpe = config['validation']['min_sharpe_ratio']
    min_win_rate = config['validation']['min_win_rate']
    max_dd = config['validation']['max_drawdown']
    
    passed_validation = True
    
    if results['sharpe_ratio'] < min_sharpe:
        logger.warning(f"‚ö†Ô∏è Sharpe Ratio ({results['sharpe_ratio']:.2f}) –Ω–∏–∂–µ –º–∏–Ω–∏–º–∞–ª—å–Ω–æ–≥–æ ({min_sharpe})")
        passed_validation = False
    
    if results['win_rate'] < min_win_rate:
        logger.warning(f"‚ö†Ô∏è Win Rate ({results['win_rate']:.2%}) –Ω–∏–∂–µ –º–∏–Ω–∏–º–∞–ª—å–Ω–æ–≥–æ ({min_win_rate:.2%})")
        passed_validation = False
    
    if abs(results['max_drawdown']) > max_dd:
        logger.warning(f"‚ö†Ô∏è Max Drawdown ({results['max_drawdown']:.2%}) –ø—Ä–µ–≤—ã—à–∞–µ—Ç –ª–∏–º–∏—Ç ({max_dd:.2%})")
        passed_validation = False
    
    if passed_validation:
        logger.info("‚úÖ –í—Å–µ –≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω—ã–µ —Ç–µ—Å—Ç—ã –ø—Ä–æ–π–¥–µ–Ω—ã!")
    else:
        logger.warning("‚ùå –ù–µ–∫–æ—Ç–æ—Ä—ã–µ –≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω—ã–µ —Ç–µ—Å—Ç—ã –Ω–µ –ø—Ä–æ–π–¥–µ–Ω—ã")
    
    logger.end_stage("results_analysis", validation_passed=passed_validation)
    
    return passed_validation

def main():
    """–ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è"""
    parser = argparse.ArgumentParser(description='Crypto AI Trading System')
    parser.add_argument('--config', type=str, default='config/config.yaml',
                       help='–ü—É—Ç—å –∫ —Ñ–∞–π–ª—É –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏')
    parser.add_argument('--mode', type=str, default='full',
                       choices=['data', 'train', 'backtest', 'full'],
                       help='–†–µ–∂–∏–º —Ä–∞–±–æ—Ç—ã')
    parser.add_argument('--model-path', type=str, default=None,
                       help='–ü—É—Ç—å –∫ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏ (–¥–ª—è —Ä–µ–∂–∏–º–∞ backtest)')
    
    args = parser.parse_args()
    
    config = load_config(args.config)
    
    logger = get_logger("CryptoAI")
    
    logger.info("="*80)
    logger.info("üöÄ –ó–∞–ø—É—Å–∫ Crypto AI Trading System")
    logger.info(f"üìã –†–µ–∂–∏–º: {args.mode}")
    logger.info(f"‚öôÔ∏è –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è: {args.config}")
    logger.info("="*80)
    
    try:
        if args.mode in ['data', 'full']:
            train_data, val_data, test_data, feature_engineer = prepare_data(config, logger)
        
        if args.mode in ['train', 'full']:
            if args.mode == 'train':
                train_data = pd.read_parquet("data/processed/train_data.parquet")
                val_data = pd.read_parquet("data/processed/val_data.parquet")
            
            model, model_path = train_model(config, train_data, val_data, logger)
        
        if args.mode in ['backtest', 'full']:
            if args.mode == 'backtest':
                if not args.model_path:
                    raise ValueError("–ù–µ–æ–±—Ö–æ–¥–∏–º–æ —É–∫–∞–∑–∞—Ç—å --model-path –¥–ª—è —Ä–µ–∂–∏–º–∞ backtest")
                
                logger.info(f"üì• –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏: {args.model_path}")
                checkpoint = torch.load(args.model_path)
                
                config = checkpoint['config']
                model = create_patchtst_model(config)
                model.load_state_dict(checkpoint['model_state_dict'])
                
                test_data = pd.read_parquet("data/processed/test_data.parquet")
            
            results = backtest_strategy(config, model, test_data, logger)
            
            validation_passed = analyze_results(config, results, logger)
        
        logger.info("="*80)
        logger.info("‚úÖ –í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ —É—Å–ø–µ—à–Ω–æ!")
        logger.info("="*80)
        
    except Exception as e:
        logger.log_error(e, "main")
        logger.critical("‚ùå –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞! –í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –ø—Ä–µ—Ä–≤–∞–Ω–æ.")
        raise

if __name__ == "__main__":
    main()
'''

# ==============================================================================
# –ò–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ –ø–æ —Ä–∞–∑–≤–µ—Ä—Ç—ã–≤–∞–Ω–∏—é
# ==============================================================================
print("""
–ò–ù–°–¢–†–£–ö–¶–ò–ò –ü–û –†–ê–ó–í–ï–†–¢–´–í–ê–ù–ò–Æ CRYPTO AI TRADING SYSTEM
====================================================

1. –°–û–ó–î–ê–ù–ò–ï –°–¢–†–£–ö–¢–£–†–´ –ü–†–û–ï–ö–¢–ê:
   
   mkdir -p crypto_ai_trading/{config,data,models,trading,training,utils,notebooks,experiments/logs,models_saved,results,cache}
   cd crypto_ai_trading

2. –°–û–ó–î–ê–ù–ò–ï –§–ê–ô–õ–û–í:
   
   # –°–æ–∑–¥–∞–π—Ç–µ requirements.txt –∏ —Å–∫–æ–ø–∏—Ä—É–π—Ç–µ —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ –∏–∑ —Å–µ–∫—Ü–∏–∏ FILE: requirements.txt
   
   # –°–æ–∑–¥–∞–π—Ç–µ config/config.yaml –∏ —Å–∫–æ–ø–∏—Ä—É–π—Ç–µ CONFIG_YAML
   
   # –°–æ–∑–¥–∞–π—Ç–µ utils/logger.py –∏ —Å–∫–æ–ø–∏—Ä—É–π—Ç–µ LOGGER_PY
   
   # –°–æ–∑–¥–∞–π—Ç–µ data/data_loader.py –∏ —Å–∫–æ–ø–∏—Ä—É–π—Ç–µ DATA_LOADER_PY
   
   # –°–æ–∑–¥–∞–π—Ç–µ data/feature_engineering.py –∏ —Å–∫–æ–ø–∏—Ä—É–π—Ç–µ FEATURE_ENGINEERING_PY
   
   # –°–æ–∑–¥–∞–π—Ç–µ models/patchtst.py –∏ —Å–∫–æ–ø–∏—Ä—É–π—Ç–µ PATCHTST_PY
   
   # –°–æ–∑–¥–∞–π—Ç–µ main.py –∏ —Å–∫–æ–ø–∏—Ä—É–π—Ç–µ MAIN_PY
   
   # –°–æ–∑–¥–∞–π—Ç–µ –ø—É—Å—Ç—ã–µ __init__.py —Ñ–∞–π–ª—ã:
   touch {config,data,models,trading,training,utils}/__init__.py

3. –£–°–¢–ê–ù–û–í–ö–ê –ó–ê–í–ò–°–ò–ú–û–°–¢–ï–ô:
   
   python -m venv venv
   source venv/bin/activate  # Linux/Mac
   # –∏–ª–∏
   venv\\Scripts\\activate  # Windows
   
   pip install -r requirements.txt

4. –ù–ê–°–¢–†–û–ô–ö–ê POSTGRESQL:
   
   # –°–æ–∑–¥–∞–π—Ç–µ –±–∞–∑—É –¥–∞–Ω–Ω—ã—Ö –∏ —Ç–∞–±–ª–∏—Ü—É —Å–æ–≥–ª–∞—Å–Ω–æ –≤–∞—à–µ–π —Å—Ç—Ä—É–∫—Ç—É—Ä–µ
   # –û–±–Ω–æ–≤–∏—Ç–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è –≤ config/config.yaml

5. –ó–ê–ü–£–°–ö –°–ò–°–¢–ï–ú–´:
   
   # –ü–æ–ª–Ω—ã–π —Ü–∏–∫–ª
   python main.py --mode full
   
   # –¢–æ–ª—å–∫–æ –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö
   python main.py --mode data
   
   # –¢–æ–ª—å–∫–æ –æ–±—É—á–µ–Ω–∏–µ
   python main.py --mode train
   
   # –¢–æ–ª—å–∫–æ –±—ç–∫—Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ
   python main.py --mode backtest --model-path models_saved/model.pth

–ü–†–ò–ú–ï–ß–ê–ù–ò–ï: –≠—Ç–æ –±–∞–∑–æ–≤–∞—è –≤–µ—Ä—Å–∏—è —Å–∏—Å—Ç–µ–º—ã. –î–ª—è –ø–æ–ª–Ω–æ–π —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ:
- –î–æ–±–∞–≤–∏—Ç—å –æ—Å—Ç–∞–ª—å–Ω—ã–µ –º–æ–¥—É–ª–∏ (dataset.py, trainer.py, risk_manager.py, backtester.py, signals.py, ensemble.py, visualization.py)
- –†–µ–∞–ª–∏–∑–æ–≤–∞—Ç—å –ø–æ–ª–Ω–æ—Ü–µ–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –∏ –±—ç–∫—Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ
- –ù–∞—Å—Ç—Ä–æ–∏—Ç—å –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—é —Å —Ç–æ—Ä–≥–æ–≤—ã–º–∏ –ø–ª–∞—Ç—Ñ–æ—Ä–º–∞–º–∏

–ü–æ–ª–Ω—ã–π –∫–æ–¥ –≤—Å–µ—Ö –º–æ–¥—É–ª–µ–π –¥–æ—Å—Ç—É–ø–µ–Ω –≤ –æ—Ç–¥–µ–ª—å–Ω—ã—Ö –∞—Ä—Ç–µ—Ñ–∞–∫—Ç–∞—Ö –≤—ã—à–µ.
""")
