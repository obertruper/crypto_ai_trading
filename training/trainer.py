"""
–û—Å–Ω–æ–≤–Ω–æ–π –º–æ–¥—É–ª—å –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π
"""

import torch
import torch.nn as nn
from torch.utils.data import DataLoader
from torch.cuda.amp import GradScaler, autocast
import numpy as np
from typing import Dict, List, Optional, Tuple, Callable, Union
from pathlib import Path
import time
from tqdm import tqdm
import json
from datetime import datetime

from utils.logger import get_logger
from models.losses import get_loss_function, MultiTaskLoss
from training.optimizer import get_optimizer, get_scheduler
from utils.metrics import MetricsTracker

class Trainer:
    """–û—Å–Ω–æ–≤–Ω–æ–π –∫–ª–∞—Å—Å –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π"""
    
    def __init__(self,
                 model: nn.Module,
                 config: Dict,
                 device: Optional[torch.device] = None):
        """
        Args:
            model: –º–æ–¥–µ–ª—å –¥–ª—è –æ–±—É—á–µ–Ω–∏—è
            config: –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –æ–±—É—á–µ–Ω–∏—è
            device: —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è
        """
        self.model = model
        self.config = config
        self.logger = get_logger("Trainer")
        
        # –£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ
        if device is None:
            self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        else:
            self.device = device
        
        self.model.to(self.device)
        
        # –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö GPU –µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–Ω—ã
        if torch.cuda.device_count() > 1 and self.device.type == 'cuda':
            self.logger.info(f"üî• –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è {torch.cuda.device_count()} GPU –¥–ª—è –æ–±—É—á–µ–Ω–∏—è")
            self.model = nn.DataParallel(self.model)
            self.is_data_parallel = True
        else:
            self.is_data_parallel = False
        
        # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –æ–±—É—á–µ–Ω–∏—è
        self.epochs = config['model']['epochs']
        # –ò–°–ü–†–ê–í–õ–ï–ù–û: –ò—Å–ø–æ–ª—å–∑—É–µ–º –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–π learning rate –¥–ª—è RTX 5090
        self.learning_rate = config['model'].get('learning_rate', 2e-5)  # –û–ø—Ç–∏–º–∞–ª—å–Ω—ã–π LR
        self.gradient_clip = config['model'].get('gradient_clip', 1.0)
        self.early_stopping_patience = config['model'].get('early_stopping_patience', 10)
        
        # Gradient accumulation –¥–ª—è –±–æ–ª—å—à–∏—Ö —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã—Ö –±–∞—Ç—á–µ–π
        self.gradient_accumulation_steps = config['performance'].get('gradient_accumulation_steps', 1)
        
        # Mixed precision training
        self.use_amp = config['performance'].get('mixed_precision', False)
        if self.use_amp:
            self.scaler = GradScaler()
        
        # –û–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä –∏ –ø–ª–∞–Ω–∏—Ä–æ–≤—â–∏–∫
        self.optimizer = self._create_optimizer()
        self.scheduler = self._create_scheduler()
        
        # –§—É–Ω–∫—Ü–∏–∏ –ø–æ—Ç–µ—Ä—å
        self.criterion = self._create_loss_function()
        
        # –ú–µ—Ç—Ä–∏–∫–∏
        self.metrics_tracker = MetricsTracker(config)
        
        # –î–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è
        self.checkpoint_dir = Path("models_saved")
        self.checkpoint_dir.mkdir(exist_ok=True)
        
        # –ò—Å—Ç–æ—Ä–∏—è –æ–±—É—á–µ–Ω–∏—è
        self.history = {
            'train_loss': [],
            'val_loss': [],
            'train_metrics': [],
            'val_metrics': [],
            'learning_rates': []
        }
        
        # GPU –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏
        if self.device.type == 'cuda':
            # –ü–µ—Ä–∏–æ–¥–∏—á–µ—Å–∫–∞—è –æ—á–∏—Å—Ç–∫–∞ –∫—ç—à–∞ GPU
            self.gpu_cache_clear_freq = config['performance'].get('gpu_cache_clear_freq', 10)
            # –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –ø–∞–º—è—Ç–∏ GPU
            self.monitor_gpu_memory = config['performance'].get('monitor_gpu_memory', True)
        
        # Early stopping (—É–ª—É—á—à–µ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è –¥–ª—è –±–æ—Ä—å–±—ã —Å –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏–µ–º)
        self.best_val_loss = float('inf')
        self.patience_counter = 0
        self.best_model_state = None
        self.min_delta = config['model'].get('min_delta', 1e-4)  # –ú–∏–Ω–∏–º–∞–ª—å–Ω–æ–µ —É–ª—É—á—à–µ–Ω–∏–µ
        self.overfitting_threshold = config['model'].get('overfitting_threshold', 0.1)  # –ü–æ—Ä–æ–≥ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è
        self.consecutive_overfitting = 0  # –°—á–µ—Ç—á–∏–∫ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ–≥–æ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è
        
    def _create_optimizer(self) -> torch.optim.Optimizer:
        """–°–æ–∑–¥–∞–Ω–∏–µ –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä–∞"""
        optimizer_config = self.config.get('optimizer', {})
        optimizer_name = optimizer_config.get('name', 'AdamW')
        
        return get_optimizer(
            optimizer_name,
            self.model.parameters(),
            lr=self.learning_rate,
            **optimizer_config.get('params', {})
        )
    
    def _create_scheduler(self) -> Optional[torch.optim.lr_scheduler._LRScheduler]:
        """–°–æ–∑–¥–∞–Ω–∏–µ –ø–ª–∞–Ω–∏—Ä–æ–≤—â–∏–∫–∞ learning rate"""
        scheduler_config = self.config.get('scheduler', {})
        
        if not scheduler_config:
            return None
        
        scheduler_name = scheduler_config.get('name', 'CosineAnnealingLR')
        
        # –°–ø–µ—Ü–∏–∞–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–ª—è OneCycleLR
        if scheduler_name == 'OneCycleLR':
            # OneCycleLR —Ç—Ä–µ–±—É–µ—Ç total_steps
            if hasattr(self, 'train_loader'):
                total_steps = self.epochs * len(self.train_loader)
            else:
                total_steps = self.epochs * 1000  # –ü—Ä–∏–º–µ—Ä–Ω–∞—è –æ—Ü–µ–Ω–∫–∞
            scheduler_config['params']['total_steps'] = total_steps
            scheduler_config['params']['epochs'] = self.epochs
        
        return get_scheduler(
            scheduler_name,
            self.optimizer,
            **scheduler_config.get('params', {})
        )
    
    def _create_loss_function(self) -> nn.Module:
        """–°–æ–∑–¥–∞–Ω–∏–µ —Ñ—É–Ω–∫—Ü–∏–∏ –ø–æ—Ç–µ—Ä—å"""
        loss_config = self.config.get('loss', {})
        loss_name = loss_config.get('name', 'mse')
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ç–∏–ø loss —Ñ—É–Ω–∫—Ü–∏–∏
        if loss_name == 'directional_multitask':
            from models.patchtst_unified import DirectionalMultiTaskLoss
            self.logger.info("üéØ –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è DirectionalMultiTaskLoss –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è direction prediction")
            return DirectionalMultiTaskLoss(self.config)
        elif loss_name == 'unified_trading':
            from models.patchtst_unified import UnifiedTradingLoss
            return UnifiedTradingLoss(self.config)
        elif 'trading' in loss_name:
            from models.trading_losses import get_trading_loss_function
            return get_trading_loss_function(self.config, loss_type='multi_task')
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –µ—Å–ª–∏ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —É–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –º–æ–¥–µ–ª—å
        model_name = self.config.get('model', {}).get('name', '')
        if model_name == 'UnifiedPatchTST':
            # –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –∏—Å–ø–æ–ª—å–∑—É–µ–º DirectionalMultiTaskLoss –¥–ª—è UnifiedPatchTST
            self.logger.info("üîß –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è DirectionalMultiTaskLoss –¥–ª—è UnifiedPatchTST")
            from models.patchtst_unified import DirectionalMultiTaskLoss
            return DirectionalMultiTaskLoss(self.config)
        
        # –ú–Ω–æ–≥–æ–∑–∞–¥–∞—á–Ω–∞—è –ø–æ—Ç–µ—Ä—è –¥–ª—è —Ç–æ—Ä–≥–æ–≤–æ–π –º–æ–¥–µ–ª–∏
        elif loss_config.get('multitask', False):
            task_losses = {}
            task_weights = {}
            
            # –ü–æ—Ç–µ—Ä—è –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è —Ü–µ–Ω—ã
            task_losses['price'] = get_loss_function(
                loss_config.get('price_loss', 'mse')
            )
            task_weights['price'] = loss_config.get('price_weight', 1.0)
            
            # –ü–æ—Ç–µ—Ä—è –¥–ª—è –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–µ–π TP
            task_losses['tp'] = get_loss_function(
                loss_config.get('tp_loss', 'bce')
            )
            task_weights['tp'] = loss_config.get('tp_weight', 1.0)
            
            # –ü–æ—Ç–µ—Ä—è –¥–ª—è –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ SL
            task_losses['sl'] = get_loss_function(
                loss_config.get('sl_loss', 'bce')
            )
            task_weights['sl'] = loss_config.get('sl_weight', 1.0)
            
            # –ü–æ—Ç–µ—Ä—è –¥–ª—è –≤–æ–ª–∞—Ç–∏–ª—å–Ω–æ—Å—Ç–∏
            task_losses['volatility'] = get_loss_function(
                loss_config.get('volatility_loss', 'mse')
            )
            task_weights['volatility'] = loss_config.get('volatility_weight', 0.5)
            
            return MultiTaskLoss(
                task_weights=task_weights,
                uncertainty_weighting=loss_config.get('uncertainty_weighting', False)
            )
        else:
            # –û–¥–Ω–∞ —Ñ—É–Ω–∫—Ü–∏—è –ø–æ—Ç–µ—Ä—å
            loss_name = loss_config.get('name', 'mse')
            return get_loss_function(loss_name, **loss_config.get('params', {}))
    
    def train_epoch(self, train_loader: DataLoader) -> Dict[str, float]:
        """–û–±—É—á–µ–Ω–∏–µ –æ–¥–Ω–æ–π —ç–ø–æ—Ö–∏"""
        self.model.train()
        
        epoch_loss = 0.0
        epoch_metrics = {}
        
        progress_bar = tqdm(train_loader, desc="Training", leave=False)
        
        # –û–±–Ω—É–ª—è–µ–º –≥—Ä–∞–¥–∏–µ–Ω—Ç—ã –≤ –Ω–∞—á–∞–ª–µ –¥–ª—è gradient accumulation
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º set_to_none=True –¥–ª—è —ç–∫–æ–Ω–æ–º–∏–∏ –ø–∞–º—è—Ç–∏ (RTX 5090 –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è)
        self.optimizer.zero_grad(set_to_none=True)
        
        for batch_idx, (inputs, targets, info) in enumerate(progress_bar):
            # –ü–µ—Ä–µ–Ω–æ—Å –Ω–∞ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ
            inputs = inputs.to(self.device)
            if isinstance(targets, torch.Tensor):
                targets = targets.to(self.device)
            elif isinstance(targets, dict):
                targets = {k: v.to(self.device) for k, v in targets.items()}
            
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –Ω–∞ NaN
            if torch.isnan(inputs).any():
                self.logger.warning(f"NaN –≤–æ –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –±–∞—Ç—á–∞ {batch_idx}, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º")
                continue
                
            # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –±–æ–ª—å—à–∏–µ –∑–Ω–∞—á–µ–Ω–∏—è –≤–æ –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
            input_max = inputs.abs().max().item()
            if input_max > 1000:
                self.logger.warning(f"–û—á–µ–Ω—å –±–æ–ª—å—à–∏–µ –∑–Ω–∞—á–µ–Ω–∏—è –≤–æ –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö: max={input_max:.4f}")
                # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –≤—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
                inputs = torch.clamp(inputs, min=-100, max=100)
                
            # Forward pass
            if self.use_amp:
                with autocast():
                    outputs = self.model(inputs)
                    
                    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –≤—ã—Ö–æ–¥–æ–≤ –º–æ–¥–µ–ª–∏ –Ω–∞ inf/nan
                    if torch.isnan(outputs).any() or torch.isinf(outputs).any():
                        self.logger.warning(f"Model outputs contain NaN/Inf at batch {batch_idx}")
                        self.logger.warning(f"  Outputs stats before clipping: min={outputs.min().item():.4f}, max={outputs.max().item():.4f}")
                        # –ö–ª–∏–ø–ø–∏–Ω–≥ —ç–∫—Å—Ç—Ä–µ–º–∞–ª—å–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π
                        outputs = torch.clamp(outputs, min=-100, max=100)
                    
                    loss = self._compute_loss(outputs, targets)
            else:
                outputs = self.model(inputs)
                
                # –ü—Ä–æ–≤–µ—Ä–∫–∞ –≤—ã—Ö–æ–¥–æ–≤ –º–æ–¥–µ–ª–∏ –Ω–∞ inf/nan
                if torch.isnan(outputs).any() or torch.isinf(outputs).any():
                    self.logger.warning(f"Model outputs contain NaN/Inf at batch {batch_idx}")
                    self.logger.warning(f"  Outputs stats before clipping: min={outputs.min().item():.4f}, max={outputs.max().item():.4f}")
                    # –ö–ª–∏–ø–ø–∏–Ω–≥ —ç–∫—Å—Ç—Ä–µ–º–∞–ª—å–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π
                    outputs = torch.clamp(outputs, min=-100, max=100)
                
                loss = self._compute_loss(outputs, targets)
            
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ NaN/Inf
            if torch.isnan(loss) or torch.isinf(loss):
                self.logger.warning(f"Loss is NaN/Inf at batch {batch_idx}: {loss.item() if not torch.isinf(loss) else 'inf'}")
                # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞
                self.logger.warning(f"  Outputs stats: min={outputs.min().item():.4f}, max={outputs.max().item():.4f}, mean={outputs.mean().item():.4f}")
                if isinstance(targets, torch.Tensor):
                    self.logger.warning(f"  Targets stats: min={targets.min().item():.4f}, max={targets.max().item():.4f}, mean={targets.mean().item():.4f}")
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º –≥—Ä–∞–¥–∏–µ–Ω—Ç—ã
                grad_norms = []
                for name, param in self.model.named_parameters():
                    if param.grad is not None:
                        grad_norm = param.grad.norm().item()
                        if grad_norm > 100:
                            self.logger.warning(f"  Large gradient in {name}: {grad_norm:.4f}")
                        grad_norms.append(grad_norm)
                
                if grad_norms:
                    self.logger.warning(f"  Max gradient norm: {max(grad_norms):.4f}")
                
                # –í–ê–ñ–ù–û: –ï—Å–ª–∏ –∏—Å–ø–æ–ª—å–∑—É–µ–º AMP, –Ω—É–∂–Ω–æ –ø—Ä–∞–≤–∏–ª—å–Ω–æ –æ–±—Ä–∞–±–æ—Ç–∞—Ç—å scaler
                if self.use_amp:
                    # –°–±—Ä–∞—Å—ã–≤–∞–µ–º –≥—Ä–∞–¥–∏–µ–Ω—Ç—ã (—Å –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–µ–π –ø–∞–º—è—Ç–∏)
                    self.optimizer.zero_grad(set_to_none=True)
                    # –û–±–Ω–æ–≤–ª—è–µ–º scaler –±–µ–∑ —à–∞–≥–∞ –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä–∞
                    self.scaler.update()
                
                continue  # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —ç—Ç–æ—Ç –±–∞—Ç—á
            
            # Backward pass —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π gradient accumulation
            # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º loss –Ω–∞ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —à–∞–≥–æ–≤ –Ω–∞–∫–æ–ø–ª–µ–Ω–∏—è
            loss = loss / self.gradient_accumulation_steps
            
            if self.use_amp:
                self.scaler.scale(loss).backward()
            else:
                loss.backward()
            
            # –û–±–Ω–æ–≤–ª—è–µ–º –≤–µ—Å–∞ —Ç–æ–ª—å–∫–æ –∫–∞–∂–¥—ã–µ gradient_accumulation_steps —à–∞–≥–æ–≤
            if (batch_idx + 1) % self.gradient_accumulation_steps == 0:
                if self.use_amp:
                    # Gradient clipping
                    if self.gradient_clip > 0:
                        self.scaler.unscale_(self.optimizer)
                        torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.gradient_clip)
                    
                    self.scaler.step(self.optimizer)
                    self.scaler.update()
                else:
                    # Gradient clipping —Å –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–π –ø—Ä–æ–≤–µ—Ä–∫–æ–π
                    if self.gradient_clip > 0:
                        grad_norm = torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.gradient_clip)
                        if grad_norm > self.gradient_clip * 10:  # –°–ª–∏—à–∫–æ–º –±–æ–ª—å—à–∏–µ –≥—Ä–∞–¥–∏–µ–Ω—Ç—ã
                            self.logger.warning(f"–û—á–µ–Ω—å –±–æ–ª—å—à–∞—è –Ω–æ—Ä–º–∞ –≥—Ä–∞–¥–∏–µ–Ω—Ç–∞: {grad_norm:.4f}")
                    
                    self.optimizer.step()
                
                # –û–±–Ω—É–ª—è–µ–º –≥—Ä–∞–¥–∏–µ–Ω—Ç—ã –ø–æ—Å–ª–µ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è (—Å –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–µ–π –ø–∞–º—è—Ç–∏)
                self.optimizer.zero_grad(set_to_none=True)
            
            # –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –º–µ—Ç—Ä–∏–∫ (–≤–æ—Å—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–π loss –¥–ª—è –ø—Ä–∞–≤–∏–ª—å–Ω–æ–≥–æ –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è)
            epoch_loss += loss.item() * self.gradient_accumulation_steps
            
            # –î–µ—Ç–∞–ª—å–Ω–æ–µ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–ª—è –ø–µ—Ä–≤—ã—Ö –±–∞—Ç—á–µ–π
            if batch_idx < 3 and not hasattr(self, '_detailed_log_done'):
                self.logger.info(f"üìä –ë–∞—Ç—á {batch_idx} –¥–µ—Ç–∞–ª–∏:")
                self.logger.info(f"   - Loss: {loss.item():.6f}")
                self.logger.info(f"   - Outputs min/max/mean: {outputs.min():.4f}/{outputs.max():.4f}/{outputs.mean():.4f}")
                self.logger.info(f"   - Targets min/max/mean: {targets.min():.4f}/{targets.max():.4f}/{targets.mean():.4f}")
                self.logger.info(f"   - –ì—Ä–∞–¥–∏–µ–Ω—Ç—ã –µ—Å—Ç—å: {loss.requires_grad}")
                if batch_idx == 2:
                    self._detailed_log_done = True
            
            # –ë–µ–∑–æ–ø–∞—Å–Ω–æ–µ –≤—ã—á–∏—Å–ª–µ–Ω–∏–µ –º–µ—Ç—Ä–∏–∫
            try:
                batch_metrics = self.metrics_tracker.metrics_calculator.compute_batch_metrics(outputs, targets)
                for key, value in batch_metrics.items():
                    if key not in epoch_metrics:
                        epoch_metrics[key] = 0.0
                    epoch_metrics[key] += value
            except Exception as e:
                if not hasattr(self, '_metrics_error_logged'):
                    self._metrics_error_logged = True
                    self.logger.warning(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –≤—ã—á–∏—Å–ª–µ–Ω–∏—è –º–µ—Ç—Ä–∏–∫: {e}")
            
            # –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ progress bar
            progress_bar.set_postfix({
                'loss': f'{loss.item():.4f}',
                'avg_loss': f'{epoch_loss / (batch_idx + 1):.4f}'
            })
        
        # –£—Å—Ä–µ–¥–Ω–µ–Ω–∏–µ –º–µ—Ç—Ä–∏–∫
        num_batches = len(train_loader)
        epoch_loss /= num_batches
        epoch_metrics = {k: v / num_batches for k, v in epoch_metrics.items()}
        epoch_metrics['loss'] = epoch_loss
        
        return epoch_metrics
    
    def validate(self, val_loader: DataLoader) -> Dict[str, float]:
        """–í–∞–ª–∏–¥–∞—Ü–∏—è –º–æ–¥–µ–ª–∏"""
        self.model.eval()
        
        val_loss = 0.0
        val_metrics = {}
        
        with torch.no_grad():
            for batch_idx, (inputs, targets, info) in enumerate(val_loader):
                # –ü–µ—Ä–µ–Ω–æ—Å –Ω–∞ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ
                inputs = inputs.to(self.device)
                if isinstance(targets, torch.Tensor):
                    targets = targets.to(self.device)
                elif isinstance(targets, dict):
                    targets = {k: v.to(self.device) for k, v in targets.items()}
                
                # Forward pass
                if self.use_amp:
                    with autocast():
                        outputs = self.model(inputs)
                        loss = self._compute_loss(outputs, targets)
                else:
                    outputs = self.model(inputs)
                    loss = self._compute_loss(outputs, targets)
                
                # –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –º–µ—Ç—Ä–∏–∫
                val_loss += loss.item()
                batch_metrics = self.metrics_tracker.metrics_calculator.compute_batch_metrics(outputs, targets)
                
                for key, value in batch_metrics.items():
                    if key not in val_metrics:
                        val_metrics[key] = 0.0
                    val_metrics[key] += value
        
        # –£—Å—Ä–µ–¥–Ω–µ–Ω–∏–µ –º–µ—Ç—Ä–∏–∫
        num_batches = len(val_loader)
        val_loss /= num_batches
        val_metrics = {k: v / num_batches for k, v in val_metrics.items()}
        val_metrics['loss'] = val_loss
        
        return val_metrics
    
    def _compute_loss(self, outputs: Union[torch.Tensor, Dict], 
                     targets: Union[torch.Tensor, Dict]) -> torch.Tensor:
        """–í—ã—á–∏—Å–ª–µ–Ω–∏–µ –ø–æ—Ç–µ—Ä—å —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π —É–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏"""
        
        # –î–ª—è —É–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏ - –ø—Ä—è–º–æ–µ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏–µ loss
        if isinstance(outputs, torch.Tensor) and isinstance(targets, torch.Tensor):
            # –ö–†–ò–¢–ò–ß–ï–°–ö–û–ï –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï: –û–±—Ä–∞–±–æ—Ç–∫–∞ —Ü–µ–ª–µ–≤—ã—Ö –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –∏–∑ –¥–∞—Ç–∞—Å–µ—Ç–∞
            # –î–∞—Ç–∞—Å–µ—Ç –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç targets —Å —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å—é (batch, 1, 36)
            # –ú–æ–¥–µ–ª—å –≤—ã–¥–∞–µ—Ç outputs —Å —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å—é (batch, 36)
            
            # –î–µ—Ç–∞–ª—å–Ω–æ–µ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–µ–π –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏
            if not hasattr(self, '_logged_dimensions'):
                self._logged_dimensions = True
                self.logger.info(f"üîç –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏ –ø—Ä–∏ –ø–µ—Ä–≤–æ–º –±–∞—Ç—á–µ:")
                self.logger.info(f"   - Outputs shape: {outputs.shape}")
                self.logger.info(f"   - Targets shape: {targets.shape}")
            
            # –ü—Ä–∏–≤–µ–¥–µ–Ω–∏–µ targets –∫ –ø—Ä–∞–≤–∏–ª—å–Ω–æ–π —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏
            if targets.dim() == 3 and targets.shape[1] == 1:
                # (batch, 1, 36) -> (batch, 36)
                targets = targets.squeeze(1)
                if not hasattr(self, '_logged_squeeze'):
                    self._logged_squeeze = True
                    self.logger.info(f"‚úÖ Targets –ø—Ä–∏–≤–µ–¥–µ–Ω—ã –∫ —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏: {targets.shape}")
            
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–µ–π –ø–æ—Å–ª–µ –ø—Ä–∏–≤–µ–¥–µ–Ω–∏—è
            if outputs.dim() == 2 and targets.dim() == 2:
                # –£–±–µ–∂–¥–∞–µ–º—Å—è —á—Ç–æ —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏ —Å–æ–≤–ø–∞–¥–∞—é—Ç
                if outputs.shape[-1] != targets.shape[-1]:
                    self.logger.warning(f"‚ö†Ô∏è –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏ –Ω–µ —Å–æ–≤–ø–∞–¥–∞—é—Ç: outputs {outputs.shape} vs targets {targets.shape}")
                    min_size = min(outputs.shape[-1], targets.shape[-1])
                    outputs = outputs[..., :min_size]
                    targets = targets[..., :min_size]
                
                # –ü—Ä–∏–º–µ–Ω—è–µ–º loss –Ω–∞–ø—Ä—è–º—É—é
                loss = self.criterion(outputs, targets)
                
                # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ NaN/Inf
                if torch.isnan(loss) or torch.isinf(loss):
                    self.logger.warning("‚ùå Loss is NaN/Inf!")
                    self.logger.warning(f"   - Outputs stats: min={outputs.min():.4f}, max={outputs.max():.4f}, mean={outputs.mean():.4f}")
                    self.logger.warning(f"   - Targets stats: min={targets.min():.4f}, max={targets.max():.4f}, mean={targets.mean():.4f}")
                    return torch.tensor(0.0, device=outputs.device, requires_grad=True)
                
                return loss
        
        # –î–ª—è —Å—Ç–∞—Ä–æ–π –ª–æ–≥–∏–∫–∏ —Å MultiTaskLoss
        if isinstance(self.criterion, MultiTaskLoss):
            # [–æ—Å—Ç–∞–≤–ª—è–µ–º —Å—Ç–∞—Ä—É—é –ª–æ–≥–∏–∫—É –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏]
            losses = {}
            
            if isinstance(outputs, dict) and isinstance(targets, dict):
                # ... —Å—Ç–∞—Ä–∞—è –ª–æ–≥–∏–∫–∞ ...
                pass
            
            return self.criterion(losses) if losses else torch.tensor(0.0, device=outputs.device)
        
        # Fallback –¥–ª—è –ø—Ä–æ—Å—Ç—ã—Ö —Å–ª—É—á–∞–µ–≤
        if isinstance(outputs, dict):
            outputs = list(outputs.values())[0]
        if isinstance(targets, dict):
            targets = list(targets.values())[0]
        
        return self.criterion(outputs, targets)
    def _align_dimensions(self, outputs: torch.Tensor, targets: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:
        """–í—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏–µ —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–µ–π –≤—ã—Ö–æ–¥–æ–≤ –∏ —Ü–µ–ª–µ–π"""
        if outputs.shape != targets.shape:
            # –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –Ω–µ—Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏—è
            self.logger.debug(f"–ù–µ—Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–µ–π: outputs {outputs.shape}, targets {targets.shape}")
            
            # –ï—Å–ª–∏ —É outputs –µ—Å—Ç—å –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–µ –∏–∑–º–µ—Ä–µ–Ω–∏–µ –ø–æ –ø—Ä–∏–∑–Ω–∞–∫–∞–º
            if outputs.dim() == 3 and targets.dim() == 3 and outputs.shape[-1] != targets.shape[-1]:
                # –ë–µ—Ä–µ–º —Ç–æ–ª—å–∫–æ –Ω—É–∂–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –∏–∑ outputs
                if outputs.shape[-1] > targets.shape[-1]:
                    outputs = outputs[..., :targets.shape[-1]]
                else:
                    # –ò–ª–∏ –Ω–∞–æ–±–æ—Ä–æ—Ç, –µ—Å–ª–∏ targets –±–æ–ª—å—à–µ
                    targets = targets[..., :outputs.shape[-1]]
            
            # –ï—Å–ª–∏ —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏ –≤—Å–µ –µ—â–µ –Ω–µ —Å–æ–≤–ø–∞–¥–∞—é—Ç
            if outputs.shape != targets.shape:
                # –ü–æ–ø—ã—Ç–∫–∞ –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏—è –ø–æ –±–∞—Ç—á–∞–º –∏ –≤—Ä–µ–º–µ–Ω–Ω—ã–º —à–∞–≥–∞–º
                if outputs.dim() == 3 and targets.dim() == 2:
                    # outputs: (batch, time, features), targets: (batch, features)
                    # –ë–µ—Ä–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–π –≤—Ä–µ–º–µ–Ω–Ω–æ–π —à–∞–≥ –∏–∑ outputs
                    outputs = outputs[:, -1, :]
                elif outputs.dim() == 2 and targets.dim() == 3:
                    # outputs: (batch, features), targets: (batch, time, features)
                    # –ë–µ—Ä–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–π –≤—Ä–µ–º–µ–Ω–Ω–æ–π —à–∞–≥ –∏–∑ targets
                    targets = targets[:, -1, :]
                elif outputs.dim() == 3 and targets.dim() == 3:
                    # –û–±–∞ –∏–º–µ—é—Ç 3 –∏–∑–º–µ—Ä–µ–Ω–∏—è, –Ω–æ —Ä–∞–∑–Ω—ã–µ —Ä–∞–∑–º–µ—Ä—ã –ø–æ –≤—Ä–µ–º–µ–Ω–∏
                    min_time = min(outputs.shape[1], targets.shape[1])
                    outputs = outputs[:, :min_time, :]
                    targets = targets[:, :min_time, :]
                
                # –§–∏–Ω–∞–ª—å–Ω–∞—è –ø–æ–ø—ã—Ç–∫–∞ - flatten –¥–æ 2D
                if outputs.shape != targets.shape:
                    if outputs.dim() > 2:
                        outputs = outputs.reshape(outputs.shape[0], -1)
                    if targets.dim() > 2:
                        targets = targets.reshape(targets.shape[0], -1)
                    
                    # –í—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏–µ –ø–æ –ø–æ—Å–ª–µ–¥–Ω–µ–º—É –∏–∑–º–µ—Ä–µ–Ω–∏—é
                    min_size = min(outputs.shape[-1], targets.shape[-1])
                    outputs = outputs[..., :min_size]
                    targets = targets[..., :min_size]
        
        return outputs, targets
    
    def train(self, 
              train_loader: DataLoader,
              val_loader: DataLoader,
              callbacks: Optional[List[Callable]] = None) -> Dict:
        """–ü–æ–ª–Ω—ã–π —Ü–∏–∫–ª –æ–±—É—á–µ–Ω–∏—è"""
        
        self.logger.info(f"–ù–∞—á–∞–ª–æ –æ–±—É—á–µ–Ω–∏—è –Ω–∞ {self.epochs} —ç–ø–æ—Ö")
        self.logger.info(f"–£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {self.device}")
        self.logger.info(f"–†–∞–∑–º–µ—Ä –±–∞—Ç—á–∞: {train_loader.batch_size}")
        self.logger.info(f"Learning rate: {self.learning_rate}")
        
        start_time = time.time()
        
        for epoch in range(self.epochs):
            epoch_start_time = time.time()
            
            # –û–±—É—á–µ–Ω–∏–µ
            train_metrics = self.train_epoch(train_loader)
            
            # –í–∞–ª–∏–¥–∞—Ü–∏—è
            val_metrics = self.validate(val_loader)
            
            # –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ learning rate
            current_lr = self.optimizer.param_groups[0]['lr']  # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ø–µ—Ä–µ–¥ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º
            
            if self.scheduler is not None:
                if isinstance(self.scheduler, torch.optim.lr_scheduler.ReduceLROnPlateau):
                    old_lr = current_lr  # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Å—Ç–∞—Ä–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ
                    self.scheduler.step(val_metrics['loss'])
                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∏–∑–º–µ–Ω–µ–Ω–∏–µ LR
                    current_lr = self.optimizer.param_groups[0]['lr']
                    if old_lr != current_lr:
                        self.logger.info(f"üìâ Learning rate —Å–Ω–∏–∂–µ–Ω —Å {old_lr:.2e} –¥–æ {current_lr:.2e}")
                else:
                    self.scheduler.step()
                    current_lr = self.optimizer.param_groups[0]['lr']
            
            # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∏—Å—Ç–æ—Ä–∏–∏
            self.history['train_loss'].append(train_metrics['loss'])
            self.history['val_loss'].append(val_metrics['loss'])
            self.history['train_metrics'].append(train_metrics)
            self.history['val_metrics'].append(val_metrics)
            self.history['learning_rates'].append(current_lr)
            
            # GPU –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏
            if self.device.type == 'cuda':
                # –ü–µ—Ä–∏–æ–¥–∏—á–µ—Å–∫–∞—è –æ—á–∏—Å—Ç–∫–∞ –∫—ç—à–∞ GPU
                if (epoch + 1) % self.gpu_cache_clear_freq == 0:
                    torch.cuda.empty_cache()
                    torch.cuda.synchronize()
                    self.logger.info(f"üßπ –û—á–∏—â–µ–Ω –∫—ç—à GPU –Ω–∞ —ç–ø–æ—Ö–µ {epoch + 1}")
                
                # –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –ø–∞–º—è—Ç–∏ GPU
                if self.monitor_gpu_memory and (epoch + 1) % 5 == 0:
                    allocated = torch.cuda.memory_allocated() / 1024**3
                    reserved = torch.cuda.memory_reserved() / 1024**3
                    self.logger.info(f"üíæ GPU –ø–∞–º—è—Ç—å: –≤—ã–¥–µ–ª–µ–Ω–æ {allocated:.2f}GB, –∑–∞—Ä–µ–∑–µ—Ä–≤–∏—Ä–æ–≤–∞–Ω–æ {reserved:.2f}GB")
            
            # –£–ª—É—á—à–µ–Ω–Ω–∞—è –ª–æ–≥–∏–∫–∞ –±–æ—Ä—å–±—ã —Å –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏–µ–º
            improvement = self.best_val_loss - val_metrics['loss']
            overfitting_ratio = val_metrics['loss'] / train_metrics['loss'] if train_metrics['loss'] > 0 else 1.0
            
            # –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –º–µ—Ç—Ä–∏–∫ –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏
            self.logger.info(f"üìä –ú–µ—Ç—Ä–∏–∫–∏ —ç–ø–æ—Ö–∏ {epoch + 1}: train_loss={train_metrics['loss']:.4f}, val_loss={val_metrics['loss']:.4f}, "
                           f"overfitting_ratio={overfitting_ratio:.3f}, improvement={improvement:.6f}, current_lr={current_lr:.2e}")
            
            if improvement > self.min_delta:
                # –ó–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ–µ —É–ª—É—á—à–µ–Ω–∏–µ
                self.best_val_loss = val_metrics['loss']
                self.patience_counter = 0
                self.consecutive_overfitting = 0
                self.best_model_state = self.model.state_dict().copy()
                self._save_checkpoint(epoch, val_metrics['loss'], is_best=True)
                self.logger.info(f"‚úÖ –ù–æ–≤–∞—è –ª—É—á—à–∞—è –º–æ–¥–µ–ª—å! Val loss —É–ª—É—á—à–µ–Ω –Ω–∞ {improvement:.6f}")
            else:
                self.patience_counter += 1
                
                # –ú–Ø–ì–ö–ê–Ø –ø—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏–µ - –¥–∞–µ–º –º–æ–¥–µ–ª–∏ –±–æ–ª—å—à–µ —Å–≤–æ–±–æ–¥—ã
                if epoch >= 5:  # –ù–∞—á–∏–Ω–∞–µ–º –ø—Ä–æ–≤–µ—Ä–∫—É —Ç–æ–ª—å–∫–æ —Å 5-–π —ç–ø–æ—Ö–∏
                    if overfitting_ratio > (1.0 + self.overfitting_threshold):
                        self.consecutive_overfitting += 1
                        self.logger.warning(f"‚ö†Ô∏è –ü–µ—Ä–µ–æ–±—É—á–µ–Ω–∏–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–æ: Train: {train_metrics['loss']:.4f}, Val: {val_metrics['loss']:.4f} (ratio: {overfitting_ratio:.3f})")
                        
                        # –û—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º—Å—è —Ç–æ–ª—å–∫–æ –ø–æ—Å–ª–µ 5 –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω—ã—Ö –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏–π (–≤–º–µ—Å—Ç–æ 2)
                        if self.consecutive_overfitting >= 5:
                            self.logger.info(f"üõë –û—Å—Ç–∞–Ω–æ–≤–∫–∞ –∏–∑-–∑–∞ —É—Å—Ç–æ–π—á–∏–≤–æ–≥–æ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è (5 —ç–ø–æ—Ö –ø–æ–¥—Ä—è–¥)")
                            break
                    else:
                        self.consecutive_overfitting = 0
                else:
                    # –ü–µ—Ä–≤—ã–µ 5 —ç–ø–æ—Ö - —Ç–æ–ª—å–∫–æ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–æ–Ω–Ω–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ
                    if overfitting_ratio > 1.5:  # 50% —Ä–∞–∑–Ω–∏—Ü–∞
                        self.logger.info(f"üìà –≠–ø–æ—Ö–∞ {epoch + 1}: val_loss –≤—ã—à–µ train_loss (ratio: {overfitting_ratio:.3f}) - –Ω–æ—Ä–º–∞–ª—å–Ω–æ –≤ –Ω–∞—á–∞–ª–µ")
            
            # –ö–†–ò–¢–ò–ß–ù–û: –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ - –µ—Å–ª–∏ val_loss –Ω–µ —É–ª—É—á—à–∞–µ—Ç—Å—è —Å–ª–∏—à–∫–æ–º –¥–æ–ª–≥–æ
            if self.patience_counter >= self.early_stopping_patience:
                self.logger.info(f"üõë Early stopping: val_loss –Ω–µ —É–ª—É—á—à–∞–µ—Ç—Å—è {self.patience_counter} —ç–ø–æ—Ö (patience={self.early_stopping_patience})")
                break
            
            # –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ
            epoch_time = time.time() - epoch_start_time
            self.logger.log_model_metrics(
                epoch=epoch + 1,
                metrics={
                    'train_loss': train_metrics['loss'],
                    'val_loss': val_metrics['loss'],
                    'learning_rate': current_lr,
                    'epoch_time': epoch_time
                }
            )
            
            # Callbacks
            if callbacks:
                for callback in callbacks:
                    callback(self, epoch, train_metrics, val_metrics)
        
        # –í–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–µ –ª—É—á—à–µ–π –º–æ–¥–µ–ª–∏
        if self.best_model_state is not None:
            self.model.load_state_dict(self.best_model_state)
        
        total_time = time.time() - start_time
        self.logger.info(f"–û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ –∑–∞ {total_time:.2f} —Å–µ–∫—É–Ω–¥")
        
        # –§–∏–Ω–∞–ª—å–Ω—ã–π –æ—Ç—á–µ—Ç
        self._save_training_report()
        
        return self.history
    
    def _save_checkpoint(self, epoch: int, val_loss: float, is_best: bool = False):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ checkpoint'–∞"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ø—Ä–∞–≤–∏–ª—å–Ω–æ –¥–ª—è DataParallel
        model_state = self.model.module.state_dict() if self.is_data_parallel else self.model.state_dict()
        
        checkpoint = {
            'epoch': epoch,
            'model_state_dict': model_state,
            'optimizer_state_dict': self.optimizer.state_dict(),
            'scheduler_state_dict': self.scheduler.state_dict() if self.scheduler else None,
            'val_loss': val_loss,
            'config': self.config,
            'history': self.history,
            'is_data_parallel': self.is_data_parallel
        }
        
        if is_best:
            path = self.checkpoint_dir / f"best_model_{timestamp}.pth"
        else:
            path = self.checkpoint_dir / f"checkpoint_epoch_{epoch}_{timestamp}.pth"
        
        torch.save(checkpoint, path)
        self.logger.info(f"Checkpoint —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {path}")
    
    def _save_training_report(self):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –æ—Ç—á–µ—Ç–∞ –æ–± –æ–±—É—á–µ–Ω–∏–∏"""
        report = {
            'config': self.config,
            'history': self.history,
            'best_val_loss': self.best_val_loss,
            'total_epochs': len(self.history['train_loss']),
            'model_architecture': str(self.model),
            'total_parameters': sum(p.numel() for p in self.model.parameters()),
            'trainable_parameters': sum(p.numel() for p in self.model.parameters() if p.requires_grad)
        }
        
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        report_path = self.checkpoint_dir / f"training_report_{timestamp}.json"
        
        with open(report_path, 'w') as f:
            json.dump(report, f, indent=4, default=str)
        
        self.logger.info(f"–û—Ç—á–µ—Ç –æ–± –æ–±—É—á–µ–Ω–∏–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {report_path}")
    
    def load_checkpoint(self, checkpoint_path: str):
        """–ó–∞–≥—Ä—É–∑–∫–∞ checkpoint'–∞"""
        checkpoint = torch.load(checkpoint_path, map_location=self.device)
        
        self.model.load_state_dict(checkpoint['model_state_dict'])
        self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
        
        if self.scheduler and checkpoint.get('scheduler_state_dict'):
            self.scheduler.load_state_dict(checkpoint['scheduler_state_dict'])
        
        self.history = checkpoint.get('history', self.history)
        
        self.logger.info(f"Checkpoint –∑–∞–≥—Ä—É–∂–µ–Ω: {checkpoint_path}")
        
        return checkpoint['epoch']