"""
–ü–æ—ç—Ç–∞–ø–Ω—ã–π —Ç—Ä–µ–π–Ω–µ—Ä –¥–ª—è –≤—ã—Ö–æ–¥–∞ –∏–∑ –ª–æ–∫–∞–ª—å–Ω–æ–≥–æ –º–∏–Ω–∏–º—É–º–∞
"""

import torch
import torch.nn as nn
from torch.utils.data import DataLoader
from typing import Dict, Optional, List
import copy
from pathlib import Path

from training.optimized_trainer import OptimizedTrainer
from utils.logger import get_logger


class StagedTrainer:
    """
    –¢—Ä–µ–π–Ω–µ—Ä —Å –ø–æ—ç—Ç–∞–ø–Ω—ã–º –æ–±—É—á–µ–Ω–∏–µ–º –¥–ª—è –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–µ–Ω–∏—è —Å—Ö–ª–æ–ø—ã–≤–∞–Ω–∏—è –º–æ–¥–µ–ª–∏
    """
    
    def __init__(self, model: nn.Module, config: Dict, device: Optional[torch.device] = None):
        self.model = model
        self.config = config
        self.device = device or torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        self.logger = get_logger("StagedTrainer")
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é –ø–æ—ç—Ç–∞–ø–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è
        self.staged_config = config.get('production', {}).get('staged_training', {})
        self.enabled = self.staged_config.get('enabled', False)
        
        if not self.enabled:
            self.logger.warning("‚ö†Ô∏è –ü–æ—ç—Ç–∞–ø–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –æ—Ç–∫–ª—é—á–µ–Ω–æ –≤ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏")
            return
            
        self.stages = self.staged_config.get('stages', [])
        if not self.stages:
            self.logger.error("‚ùå –ù–µ –Ω–∞–π–¥–µ–Ω—ã —ç—Ç–∞–ø—ã –æ–±—É—á–µ–Ω–∏—è –≤ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏")
            self.enabled = False
            return
            
        self.logger.info(f"‚úÖ –ü–æ—ç—Ç–∞–ø–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –≤–∫–ª—é—á–µ–Ω–æ —Å {len(self.stages)} —ç—Ç–∞–ø–∞–º–∏")
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—É—é –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é
        self.original_config = copy.deepcopy(config)
        
    def train(self, train_loader: DataLoader, val_loader: Optional[DataLoader] = None) -> Dict:
        """
        –ó–∞–ø—É—Å–∫ –ø–æ—ç—Ç–∞–ø–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è
        """
        if not self.enabled:
            # Fallback –∫ –æ–±—ã—á–Ω–æ–º—É –æ–±—É—á–µ–Ω–∏—é
            trainer = OptimizedTrainer(self.model, self.config, self.device)
            return trainer.train(train_loader, val_loader)
            
        self.logger.info("üöÄ –ù–∞—á–∞–ª–æ –ø–æ—ç—Ç–∞–ø–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è")
        self.logger.info("="*80)
        
        all_history = {}
        total_epochs = 0
        
        for stage_idx, stage in enumerate(self.stages):
            self.logger.info(f"\n{'='*80}")
            self.logger.info(f"üìä –≠–¢–ê–ü {stage_idx + 1}/{len(self.stages)}: {stage['name']}")
            self.logger.info(f"üìù {stage.get('description', '–ë–µ–∑ –æ–ø–∏—Å–∞–Ω–∏—è')}")
            self.logger.info(f"‚è±Ô∏è –≠–ø–æ—Ö: {stage['epochs']}")
            self.logger.info("="*80)
            
            # –°–æ–∑–¥–∞–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é –¥–ª—è —Ç–µ–∫—É—â–µ–≥–æ —ç—Ç–∞–ø–∞
            stage_config = self._create_stage_config(stage)
            
            # –°–æ–∑–¥–∞–µ–º —Ç—Ä–µ–π–Ω–µ—Ä –¥–ª—è —ç—Ç–∞–ø–∞
            trainer = OptimizedTrainer(self.model, stage_config, self.device)
            
            # –ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º –∞–∫—Ç–∏–≤–Ω—ã–µ losses
            self._configure_losses(trainer, stage)
            
            # –û–±—É—á–µ–Ω–∏–µ –Ω–∞ —ç—Ç–∞–ø–µ
            stage_history = trainer.train(train_loader, val_loader)
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∏—Å—Ç–æ—Ä–∏—é
            stage_name = f"stage_{stage_idx}_{stage['name']}"
            all_history[stage_name] = stage_history
            total_epochs += stage['epochs']
            
            # –ê–Ω–∞–ª–∏–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ —ç—Ç–∞–ø–∞
            self._analyze_stage_results(stage_name, stage_history, val_loader)
            
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ —Å—Ö–ª–æ–ø—ã–≤–∞–Ω–∏–µ –ø–æ—Å–ª–µ –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏—Ö —ç—Ç–∞–ø–æ–≤
            if stage_idx < 2:  # –ü–µ—Ä–≤—ã–µ –¥–≤–∞ —ç—Ç–∞–ø–∞ –∫—Ä–∏—Ç–∏—á–Ω—ã
                if self._check_collapse(trainer, val_loader):
                    self.logger.error("‚ö†Ô∏è –û–±–Ω–∞—Ä—É–∂–µ–Ω–æ —Å—Ö–ª–æ–ø—ã–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏! –ü—Ä–∏–º–µ–Ω—è–µ–º –∫–æ—Ä—Ä–µ–∫—Ü–∏—é...")
                    self._apply_correction(stage_idx)
        
        self.logger.info(f"\n{'='*80}")
        self.logger.info(f"‚úÖ –ü–æ—ç—Ç–∞–ø–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ! –í—Å–µ–≥–æ —ç–ø–æ—Ö: {total_epochs}")
        self.logger.info("="*80)
        
        return all_history
        
    def _create_stage_config(self, stage: Dict) -> Dict:
        """
        –°–æ–∑–¥–∞–µ—Ç –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é –¥–ª—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ —ç—Ç–∞–ø–∞
        """
        # –ö–æ–ø–∏—Ä—É–µ–º –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—É—é –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é
        stage_config = copy.deepcopy(self.original_config)
        
        # –û–±–Ω–æ–≤–ª—è–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –∏–∑ —ç—Ç–∞–ø–∞
        if 'learning_rate' in stage:
            stage_config['model']['learning_rate'] = stage['learning_rate']
            self.logger.info(f"üìà Learning rate: {stage['learning_rate']}")
            
        if 'dropout' in stage:
            stage_config['model']['dropout'] = stage['dropout']
            stage_config['model']['attention_dropout'] = stage['dropout'] * 0.5
            self.logger.info(f"üíß Dropout: {stage['dropout']}")
            
        if 'label_smoothing' in stage:
            stage_config['model']['label_smoothing'] = stage['label_smoothing']
            self.logger.info(f"üîÑ Label smoothing: {stage['label_smoothing']}")
            
        if 'class_weights' in stage:
            stage_config['loss']['class_weights'] = stage['class_weights']
            self.logger.info(f"‚öñÔ∏è Class weights: {stage['class_weights']}")
            
        if 'gradient_clip' in stage:
            stage_config['model']['gradient_clip'] = stage['gradient_clip']
            self.logger.info(f"‚úÇÔ∏è Gradient clipping: {stage['gradient_clip']}")
            
        # –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —ç–ø–æ—Ö
        stage_config['model']['epochs'] = stage['epochs']
        
        return stage_config
        
    def _configure_losses(self, trainer: OptimizedTrainer, stage: Dict):
        """
        –ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ—Ç –∞–∫—Ç–∏–≤–Ω—ã–µ loss —Ñ—É–Ω–∫—Ü–∏–∏ –¥–ª—è —ç—Ç–∞–ø–∞
        """
        active_losses = stage.get('active_losses', ['all'])
        
        if hasattr(trainer.criterion, 'set_active_losses'):
            trainer.criterion.set_active_losses(active_losses)
            self.logger.info(f"üéØ –ê–∫—Ç–∏–≤–Ω—ã–µ losses: {active_losses}")
            
    def _check_collapse(self, trainer: OptimizedTrainer, val_loader: DataLoader) -> bool:
        """
        –ü—Ä–æ–≤–µ—Ä—è–µ—Ç, –Ω–µ —Å—Ö–ª–æ–ø–Ω—É–ª–∞—Å—å –ª–∏ –º–æ–¥–µ–ª—å –≤ –æ–¥–∏–Ω –∫–ª–∞—Å—Å
        """
        if not val_loader:
            return False
            
        # –ü–æ–ª—É—á–∞–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –Ω–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–∏
        predictions = []
        self.model.eval()
        
        with torch.no_grad():
            for batch_idx, (inputs, targets, info) in enumerate(val_loader):
                if batch_idx > 10:  # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ç–æ–ª—å–∫–æ –ø–µ—Ä–≤—ã–µ –±–∞—Ç—á–∏
                    break
                    
                inputs = inputs.to(self.device)
                outputs = self.model(inputs)
                
                # –ò–∑–≤–ª–µ–∫–∞–µ–º direction –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
                if hasattr(outputs, '_direction_logits'):
                    direction_logits = outputs._direction_logits[:, 0, :]  # 15m
                    preds = torch.argmax(torch.softmax(direction_logits, dim=-1), dim=-1)
                    predictions.extend(preds.cpu().numpy())
                    
        if not predictions:
            return False
            
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ
        import numpy as np
        unique, counts = np.unique(predictions, return_counts=True)
        max_ratio = max(counts) / sum(counts)
        
        collapse_threshold = self.config.get('loss', {}).get('collapse_threshold', 0.8)
        
        if max_ratio > collapse_threshold:
            self.logger.warning(f"‚ö†Ô∏è –°—Ö–ª–æ–ø—ã–≤–∞–Ω–∏–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–æ! –û–¥–∏–Ω –∫–ª–∞—Å—Å —Å–æ—Å—Ç–∞–≤–ª—è–µ—Ç {max_ratio*100:.1f}%")
            return True
            
        return False
        
    def _apply_correction(self, stage_idx: int):
        """
        –ü—Ä–∏–º–µ–Ω—è–µ—Ç –∫–æ—Ä—Ä–µ–∫—Ü–∏—é –ø—Ä–∏ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏–∏ —Å—Ö–ª–æ–ø—ã–≤–∞–Ω–∏—è
        """
        # –ö–æ–Ω—Å–µ—Ä–≤–∞—Ç–∏–≤–Ω–∞—è –ø–µ—Ä–µ–∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è direction head
        if hasattr(self.model, 'direction_head'):
            self.logger.info("üîß –ü–µ—Ä–µ–∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è direction head...")
            
            for module in self.model.direction_head.modules():
                if isinstance(module, nn.Linear) and module.out_features == 12:
                    # –ö–æ–Ω—Å–µ—Ä–≤–∞—Ç–∏–≤–Ω–∞—è –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –¥–ª—è –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–µ–Ω–∏—è NaN
                    nn.init.xavier_uniform_(module.weight, gain=0.3)
                    if module.bias is not None:
                        with torch.no_grad():
                            bias = module.bias.view(4, 3)
                            bias[:, 0] = 0.3    # LONG bias (—É–º–µ—Ä–µ–Ω–Ω–æ –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã–π)
                            bias[:, 1] = 0.3    # SHORT bias (—É–º–µ—Ä–µ–Ω–Ω–æ –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã–π)  
                            bias[:, 2] = -1.2   # FLAT bias (—Å–∏–ª—å–Ω–æ –æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã–π –¥–ª—è –ø–æ–¥–∞–≤–ª–µ–Ω–∏—è)
                            
        # –ù–µ —É–≤–µ–ª–∏—á–∏–≤–∞–µ–º learning rate —á—Ç–æ–±—ã –∏–∑–±–µ–∂–∞—Ç—å NaN
        self.logger.info("üîß –ö–æ—Ä—Ä–µ–∫—Ü–∏—è –ø—Ä–∏–º–µ–Ω–µ–Ω–∞ –±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏—è learning rate")
                
    def _analyze_stage_results(self, stage_name: str, history: Dict, val_loader: DataLoader):
        """
        –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã —ç—Ç–∞–ø–∞
        """
        if 'val_loss' in history and history['val_loss']:
            final_val_loss = history['val_loss'][-1]
            best_val_loss = min(history['val_loss'])
            self.logger.info(f"üìä –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —ç—Ç–∞–ø–∞ {stage_name}:")
            self.logger.info(f"   Final val loss: {final_val_loss:.4f}")
            self.logger.info(f"   Best val loss: {best_val_loss:.4f}")