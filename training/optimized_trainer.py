"""
–û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π Trainer –¥–ª—è –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–π —É—Ç–∏–ª–∏–∑–∞—Ü–∏–∏ GPU
"""

import torch
import torch.nn as nn
from torch.utils.data import DataLoader
from torch.cuda.amp import GradScaler, autocast
import numpy as np
from typing import Dict, Optional
from pathlib import Path
import time
from tqdm import tqdm
from collections import deque

from utils.logger import get_logger
from training.trainer import Trainer
import torch.nn.functional as F

class OptimizedTrainer(Trainer):
    """–û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è Trainer —Å –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–π –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å—é GPU"""
    
    def __init__(self, model: nn.Module, config: Dict, device: Optional[torch.device] = None):
        super().__init__(model, config, device)
        
        self.logger = get_logger("OptimizedTrainer")
        
        # –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –¥–ª—è GPU
        self.async_metrics = True  # –ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π —Ä–∞—Å—á–µ—Ç –º–µ—Ç—Ä–∏–∫
        self.log_interval = 10  # –õ–æ–≥–∏—Ä–æ–≤–∞—Ç—å –∫–∞–∂–¥—ã–µ N –±–∞—Ç—á–µ–π
        self.metrics_buffer = deque(maxlen=100)  # –ë—É—Ñ–µ—Ä –¥–ª—è –º–µ—Ç—Ä–∏–∫
        
        # EMA (Exponential Moving Average) –¥–ª—è –≤–µ—Å–æ–≤ –º–æ–¥–µ–ª–∏ - –û–¢–ö–õ–Æ–ß–ï–ù–û
        self.use_ema = False  # –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ –æ—Ç–∫–ª—é—á–µ–Ω–æ –¥–ª—è –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–µ–Ω–∏—è –º–∞—Å–∫–∏—Ä–æ–≤–∞–Ω–∏—è –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è
        self.ema_decay = config.get('model', {}).get('ema_decay', 0.99)  # –°–Ω–∏–∂–µ–Ω decay
        self.ema_model = None
        if self.use_ema:
            self._init_ema()
            self.logger.info(f"‚úÖ EMA –≤–∫–ª—é—á–µ–Ω —Å decay={self.ema_decay}")
        
        # Dropout schedule –ø–∞—Ä–∞–º–µ—Ç—Ä—ã - –û–¢–ö–õ–Æ–ß–ï–ù–û
        self.use_dropout_schedule = False  # –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ –æ—Ç–∫–ª—é—á–µ–Ω–æ
        if self.use_dropout_schedule:
            self.initial_dropout = config.get('model', {}).get('dropout', 0.3)
            self.final_dropout = 0.1  # –ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–π dropout
            self.dropout_warmup_epochs = 20  # –≠–ø–æ—Ö–∏ –¥–ª—è —Å–Ω–∏–∂–µ–Ω–∏—è dropout
            self.logger.info(f"‚úÖ Dropout Schedule –≤–∫–ª—é—á–µ–Ω: {self.initial_dropout} ‚Üí {self.final_dropout}")
        
        # Mixup augmentation –¥–ª—è direction –∑–∞–¥–∞—á–∏ - –û–¢–ö–õ–Æ–ß–ï–ù–û
        self.use_mixup = False  # –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ –æ—Ç–∫–ª—é—á–µ–Ω–æ –¥–ª—è –º–Ω–æ–≥–æ–∑–∞–¥–∞—á–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è
        if self.use_mixup:
            self.mixup_alpha = config.get('model', {}).get('mixup_alpha', 0.2)
            self.logger.info(f"‚úÖ Mixup augmentation –≤–∫–ª—é—á–µ–Ω: alpha={self.mixup_alpha}")
        
        # –ö–æ–º–ø–∏–ª—è—Ü–∏—è –º–æ–¥–µ–ª–∏ –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è (PyTorch 2.0+)
        if hasattr(torch, 'compile') and config.get('model', {}).get('compile_model', False):
            if self.device.type == 'cuda':
                gpu_name = torch.cuda.get_device_name(0)
                if 'RTX 5090' in gpu_name:
                    self.logger.warning("‚ö†Ô∏è torch.compile –æ—Ç–∫–ª—é—á–µ–Ω –¥–ª—è RTX 5090 (sm_120) - –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç—Å—è —Ç–µ–∫—É—â–µ–π –≤–µ—Ä—Å–∏–µ–π PyTorch")
                else:
                    self.logger.info("üöÄ –ö–æ–º–ø–∏–ª—è—Ü–∏—è –º–æ–¥–µ–ª–∏ —Å torch.compile...")
                    self.model = torch.compile(self.model, mode='reduce-overhead')
            else:
                self.logger.info("üöÄ –ö–æ–º–ø–∏–ª—è—Ü–∏—è –º–æ–¥–µ–ª–∏ —Å torch.compile...")
                self.model = torch.compile(self.model, mode='reduce-overhead')
        
        # CUDA –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏
        if self.device.type == 'cuda':
            # –í–∫–ª—é—á–∞–µ–º TensorFloat-32 –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è –Ω–∞ Ampere+ (RTX 30xx –∏ –≤—ã—à–µ)
            torch.backends.cuda.matmul.allow_tf32 = True
            torch.backends.cudnn.allow_tf32 = True
            
            # Benchmark mode –¥–ª—è cudnn
            torch.backends.cudnn.benchmark = True
            torch.backends.cudnn.deterministic = False
            
            self.logger.info("‚úÖ GPU –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –≤–∫–ª—é—á–µ–Ω—ã:")
            self.logger.info(f"   - TF32: {torch.backends.cuda.matmul.allow_tf32}")
            self.logger.info(f"   - cuDNN benchmark: {torch.backends.cudnn.benchmark}")
            self.logger.info(f"   - Mixed Precision: {self.use_amp}")
    
    def update_dropout(self, epoch: int):
        """–û–±–Ω–æ–≤–ª—è–µ—Ç dropout rate —Å–æ–≥–ª–∞—Å–Ω–æ —Ä–∞—Å–ø–∏—Å–∞–Ω–∏—é"""
        if not self.use_dropout_schedule:
            return
            
        if epoch < self.dropout_warmup_epochs:
            # –õ–∏–Ω–µ–π–Ω–æ–µ —Å–Ω–∏–∂–µ–Ω–∏–µ dropout
            progress = epoch / self.dropout_warmup_epochs
            current_dropout = self.initial_dropout - (self.initial_dropout - self.final_dropout) * progress
            
            # –û–±–Ω–æ–≤–ª—è–µ–º dropout –≤–æ –≤—Å–µ—Ö —Å–ª–æ—è—Ö –º–æ–¥–µ–ª–∏
            for module in self.model.modules():
                if isinstance(module, nn.Dropout):
                    module.p = current_dropout
                    
            if epoch % 5 == 0:
                self.logger.info(f"üìä Dropout –æ–±–Ω–æ–≤–ª–µ–Ω: {current_dropout:.3f}")
    
    def _init_ema(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è EMA –º–æ–¥–µ–ª–∏"""
        import copy
        self.ema_model = copy.deepcopy(self.model)
        self.ema_model.eval()
        for param in self.ema_model.parameters():
            param.requires_grad = False
    
    def _update_ema(self):
        """–û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –≤–µ—Å–æ–≤ EMA –º–æ–¥–µ–ª–∏"""
        if not self.use_ema or self.ema_model is None:
            return
            
        with torch.no_grad():
            for ema_param, model_param in zip(self.ema_model.parameters(), self.model.parameters()):
                ema_param.data.mul_(self.ema_decay).add_(model_param.data, alpha=1 - self.ema_decay)
    
    def mixup_data(self, x: torch.Tensor, y: torch.Tensor, alpha: float = 0.2):
        """–ü—Ä–∏–º–µ–Ω—è–µ—Ç mixup augmentation –∫ –¥–∞–Ω–Ω—ã–º"""
        if alpha > 0:
            lam = np.random.beta(alpha, alpha)
        else:
            lam = 1
            
        batch_size = x.size(0)
        index = torch.randperm(batch_size).to(x.device)
        
        mixed_x = lam * x + (1 - lam) * x[index]
        y_a, y_b = y, y[index]
        
        return mixed_x, y_a, y_b, lam
    
    def train_epoch(self, train_loader: DataLoader) -> Dict[str, float]:
        """–û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –æ–¥–Ω–æ–π —ç–ø–æ—Ö–∏"""
        self.model.train()
        
        epoch_loss = 0.0
        epoch_metrics = {}
        batch_times = deque(maxlen=50)
        
        progress_bar = tqdm(train_loader, desc="Training", leave=False)
        
        # –û–±–Ω—É–ª–µ–Ω–∏–µ –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤ —Å –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–µ–π –ø–∞–º—è—Ç–∏
        self.optimizer.zero_grad(set_to_none=True)
        
        # –í—Ä–µ–º—è –Ω–∞—á–∞–ª–∞ —ç–ø–æ—Ö–∏
        epoch_start = time.time()
        last_log_time = epoch_start
        
        for batch_idx, (inputs, targets, info) in enumerate(progress_bar):
            batch_start = time.time()
            
            # –ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π –ø–µ—Ä–µ–Ω–æ—Å –Ω–∞ GPU —Å non_blocking
            inputs = inputs.to(self.device, non_blocking=True)
            if isinstance(targets, torch.Tensor):
                targets = targets.to(self.device, non_blocking=True)
            elif isinstance(targets, dict):
                targets = {k: v.to(self.device, non_blocking=True) for k, v in targets.items()}
            
            # Forward pass —Å AMP
            if self.use_amp:
                with autocast():
                    outputs = self.model(inputs)
                    loss = self._compute_loss(outputs, targets)
            else:
                outputs = self.model(inputs)
                loss = self._compute_loss(outputs, targets)
            
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ NaN/Inf
            if torch.isnan(loss) or torch.isinf(loss):
                self.logger.warning(f"‚ö†Ô∏è NaN/Inf –æ–±–Ω–∞—Ä—É–∂–µ–Ω –≤ loss –Ω–∞ –±–∞—Ç—á–µ {batch_idx}")
                self.logger.warning(f"   Loss value: {loss.item()}")
                # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —ç—Ç–æ—Ç –±–∞—Ç—á
                self.optimizer.zero_grad()
                continue
                
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ outputs –Ω–∞ NaN
            if torch.isnan(outputs).any():
                self.logger.warning(f"‚ö†Ô∏è NaN –æ–±–Ω–∞—Ä—É–∂–µ–Ω –≤ outputs –Ω–∞ –±–∞—Ç—á–µ {batch_idx}")
                self.logger.warning(f"   NaN count: {torch.isnan(outputs).sum().item()}")
                # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —ç—Ç–æ—Ç –±–∞—Ç—á
                self.optimizer.zero_grad()
                continue
            
            # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è loss –¥–ª—è gradient accumulation
            loss = loss / self.gradient_accumulation_steps
            
            # Backward pass
            if self.use_amp:
                self.scaler.scale(loss).backward()
            else:
                loss.backward()
            
            # –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –≤–µ—Å–æ–≤ –∫–∞–∂–¥—ã–µ N —à–∞–≥–æ–≤
            if (batch_idx + 1) % self.gradient_accumulation_steps == 0:
                if self.use_amp:
                    # Unscale –ø–µ—Ä–µ–¥ clipping
                    self.scaler.unscale_(self.optimizer)
                    # Gradient clipping
                    if self.gradient_clip > 0:
                        torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.gradient_clip)
                    # Optimizer step
                    self.scaler.step(self.optimizer)
                    self.scaler.update()
                else:
                    # Gradient clipping
                    if self.gradient_clip > 0:
                        torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.gradient_clip)
                    # Optimizer step
                    self.optimizer.step()
                    
                    # –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ EMA –º–æ–¥–µ–ª–∏
                    self._update_ema()
                
                # –û–±–Ω—É–ª–µ–Ω–∏–µ –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤
                self.optimizer.zero_grad(set_to_none=True)
            
            # –ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ–µ –Ω–∞–∫–æ–ø–ª–µ–Ω–∏–µ –º–µ—Ç—Ä–∏–∫ (–±–µ–∑ .item() –∫–∞–∂–¥—ã–π –±–∞—Ç—á)
            batch_loss = loss.detach() * self.gradient_accumulation_steps
            epoch_loss += batch_loss
            
            # –í—Ä–µ–º—è –±–∞—Ç—á–∞
            batch_time = time.time() - batch_start
            batch_times.append(batch_time)
            
            # –ü–µ—Ä–∏–æ–¥–∏—á–µ—Å–∫–æ–µ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ
            if batch_idx % self.log_interval == 0:
                # –°–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ü–∏—è –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –∞–∫—Ç—É–∞–ª—å–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π
                if self.device.type == 'cuda':
                    torch.cuda.synchronize()
                
                current_loss = batch_loss.item()
                avg_batch_time = np.mean(batch_times)
                samples_per_sec = train_loader.batch_size / avg_batch_time
                
                # –†–∞—Å—à–∏—Ä–µ–Ω–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ GPU –¥–ª—è RTX 5090
                if self.device.type == 'cuda':
                    gpu_memory_allocated = torch.cuda.memory_allocated() / 1024**3
                    gpu_memory_reserved = torch.cuda.memory_reserved() / 1024**3
                    gpu_utilization = (gpu_memory_allocated / gpu_memory_reserved * 100) if gpu_memory_reserved > 0 else 0
                else:
                    gpu_memory_allocated = 0
                    gpu_memory_reserved = 0
                    gpu_utilization = 0
                
                progress_bar.set_postfix({
                    'loss': f'{current_loss:.4f}',
                    'samples/s': f'{samples_per_sec:.0f}',
                    'gpu_mem': f'{gpu_memory_allocated:.1f}/{gpu_memory_reserved:.1f}GB',
                    'gpu_use': f'{gpu_utilization:.0f}%',
                    'batch_ms': f'{avg_batch_time*1000:.0f}'
                })
                
                # –î–µ—Ç–∞–ª—å–Ω–æ–µ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –∫–∞–∂–¥—ã–µ 100 –±–∞—Ç—á–µ–π
                if batch_idx % 100 == 0 and batch_idx > 0:
                    elapsed = time.time() - last_log_time
                    self.logger.info(f"Batch {batch_idx}: "
                                   f"loss={current_loss:.4f}, "
                                   f"speed={samples_per_sec:.0f} samples/s, "
                                   f"time={elapsed:.1f}s")
                    last_log_time = time.time()
            
            # –ü–µ—Ä–∏–æ–¥–∏—á–µ—Å–∫–∞—è –æ—á–∏—Å—Ç–∫–∞ –∫—ç—à–∞ GPU
            if self.device.type == 'cuda' and batch_idx % self.gpu_cache_clear_freq == 0:
                torch.cuda.empty_cache()
        
        # –§–∏–Ω–∞–ª—å–Ω–∞—è —Å–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ü–∏—è
        if self.device.type == 'cuda':
            torch.cuda.synchronize()
        
        # –í—ã—á–∏—Å–ª–µ–Ω–∏–µ —Å—Ä–µ–¥–Ω–∏—Ö –º–µ—Ç—Ä–∏–∫
        avg_loss = (epoch_loss / len(train_loader)).item()
        epoch_time = time.time() - epoch_start
        
        self.logger.info(f"üìä –≠–ø–æ—Ö–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞ –∑–∞ {epoch_time:.1f}—Å")
        self.logger.info(f"   –°—Ä–µ–¥–Ω—è—è —Å–∫–æ—Ä–æ—Å—Ç—å: {len(train_loader.dataset)/epoch_time:.0f} samples/s")
        self.logger.info(f"   –°—Ä–µ–¥–Ω–∏–π loss: {avg_loss:.4f}")
        
        return {
            'loss': avg_loss,
            'epoch_time': epoch_time,
            'samples_per_second': len(train_loader.dataset) / epoch_time
        }
    
    def validate(self, val_loader: DataLoader) -> Dict[str, float]:
        """–û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –≤–∞–ª–∏–¥–∞—Ü–∏—è"""
        self.model.eval()
        
        val_loss = 0.0
        val_metrics = {}
        
        with torch.no_grad():
            for batch_idx, (inputs, targets, info) in enumerate(tqdm(val_loader, desc="Validation", leave=False)):
                # –ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π –ø–µ—Ä–µ–Ω–æ—Å –Ω–∞ GPU
                inputs = inputs.to(self.device, non_blocking=True)
                if isinstance(targets, torch.Tensor):
                    targets = targets.to(self.device, non_blocking=True)
                elif isinstance(targets, dict):
                    targets = {k: v.to(self.device, non_blocking=True) for k, v in targets.items()}
                
                # Forward pass —Å AMP
                if self.use_amp:
                    with autocast():
                        outputs = self.model(inputs)
                        loss = self._compute_loss(outputs, targets)
                else:
                    outputs = self.model(inputs)
                    loss = self._compute_loss(outputs, targets)
                
                # –ù–∞–∫–æ–ø–ª–µ–Ω–∏–µ loss
                val_loss += loss.detach()
                
                # –ü–µ—Ä–∏–æ–¥–∏—á–µ—Å–∫–∞—è –æ—á–∏—Å—Ç–∫–∞ –∫—ç—à–∞
                if self.device.type == 'cuda' and batch_idx % 50 == 0:
                    torch.cuda.empty_cache()
        
        # –§–∏–Ω–∞–ª—å–Ω–∞—è —Å–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ü–∏—è
        if self.device.type == 'cuda':
            torch.cuda.synchronize()
        
        avg_val_loss = (val_loss / len(val_loader)).item()
        
        return {
            'val_loss': avg_val_loss
        }
    
    # –ê–ª–∏–∞—Å –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏
    validate_epoch = validate
    
    def train(self, train_loader: DataLoader, val_loader: Optional[DataLoader] = None) -> Dict:
        """–û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –ø—Ä–æ—Ü–µ—Å—Å –æ–±—É—á–µ–Ω–∏—è"""
        self.logger.info("üöÄ –ù–∞—á–∞–ª–æ –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è")
        self.logger.info(f"   –≠–ø–æ—Ö: {self.epochs}")
        self.logger.info(f"   Batch size: {train_loader.batch_size}")
        self.logger.info(f"   Gradient accumulation: {self.gradient_accumulation_steps}")
        self.logger.info(f"   Effective batch size: {train_loader.batch_size * self.gradient_accumulation_steps}")
        
        # –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ GPU
        if self.device.type == 'cuda':
            gpu_name = torch.cuda.get_device_name(0)
            gpu_memory_total = torch.cuda.get_device_properties(0).total_memory / 1024**3
            self.logger.info(f"üî• GPU: {gpu_name}")
            self.logger.info(f"   –ü–∞–º—è—Ç—å GPU: {gpu_memory_total:.1f} GB")
            self.logger.info(f"   Mixed Precision: {'–í–∫–ª—é—á–µ–Ω–æ' if self.use_amp else '–í—ã–∫–ª—é—á–µ–Ω–æ'}")
            if 'RTX 5090' in gpu_name:
                self.logger.info("   ‚ö° –û–±–Ω–∞—Ä—É–∂–µ–Ω–∞ RTX 5090 - –∏—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –¥–ª—è sm_120")
        
        best_val_loss = float('inf')
        patience_counter = 0
        
        for epoch in range(self.epochs):
            self.logger.info(f"\n{'='*60}")
            self.logger.info(f"–≠–ø–æ—Ö–∞ {epoch + 1}/{self.epochs}")
            
            # –û–±–Ω–æ–≤–ª—è–µ–º dropout —Å–æ–≥–ª–∞—Å–Ω–æ —Ä–∞—Å–ø–∏—Å–∞–Ω–∏—é
            self.update_dropout(epoch)
            
            # –û–±–Ω–æ–≤–ª—è–µ–º —ç–ø–æ—Ö—É –≤ loss —Ñ—É–Ω–∫—Ü–∏–∏ –¥–ª—è –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏—Ö –≤–µ—Å–æ–≤
            if hasattr(self.criterion, 'set_epoch'):
                self.criterion.set_epoch(epoch)
                if epoch < 10:  # –õ–æ–≥–∏—Ä—É–µ–º warmup –ø—Ä–æ–≥—Ä–µ—Å—Å
                    current_weight = self.criterion.get_dynamic_direction_weight()
                    self.logger.info(f"üìà Direction loss weight (warmup): {current_weight:.2f}")
            
            # –û–±—É—á–µ–Ω–∏–µ
            train_metrics = self.train_epoch(train_loader)
            self.history['train_loss'].append(train_metrics['loss'])
            
            # –í–∞–ª–∏–¥–∞—Ü–∏—è —Å —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–º–∏ –º–µ—Ç—Ä–∏–∫–∞–º–∏
            if val_loader is not None:
                val_metrics = self.validate_with_enhanced_metrics(val_loader)
                self.history['val_loss'].append(val_metrics['val_loss'])
                
                # Early stopping
                if val_metrics['val_loss'] < best_val_loss - self.config['model'].get('min_delta', 0.0001):
                    best_val_loss = val_metrics['val_loss']
                    patience_counter = 0
                    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –ª—É—á—à–µ–π –º–æ–¥–µ–ª–∏
                    self._save_checkpoint(epoch, val_metrics['val_loss'], is_best=True)
                else:
                    patience_counter += 1
                
                self.logger.info(f"üìà Train Loss: {train_metrics['loss']:.4f}, "
                               f"Val Loss: {val_metrics['val_loss']:.4f} "
                               f"(best: {best_val_loss:.4f}, patience: {patience_counter})")
                
                if patience_counter >= self.early_stopping_patience:
                    self.logger.info("‚ö†Ô∏è Early stopping triggered!")
                    break
            else:
                self.logger.info(f"üìà Train Loss: {train_metrics['loss']:.4f}")
            
            # Scheduler step
            if self.scheduler is not None:
                if hasattr(self.scheduler, 'step'):
                    # ReduceLROnPlateau —Ç—Ä–µ–±—É–µ—Ç –º–µ—Ç—Ä–∏–∫—É
                    if type(self.scheduler).__name__ == 'ReduceLROnPlateau':
                        if val_loader is not None:
                            self.scheduler.step(val_metrics['val_loss'])
                        else:
                            self.scheduler.step(train_metrics['loss'])
                    else:
                        self.scheduler.step()
            
            # –ü–µ—Ä–∏–æ–¥–∏—á–µ—Å–∫–æ–µ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ confusion matrix (–∫–∞–∂–¥—ã–µ 5 —ç–ø–æ—Ö)
            if (epoch + 1) % 5 == 0 and val_loader is not None:
                self.log_confusion_matrix(val_loader, epoch + 1)
            
            # –ü–µ—Ä–∏–æ–¥–∏—á–µ—Å–∫–æ–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ
            if (epoch + 1) % 10 == 0:
                self._save_checkpoint(epoch, train_metrics['loss'], is_best=False)
        
        self.logger.info("‚úÖ –û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ!")
        return self.history
    
    def compute_direction_metrics(self, outputs: torch.Tensor, targets: torch.Tensor) -> Dict[str, float]:
        """
        –†–∞—Å—á–µ—Ç –º–µ—Ç—Ä–∏–∫ –¥–ª—è direction –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π
        
        Args:
            outputs: (batch_size, 20) - –≤—ã—Ö–æ–¥—ã –º–æ–¥–µ–ª–∏
            targets: (batch_size, 20) - —Ü–µ–ª–µ–≤—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è
            
        Returns:
            Dict —Å –º–µ—Ç—Ä–∏–∫–∞–º–∏ directional accuracy –∏ win rate
        """
        metrics = {}
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º direction –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ (–∏–Ω–¥–µ–∫—Å—ã 4-7)
        direction_outputs = outputs[:, 4:8]  # direction_15m, 1h, 4h, 12h
        direction_targets = targets[:, 4:8]
        
        # –†–∞—Å—á–µ—Ç directional accuracy –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Ç–∞–π–º—Ñ—Ä–µ–π–º–∞
        timeframes = ['15m', '1h', '4h', '12h']
        
        for i, tf in enumerate(timeframes):
            # –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï: –ü—Ä–∞–≤–∏–ª—å–Ω–∞—è –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏—è –≤—ã—Ö–æ–¥–æ–≤ –º–æ–¥–µ–ª–∏
            # –ï—Å–ª–∏ outputs –∏–º–µ–µ—Ç _direction_logits, –∏—Å–ø–æ–ª—å–∑—É–µ–º –∏—Ö
            if hasattr(outputs, '_direction_logits'):
                # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ª–æ–≥–∏—Ç—ã –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –∫–ª–∞—Å—Å–æ–≤ —á–µ—Ä–µ–∑ softmax + argmax
                direction_logits = outputs._direction_logits[:, i, :]  # (batch_size, 3)
                pred_classes = torch.argmax(torch.softmax(direction_logits, dim=-1), dim=-1)
            else:
                # Fallback: –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∏—Ä—É–µ–º –∫–∞–∫ –Ω–µ–ø—Ä–µ—Ä—ã–≤–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è [0, 2]
                pred_classes = torch.round(direction_outputs[:, i]).clamp(0, 2).long()
            
            true_classes = direction_targets[:, i].long()
            
            # –¢–æ—á–Ω–æ—Å—Ç—å –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏—è
            correct = (pred_classes == true_classes).float()
            accuracy = correct.mean().item()
            metrics[f'direction_accuracy_{tf}'] = accuracy
            
            # –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π
            unique_preds, pred_counts = torch.unique(pred_classes, return_counts=True)
            pred_distribution = {}
            for class_idx in range(3):  # LONG=0, SHORT=1, FLAT=2
                count = pred_counts[unique_preds == class_idx].sum().item() if (unique_preds == class_idx).any() else 0
                pred_distribution[class_idx] = count
            
            # –í—ã—á–∏—Å–ª—è–µ–º —ç–Ω—Ç—Ä–æ–ø–∏—é –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏—è
            total_preds = pred_classes.shape[0]
            pred_probs = torch.tensor([pred_distribution.get(i, 0) / total_preds for i in range(3)])
            pred_probs = pred_probs + 1e-8  # –ò–∑–±–µ–≥–∞–µ–º log(0)
            entropy = -(pred_probs * torch.log(pred_probs)).sum().item()
            normalized_entropy = entropy / np.log(3)  # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –∫ [0, 1]
            
            metrics[f'pred_entropy_{tf}'] = normalized_entropy
            metrics[f'pred_long_ratio_{tf}'] = pred_distribution.get(0, 0) / total_preds
            metrics[f'pred_short_ratio_{tf}'] = pred_distribution.get(1, 0) / total_preds
            metrics[f'pred_flat_ratio_{tf}'] = pred_distribution.get(2, 0) / total_preds
            
            # –¢–æ—á–Ω–æ—Å—Ç—å –¥–ª—è UP/DOWN (–∏—Å–∫–ª—é—á–∞–µ–º FLAT)
            non_flat_mask = (true_classes != 2)
            if non_flat_mask.sum() > 0:
                up_down_correct = correct[non_flat_mask].mean().item()
                metrics[f'up_down_accuracy_{tf}'] = up_down_correct
            
            # –ü—Ä–æ—Ü–µ–Ω—Ç –ø—Ä–∞–≤–∏–ª—å–Ω—ã—Ö UP –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π
            up_mask = (true_classes == 0)
            if up_mask.sum() > 0:
                up_correct = correct[up_mask].mean().item()
                metrics[f'up_accuracy_{tf}'] = up_correct
            
            # –ü—Ä–æ—Ü–µ–Ω—Ç –ø—Ä–∞–≤–∏–ª—å–Ω—ã—Ö DOWN –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π  
            down_mask = (true_classes == 1)
            if down_mask.sum() > 0:
                down_correct = correct[down_mask].mean().item()
                metrics[f'down_accuracy_{tf}'] = down_correct
        
        # –û–±—â–∞—è directional accuracy (—Å—Ä–µ–¥–Ω–µ–µ –ø–æ –≤—Å–µ–º —Ç–∞–π–º—Ñ—Ä–µ–π–º–∞–º)
        overall_accuracy = np.mean([metrics[f'direction_accuracy_{tf}'] for tf in timeframes])
        metrics['direction_accuracy_overall'] = overall_accuracy
        
        # –û–±—â–∞—è –º–µ—Ç—Ä–∏–∫–∞ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π
        overall_entropy = np.mean([metrics[f'pred_entropy_{tf}'] for tf in timeframes])
        metrics['pred_entropy_overall'] = overall_entropy
        
        # –°—Ä–µ–¥–Ω–∏–µ —Å–æ–æ—Ç–Ω–æ—à–µ–Ω–∏—è –∫–ª–∞—Å—Å–æ–≤
        metrics['pred_long_ratio_overall'] = np.mean([metrics[f'pred_long_ratio_{tf}'] for tf in timeframes])
        metrics['pred_short_ratio_overall'] = np.mean([metrics[f'pred_short_ratio_{tf}'] for tf in timeframes])
        metrics['pred_flat_ratio_overall'] = np.mean([metrics[f'pred_flat_ratio_{tf}'] for tf in timeframes])
        
        # –ü—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ –µ—Å–ª–∏ –º–æ–¥–µ–ª—å –ø—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞–µ—Ç —Å–ª–∏—à–∫–æ–º –æ–¥–Ω–æ–æ–±—Ä–∞–∑–Ω–æ
        if overall_entropy < 0.3:
            self.logger.warning(f"‚ö†Ô∏è –ù–∏–∑–∫–æ–µ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π! –≠–Ω—Ç—Ä–æ–ø–∏—è: {overall_entropy:.3f}")
            self.logger.warning(f"   LONG: {metrics['pred_long_ratio_overall']:.1%}, "
                              f"SHORT: {metrics['pred_short_ratio_overall']:.1%}, "
                              f"FLAT: {metrics['pred_flat_ratio_overall']:.1%}")
        
        return metrics
    
    def compute_trading_metrics(self, outputs: torch.Tensor, targets: torch.Tensor) -> Dict[str, float]:
        """
        –†–∞—Å—á–µ—Ç —Ç–æ—Ä–≥–æ–≤—ã—Ö –º–µ—Ç—Ä–∏–∫: win rate, profit factor, etc.
        
        Args:
            outputs: (batch_size, 20) - –≤—ã—Ö–æ–¥—ã –º–æ–¥–µ–ª–∏
            targets: (batch_size, 20) - —Ü–µ–ª–µ–≤—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è
            
        Returns:
            Dict —Å —Ç–æ—Ä–≥–æ–≤—ã–º–∏ –º–µ—Ç—Ä–∏–∫–∞–º–∏
        """
        metrics = {}
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º –Ω—É–∂–Ω—ã–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ
        future_returns = targets[:, 0:4]  # future_return_15m, 1h, 4h, 12h
        direction_outputs = outputs[:, 4:8]
        direction_targets = targets[:, 4:8]
        
        timeframes = ['15m', '1h', '4h', '12h']
        
        for i, tf in enumerate(timeframes):
            # –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï: –ü—Ä–∞–≤–∏–ª—å–Ω–∞—è –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã—Ö –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–π
            if hasattr(outputs, '_direction_logits'):
                # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ª–æ–≥–∏—Ç—ã –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –∫–ª–∞—Å—Å–æ–≤ —á–µ—Ä–µ–∑ softmax + argmax
                direction_logits = outputs._direction_logits[:, i, :]  # (batch_size, 3)
                pred_classes = torch.argmax(torch.softmax(direction_logits, dim=-1), dim=-1)
            else:
                # Fallback: –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∏—Ä—É–µ–º –∫–∞–∫ –Ω–µ–ø—Ä–µ—Ä—ã–≤–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è [0, 2]
                pred_classes = torch.round(direction_outputs[:, i]).clamp(0, 2).long()
            
            true_returns = future_returns[:, i]
            
            # –ò–º–∏—Ç–∏—Ä—É–µ–º —Ç–æ—Ä–≥–æ–≤—ã–µ —Å–∏–≥–Ω–∞–ª—ã
            # UP –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ (0) = LONG –ø–æ–∑–∏—Ü–∏—è
            long_mask = (pred_classes == 0)
            # DOWN –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ (1) = SHORT –ø–æ–∑–∏—Ü–∏—è  
            short_mask = (pred_classes == 1)
            # FLAT –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ (2) = –Ω–µ—Ç –ø–æ–∑–∏—Ü–∏–∏
            
            if long_mask.sum() > 0 or short_mask.sum() > 0:
                # –†–∞—Å—á–µ—Ç P&L
                pnl = torch.zeros_like(true_returns)
                
                # LONG –ø–æ–∑–∏—Ü–∏–∏: –ø—Ä–∏–±—ã–ª—å = –∏–∑–º–µ–Ω–µ–Ω–∏–µ —Ü–µ–Ω—ã
                if long_mask.sum() > 0:
                    pnl[long_mask] = true_returns[long_mask]
                
                # SHORT –ø–æ–∑–∏—Ü–∏–∏: –ø—Ä–∏–±—ã–ª—å = -–∏–∑–º–µ–Ω–µ–Ω–∏–µ —Ü–µ–Ω—ã
                if short_mask.sum() > 0:
                    pnl[short_mask] = -true_returns[short_mask]
                
                # –£–±–∏—Ä–∞–µ–º –∫–æ–º–∏—Å—Å–∏—é
                commission = 0.001  # 0.1%
                trading_mask = long_mask | short_mask
                pnl[trading_mask] -= commission
                
                # Win Rate
                profitable_trades = pnl[trading_mask] > 0
                if trading_mask.sum() > 0:
                    win_rate = profitable_trades.float().mean().item()
                    metrics[f'win_rate_{tf}'] = win_rate
                    
                    # Profit Factor
                    profits = pnl[trading_mask & (pnl > 0)]
                    losses = pnl[trading_mask & (pnl < 0)]
                    
                    if len(profits) > 0 and len(losses) > 0:
                        profit_factor = profits.sum().item() / abs(losses.sum().item())
                        metrics[f'profit_factor_{tf}'] = profit_factor
                    
                    # –°—Ä–µ–¥–Ω–∏–π P&L
                    avg_pnl = pnl[trading_mask].mean().item()
                    metrics[f'avg_pnl_{tf}'] = avg_pnl
                    
                    # –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –ø—Ä–æ—Å–∞–¥–∫–∞ (—É–ø—Ä–æ—â–µ–Ω–Ω–∞—è)
                    cumulative_pnl = torch.cumsum(pnl[trading_mask], dim=0)
                    running_max = torch.cummax(cumulative_pnl, dim=0)[0]
                    drawdown = running_max - cumulative_pnl
                    max_drawdown = drawdown.max().item()
                    metrics[f'max_drawdown_{tf}'] = max_drawdown
        
        # –û–±—â–∏–µ –º–µ—Ç—Ä–∏–∫–∏
        if any(f'win_rate_{tf}' in metrics for tf in timeframes):
            # –°—Ä–µ–¥–Ω–∏–π win rate
            win_rates = [metrics[f'win_rate_{tf}'] for tf in timeframes if f'win_rate_{tf}' in metrics]
            if win_rates:
                metrics['win_rate_overall'] = np.mean(win_rates)
        
        return metrics
    
    def compute_class_distribution_metrics(self, predictions: torch.Tensor, targets: torch.Tensor, 
                                          timeframe: str) -> Dict[str, float]:
        """
        –†–∞—Å—á–µ—Ç –¥–µ—Ç–∞–ª—å–Ω—ã—Ö –º–µ—Ç—Ä–∏–∫ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –∫–ª–∞—Å—Å–æ–≤
        
        Args:
            predictions: –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã–µ –∫–ª–∞—Å—Å—ã
            targets: –∏—Å—Ç–∏–Ω–Ω—ã–µ –∫–ª–∞—Å—Å—ã
            timeframe: —Ç–∞–π–º—Ñ—Ä–µ–π–º ('15m', '1h', '4h', '12h')
            
        Returns:
            Dict —Å –º–µ—Ç—Ä–∏–∫–∞–º–∏ –ø–æ –∫–ª–∞—Å—Å–∞–º
        """
        metrics = {}
        
        # –ü–æ–¥—Å—á–µ—Ç –∫–ª–∞—Å—Å–æ–≤
        unique_pred, pred_counts = torch.unique(predictions, return_counts=True)
        unique_true, true_counts = torch.unique(targets, return_counts=True)
        
        # –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π
        total_samples = len(predictions)
        for class_id in [0, 1, 2]:  # LONG, SHORT, FLAT
            class_name = ['LONG', 'SHORT', 'FLAT'][class_id]
            
            # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π –¥–∞–Ω–Ω–æ–≥–æ –∫–ª–∞—Å—Å–∞
            pred_count = pred_counts[unique_pred == class_id].sum().item() if (unique_pred == class_id).any() else 0
            true_count = true_counts[unique_true == class_id].sum().item() if (unique_true == class_id).any() else 0
            
            # –ü—Ä–æ—Ü–µ–Ω—Ç–Ω–æ–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ
            pred_pct = (pred_count / total_samples * 100) if total_samples > 0 else 0
            true_pct = (true_count / total_samples * 100) if total_samples > 0 else 0
            
            metrics[f'{timeframe}_{class_name}_pred_pct'] = pred_pct
            metrics[f'{timeframe}_{class_name}_true_pct'] = true_pct
            metrics[f'{timeframe}_{class_name}_diff_pct'] = abs(pred_pct - true_pct)
            
            # –¢–æ—á–Ω–æ—Å—Ç—å –¥–ª—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ –∫–ª–∞—Å—Å–∞ (precision)
            if pred_count > 0:
                class_correct = ((predictions == class_id) & (targets == class_id)).sum().item()
                precision = class_correct / pred_count
                metrics[f'{timeframe}_{class_name}_precision'] = precision
            else:
                metrics[f'{timeframe}_{class_name}_precision'] = 0.0
            
            # –ü–æ–ª–Ω–æ—Ç–∞ –¥–ª—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ –∫–ª–∞—Å—Å–∞ (recall)
            if true_count > 0:
                class_correct = ((predictions == class_id) & (targets == class_id)).sum().item()
                recall = class_correct / true_count
                metrics[f'{timeframe}_{class_name}_recall'] = recall
            else:
                metrics[f'{timeframe}_{class_name}_recall'] = 0.0
        
        # –û–±—â–∏–π –¥–∏—Å–±–∞–ª–∞–Ω—Å –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π
        pred_entropy = -sum([
            (c/total_samples) * torch.log(torch.tensor(c/total_samples + 1e-8)) 
            for c in pred_counts.cpu().numpy()
        ]).item() if total_samples > 0 else 0
        
        max_entropy = torch.log(torch.tensor(3.0)).item()  # log(3) –¥–ª—è 3 –∫–ª–∞—Å—Å–æ–≤
        normalized_entropy = pred_entropy / max_entropy if max_entropy > 0 else 0
        
        metrics[f'{timeframe}_prediction_entropy'] = normalized_entropy
        metrics[f'{timeframe}_prediction_diversity'] = len(unique_pred)
        
        return metrics
    
    def log_confusion_matrix(self, val_loader: DataLoader, epoch: int):
        """
        –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ confusion matrix –¥–ª—è direction –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π
        
        Args:
            val_loader: DataLoader –¥–ª—è –≤–∞–ª–∏–¥–∞—Ü–∏–∏
            epoch: –Ω–æ–º–µ—Ä —ç–ø–æ—Ö–∏
        """
        self.model.eval()
        
        # –°–ª–æ–≤–∞—Ä—å –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è confusion matrices –ø–æ —Ç–∞–π–º—Ñ—Ä–µ–π–º–∞–º
        confusion_matrices = {tf: torch.zeros(3, 3, dtype=torch.long) for tf in ['15m', '1h', '4h', '12h']}
        
        with torch.no_grad():
            for inputs, targets, _ in tqdm(val_loader, desc="Computing Confusion Matrix", leave=False):
                inputs = inputs.to(self.device)
                targets = targets.to(self.device)
                
                # –ü—Ä–∏–≤–æ–¥–∏–º targets –∫ –ø—Ä–∞–≤–∏–ª—å–Ω–æ–π —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏ –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
                if targets.dim() == 3 and targets.shape[1] == 1:
                    targets = targets.squeeze(1)
                
                outputs = self.model(inputs)
                
                # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∫–∞–∂–¥—ã–π —Ç–∞–π–º—Ñ—Ä–µ–π–º
                for i, tf in enumerate(['15m', '1h', '4h', '12h']):
                    if hasattr(outputs, '_direction_logits'):
                        direction_logits = outputs._direction_logits[:, i, :]
                        pred_classes = torch.argmax(torch.softmax(direction_logits, dim=-1), dim=-1)
                    else:
                        direction_outputs = outputs[:, 4+i]
                        pred_classes = torch.round(direction_outputs).clamp(0, 2).long()
                    
                    true_classes = targets[:, 4+i].long()
                    
                    # –û–±–Ω–æ–≤–ª—è–µ–º confusion matrix
                    for t, p in zip(true_classes, pred_classes):
                        confusion_matrices[tf][t.item(), p.item()] += 1
        
        # –õ–æ–≥–∏—Ä—É–µ–º confusion matrices
        self.logger.info(f"\n{'='*60}")
        self.logger.info(f"üìä Confusion Matrices - Epoch {epoch}")
        self.logger.info(f"{'='*60}")
        
        for tf, cm in confusion_matrices.items():
            self.logger.info(f"\nüïê –¢–∞–π–º—Ñ—Ä–µ–π–º {tf}:")
            self.logger.info("   Pred‚Üí  LONG  SHORT  FLAT")
            self.logger.info("True‚Üì")
            
            class_names = ['LONG', 'SHORT', 'FLAT']
            for i, class_name in enumerate(class_names):
                row = cm[i]
                total = row.sum().item()
                if total > 0:
                    percentages = (row.float() / total * 100).numpy()
                    self.logger.info(f"{class_name:5s}  {row[0]:5d} {row[1]:5d} {row[2]:5d}  "
                                   f"({percentages[0]:4.1f}% {percentages[1]:4.1f}% {percentages[2]:4.1f}%)")
                else:
                    self.logger.info(f"{class_name:5s}     0     0     0")
            
            # –û–±—â–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å
            correct = cm.diag().sum().item()
            total = cm.sum().item()
            accuracy = correct / total if total > 0 else 0
            self.logger.info(f"\n–û–±—â–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å: {accuracy:.3f} ({correct}/{total})")
            
            # F1 scores –ø–æ –∫–ª–∞—Å—Å–∞–º
            f1_scores = []
            for i in range(3):
                tp = cm[i, i].item()
                fp = cm[:, i].sum().item() - tp
                fn = cm[i, :].sum().item() - tp
                
                precision = tp / (tp + fp) if (tp + fp) > 0 else 0
                recall = tp / (tp + fn) if (tp + fn) > 0 else 0
                f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0
                f1_scores.append(f1)
                
                self.logger.info(f"F1 {class_names[i]}: {f1:.3f} (P={precision:.3f}, R={recall:.3f})")
            
            # Macro F1
            macro_f1 = np.mean(f1_scores)
            self.logger.info(f"Macro F1: {macro_f1:.3f}")
        
        self.logger.info(f"{'='*60}\n")
    
    def validate_with_enhanced_metrics(self, val_loader: DataLoader) -> Dict[str, float]:
        """–í–∞–ª–∏–¥–∞—Ü–∏—è —Å —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–º–∏ –º–µ—Ç—Ä–∏–∫–∞–º–∏ –¥–ª—è direction –∏ trading"""
        self.model.eval()
        
        val_loss = 0.0
        all_outputs = []
        all_targets = []
        
        with torch.no_grad():
            for batch_idx, (inputs, targets, info) in enumerate(tqdm(val_loader, desc="Enhanced Validation", leave=False)):
                # –ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π –ø–µ—Ä–µ–Ω–æ—Å –Ω–∞ GPU
                inputs = inputs.to(self.device, non_blocking=True)
                if isinstance(targets, torch.Tensor):
                    targets = targets.to(self.device, non_blocking=True)
                elif isinstance(targets, dict):
                    targets = {k: v.to(self.device, non_blocking=True) for k, v in targets.items()}
                
                # Forward pass —Å AMP
                if self.use_amp:
                    with autocast():
                        outputs = self.model(inputs)
                        loss = self._compute_loss(outputs, targets)
                else:
                    outputs = self.model(inputs)
                    loss = self._compute_loss(outputs, targets)
                
                # –ù–∞–∫–æ–ø–ª–µ–Ω–∏–µ loss
                val_loss += loss.detach()
                
                # –°–æ—Ö—Ä–∞–Ω—è–µ–º outputs –∏ targets –¥–ª—è —Ä–∞—Å—á–µ—Ç–∞ –º–µ—Ç—Ä–∏–∫
                all_outputs.append(outputs.detach().cpu())
                all_targets.append(targets.detach().cpu())
                
                # –ü–µ—Ä–∏–æ–¥–∏—á–µ—Å–∫–∞—è –æ—á–∏—Å—Ç–∫–∞ –∫—ç—à–∞
                if self.device.type == 'cuda' and batch_idx % 50 == 0:
                    torch.cuda.empty_cache()
        
        # –§–∏–Ω–∞–ª—å–Ω–∞—è —Å–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ü–∏—è
        if self.device.type == 'cuda':
            torch.cuda.synchronize()
        
        # –û–±—ä–µ–¥–∏–Ω—è–µ–º –≤—Å–µ –±–∞—Ç—á–∏
        all_outputs = torch.cat(all_outputs, dim=0)
        all_targets = torch.cat(all_targets, dim=0)
        
        # –ü—Ä–∏–≤–æ–¥–∏–º targets –∫ –ø—Ä–∞–≤–∏–ª—å–Ω–æ–π —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏ –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
        if all_targets.dim() == 3 and all_targets.shape[1] == 1:
            all_targets = all_targets.squeeze(1)
        
        # –†–∞—Å—á–µ—Ç –±–∞–∑–æ–≤—ã—Ö –º–µ—Ç—Ä–∏–∫
        avg_val_loss = (val_loss / len(val_loader)).item()
        metrics = {'val_loss': avg_val_loss}
        
        # –†–∞—Å—á–µ—Ç enhanced –º–µ—Ç—Ä–∏–∫
        try:
            direction_metrics = self.compute_direction_metrics(all_outputs, all_targets)
            trading_metrics = self.compute_trading_metrics(all_outputs, all_targets)
            
            metrics.update(direction_metrics)
            metrics.update(trading_metrics)
            
            # –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –∫–ª—é—á–µ–≤—ã—Ö –º–µ—Ç—Ä–∏–∫
            if 'direction_accuracy_overall' in metrics:
                self.logger.info(f"üìä Direction Accuracy: {metrics['direction_accuracy_overall']:.3f}")
            
            # –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π
            if 'pred_entropy_overall' in metrics:
                self.logger.info(f"üé≤ Prediction Diversity: Entropy={metrics['pred_entropy_overall']:.3f} "
                               f"(LONG: {metrics.get('pred_long_ratio_overall', 0):.1%}, "
                               f"SHORT: {metrics.get('pred_short_ratio_overall', 0):.1%}, "
                               f"FLAT: {metrics.get('pred_flat_ratio_overall', 0):.1%})")
            
            # –î–æ–±–∞–≤–ª—è–µ–º –¥–µ—Ç–∞–ª—å–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏ –ø–æ –∫–ª–∞—Å—Å–∞–º
            timeframes = ['15m', '1h', '4h', '12h']
            for i, tf in enumerate(timeframes):
                # –ò–∑–≤–ª–µ–∫–∞–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –∏ —Ü–µ–ª–∏ –¥–ª—è —Ç–µ–∫—É—â–µ–≥–æ —Ç–∞–π–º—Ñ—Ä–µ–π–º–∞
                if hasattr(all_outputs, '_direction_logits'):
                    direction_logits = all_outputs._direction_logits[:, i, :]
                    pred_classes = torch.argmax(torch.softmax(direction_logits, dim=-1), dim=-1)
                else:
                    direction_outputs = all_outputs[:, 4+i]
                    pred_classes = torch.round(direction_outputs).clamp(0, 2).long()
                
                true_classes = all_targets[:, 4+i].long()
                
                # –†–∞—Å—á–µ—Ç –º–µ—Ç—Ä–∏–∫ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –∫–ª–∞—Å—Å–æ–≤
                class_metrics = self.compute_class_distribution_metrics(pred_classes, true_classes, tf)
                metrics.update(class_metrics)
                
                # –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –∫–ª—é—á–µ–≤—ã—Ö –º–µ—Ç—Ä–∏–∫ –ø–æ –∫–ª–∞—Å—Å–∞–º
                if i == 0:  # –î–µ—Ç–∞–ª—å–Ω–æ –ª–æ–≥–∏—Ä—É–µ–º —Ç–æ–ª—å–∫–æ –¥–ª—è –ø–µ—Ä–≤–æ–≥–æ —Ç–∞–π–º—Ñ—Ä–µ–π–º–∞
                    self.logger.info(f"\nüìä –î–µ—Ç–∞–ª—å–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏ –∫–ª–∞—Å—Å–æ–≤ –¥–ª—è {tf}:")
                    for class_name in ['LONG', 'SHORT', 'FLAT']:
                        pred_pct = class_metrics.get(f'{tf}_{class_name}_pred_pct', 0)
                        true_pct = class_metrics.get(f'{tf}_{class_name}_true_pct', 0)
                        precision = class_metrics.get(f'{tf}_{class_name}_precision', 0)
                        recall = class_metrics.get(f'{tf}_{class_name}_recall', 0)
                        
                        self.logger.info(f"   {class_name}: pred={pred_pct:.1f}% true={true_pct:.1f}% | "
                                       f"precision={precision:.3f} recall={recall:.3f}")
            
            # –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏ –µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–Ω–∞
            if hasattr(all_outputs, '_confidence_scores'):
                confidence_scores = all_outputs._confidence_scores.cpu()
                # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –∏–∑ [-1, 1] –≤ [0, 1] (—Ç–∞–∫ –∫–∞–∫ –∏—Å–ø–æ–ª—å–∑—É–µ–º Tanh –≤ –º–æ–¥–µ–ª–∏)
                confidence_probs = (confidence_scores + 1) / 2
                
                avg_confidence = confidence_probs.mean().item()
                min_confidence = confidence_probs.min().item()
                max_confidence = confidence_probs.max().item()
                
                self.logger.info(f"üí™ Confidence Scores: avg={avg_confidence:.3f}, "
                               f"min={min_confidence:.3f}, max={max_confidence:.3f}")
                
                # –ü—Ä–æ—Ü–µ–Ω—Ç –≤—ã—Å–æ–∫–æ—É–≤–µ—Ä–µ–Ω–Ω—ã—Ö –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π
                high_conf_threshold = 0.6
                high_conf_ratio = (confidence_probs > high_conf_threshold).float().mean().item()
                self.logger.info(f"   –í—ã—Å–æ–∫–æ—É–≤–µ—Ä–µ–Ω–Ω—ã—Ö (>{high_conf_threshold}): {high_conf_ratio:.1%}")
            
            if 'win_rate_overall' in metrics:
                self.logger.info(f"üí∞ Win Rate: {metrics['win_rate_overall']:.3f}")
            
        except Exception as e:
            self.logger.warning(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ —Ä–∞—Å—á–µ—Ç–∞ enhanced –º–µ—Ç—Ä–∏–∫: {e}")
        
        return metrics