"""
Fine-tuning –º–æ–¥—É–ª—å –¥–ª—è –¥–æ–æ–±—É—á–µ–Ω–∏—è —Å—É—â–µ—Å—Ç–≤—É—é—â–µ–π –º–æ–¥–µ–ª–∏
–ò—Å–ø–æ–ª—å–∑—É–µ—Ç –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω—ã–µ –≤–µ—Å–∞ –∏ –æ–ø—Ç–∏–º–∏–∑–∏—Ä—É–µ—Ç —Ç–æ–ª—å–∫–æ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω—ã–µ —Å–ª–æ–∏
"""

import torch
import torch.nn as nn
from torch.utils.data import DataLoader
import numpy as np
from typing import Dict, Optional, List
from pathlib import Path
import yaml
from tqdm import tqdm
import time

from models.patchtst_unified import UnifiedPatchTSTForTrading as UnifiedPatchTST
from training.optimized_trainer import OptimizedTrainer
from utils.logger import get_logger


class FineTuner(OptimizedTrainer):
    """–°–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π trainer –¥–ª—è fine-tuning —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö –º–æ–¥–µ–ª–µ–π"""
    
    def __init__(self, 
                 model: nn.Module, 
                 config: Dict, 
                 checkpoint_path: str,
                 device: Optional[torch.device] = None):
        super().__init__(model, config, device)
        
        self.logger = get_logger("FineTuner")
        self.checkpoint_path = checkpoint_path
        
        # Fine-tuning —Å–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
        self.freeze_backbone = config.get('fine_tuning', {}).get('freeze_backbone', True)
        self.unfreeze_layers = config.get('fine_tuning', {}).get('unfreeze_layers', ['direction', 'confidence'])
        self.curriculum_learning = config.get('fine_tuning', {}).get('curriculum_learning', True)
        self.noise_injection_std = config.get('fine_tuning', {}).get('noise_injection_std', 0.01)
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω—ã–µ –≤–µ—Å–∞
        self._load_pretrained_weights()
        
        # –ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º –∫–∞–∫–∏–µ —Å–ª–æ–∏ –æ–±—É—á–∞—Ç—å
        self._setup_trainable_layers()
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º temperature scaling
        self.temperature = nn.Parameter(torch.ones(1) * 1.5)
        
        # Mixup –ø–∞—Ä–∞–º–µ—Ç—Ä—ã (—É—Å–∏–ª–µ–Ω—ã –¥–ª—è fine-tuning)
        self.use_mixup = True
        self.mixup_alpha = config.get('fine_tuning', {}).get('mixup_alpha', 0.3)
        
        self.logger.info(f"‚úÖ Fine-tuning –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω —Å checkpoint: {checkpoint_path}")
        self.logger.info(f"   - –ó–∞–º–æ—Ä–æ–∂–µ–Ω backbone: {self.freeze_backbone}")
        self.logger.info(f"   - –û–±—É—á–∞–µ–º—ã–µ —Å–ª–æ–∏: {self.unfreeze_layers}")
        self.logger.info(f"   - Curriculum learning: {self.curriculum_learning}")
        self.logger.info(f"   - Mixup alpha: {self.mixup_alpha}")
    
    def _load_pretrained_weights(self):
        """–ó–∞–≥—Ä—É–∑–∫–∞ –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω—ã—Ö –≤–µ—Å–æ–≤"""
        checkpoint = torch.load(self.checkpoint_path, map_location=self.device)
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –≤–µ—Å–∞ –º–æ–¥–µ–ª–∏
        if 'model_state_dict' in checkpoint:
            self.model.load_state_dict(checkpoint['model_state_dict'])
        else:
            self.model.load_state_dict(checkpoint)
            
        self.logger.info(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω—ã –≤–µ—Å–∞ –∏–∑ {self.checkpoint_path}")
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–µ—Ç—Ä–∏–∫–∏ –µ—Å–ª–∏ –µ—Å—Ç—å
        if 'metrics' in checkpoint:
            self.logger.info(f"üìä –ü—Ä–µ–¥—ã–¥—É—â–∏–µ –º–µ—Ç—Ä–∏–∫–∏:")
            for key, value in checkpoint['metrics'].items():
                self.logger.info(f"   - {key}: {value:.4f}")
    
    def _setup_trainable_layers(self):
        """–ù–∞—Å—Ç—Ä–æ–π–∫–∞ –æ–±—É—á–∞–µ–º—ã—Ö —Å–ª–æ–µ–≤"""
        # –°–Ω–∞—á–∞–ª–∞ –∑–∞–º–æ—Ä–∞–∂–∏–≤–∞–µ–º –≤—Å–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
        if self.freeze_backbone:
            for param in self.model.parameters():
                param.requires_grad = False
                
        # –†–∞–∑–º–æ—Ä–∞–∂–∏–≤–∞–µ–º —Ç–æ–ª—å–∫–æ —É–∫–∞–∑–∞–Ω–Ω—ã–µ —Å–ª–æ–∏
        trainable_params = 0
        total_params = 0
        
        for name, param in self.model.named_parameters():
            total_params += param.numel()
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –Ω—É–∂–Ω–æ –ª–∏ —Ä–∞–∑–º–æ—Ä–æ–∑–∏—Ç—å —ç—Ç–æ—Ç —Å–ª–æ–π
            should_unfreeze = any(layer_name in name for layer_name in self.unfreeze_layers)
            
            if should_unfreeze:
                param.requires_grad = True
                trainable_params += param.numel()
                if param.dim() > 1:  # –¢–æ–ª—å–∫–æ –¥–ª—è –º–∞—Ç—Ä–∏—Ü –≤–µ—Å–æ–≤
                    self.logger.debug(f"   ‚úÖ –†–∞–∑–º–æ—Ä–æ–∂–µ–Ω: {name} ({param.shape})")
        
        # –í—Å–µ–≥–¥–∞ —Ä–∞–∑–º–æ—Ä–∞–∂–∏–≤–∞–µ–º temperature –¥–ª—è calibration
        self.temperature.requires_grad = True
        
        self.logger.info(f"üìä –û–±—É—á–∞–µ–º—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã: {trainable_params:,} / {total_params:,} "
                        f"({100 * trainable_params / total_params:.1f}%)")
    
    def inject_noise(self, inputs: torch.Tensor) -> torch.Tensor:
        """–î–æ–±–∞–≤–ª–µ–Ω–∏–µ —à—É–º–∞ –∫ –≤—Ö–æ–¥–Ω—ã–º –¥–∞–Ω–Ω—ã–º –¥–ª—è —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏–∏"""
        if self.training and self.noise_injection_std > 0:
            noise = torch.randn_like(inputs) * self.noise_injection_std
            return inputs + noise
        return inputs
    
    def apply_temperature_scaling(self, logits: torch.Tensor) -> torch.Tensor:
        """–ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ temperature scaling –¥–ª—è –∫–∞–ª–∏–±—Ä–æ–≤–∫–∏ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏"""
        return logits / self.temperature
    
    def get_curriculum_weights(self, targets: Dict[str, torch.Tensor], epoch: int) -> torch.Tensor:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –≤–µ—Å–æ–≤ –¥–ª—è curriculum learning"""
        if not self.curriculum_learning:
            return torch.ones(targets['direction_15m'].shape[0], device=self.device)
        
        # –ù–∞—á–∏–Ω–∞–µ–º —Å –ø—Ä–æ—Å—Ç—ã—Ö –ø—Ä–∏–º–µ—Ä–æ–≤ (–≤—ã—Å–æ–∫–∞—è –≤–æ–ª–∞—Ç–∏–ª—å–Ω–æ—Å—Ç—å)
        future_returns = targets.get('future_return_15m', torch.zeros_like(targets['direction_15m']))
        volatility = torch.abs(future_returns)
        
        # –ü—Ä–æ–≥—Ä–µ—Å—Å –æ–±—É—á–µ–Ω–∏—è (0 -> 1)
        progress = min(epoch / 20, 1.0)  # –ü–æ–ª–Ω—ã–π curriculum –∑–∞ 20 —ç–ø–æ—Ö
        
        # –í –Ω–∞—á–∞–ª–µ —Ñ–æ–∫—É—Å–∏—Ä—É–µ–º—Å—è –Ω–∞ –≤—ã—Å–æ–∫–æ–π –≤–æ–ª–∞—Ç–∏–ª—å–Ω–æ—Å—Ç–∏
        # –ü–æ–∑–∂–µ –≤–∫–ª—é—á–∞–µ–º –≤—Å–µ –ø—Ä–∏–º–µ—Ä—ã
        threshold = torch.quantile(volatility, 1.0 - progress * 0.8)  # –û—Ç 20% –¥–æ 100% –¥–∞–Ω–Ω—ã—Ö
        
        weights = torch.ones_like(volatility)
        weights[volatility < threshold] = 0.1 + 0.9 * progress  # –ü–ª–∞–≤–Ω–æ–µ —É–≤–µ–ª–∏—á–µ–Ω–∏–µ –≤–µ—Å–∞
        
        return weights
    
    def train_epoch(self, train_loader: DataLoader) -> Dict[str, float]:
        """–ú–æ–¥–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω–∞—è —ç–ø–æ—Ö–∞ –æ–±—É—á–µ–Ω–∏—è —Å fine-tuning —Ç–µ—Ö–Ω–∏–∫–∞–º–∏"""
        self.model.train()
        
        epoch_loss = 0.0
        epoch_metrics = {}
        
        progress_bar = tqdm(train_loader, desc="Fine-tuning", leave=False)
        
        for batch_idx, (inputs, targets, info) in enumerate(progress_bar):
            # –ü–µ—Ä–µ–Ω–æ—Å –Ω–∞ GPU
            inputs = inputs.to(self.device, non_blocking=True)
            if isinstance(targets, torch.Tensor):
                targets = targets.to(self.device, non_blocking=True)
            else:
                targets = {k: v.to(self.device, non_blocking=True) 
                          for k, v in targets.items()}
            
            # –î–æ–±–∞–≤–ª—è–µ–º —à—É–º –∫ –≤—Ö–æ–¥–Ω—ã–º –¥–∞–Ω–Ω—ã–º
            inputs = self.inject_noise(inputs)
            
            # –ü—Ä–∏–º–µ–Ω—è–µ–º mixup —Å —É–≤–µ–ª–∏—á–µ–Ω–Ω—ã–º alpha
            if self.use_mixup and np.random.rand() < 0.5:  # 50% –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å mixup
                inputs, targets_a, targets_b, lam = self.mixup_data(inputs, targets, self.mixup_alpha)
                
                # Forward pass —Å mixup
                with torch.cuda.amp.autocast(enabled=self.use_amp):
                    outputs = self.model(inputs)
                    
                    # Temperature scaling –¥–ª—è direction –ª–æ–≥–∏—Ç–æ–≤
                    if hasattr(outputs, '_direction_logits'):
                        outputs._direction_logits = self.apply_temperature_scaling(outputs._direction_logits)
                    
                    # Mixup loss
                    loss_a = self.loss_fn(outputs, targets_a)
                    loss_b = self.loss_fn(outputs, targets_b)
                    loss = lam * loss_a + (1 - lam) * loss_b
            else:
                # –û–±—ã—á–Ω—ã–π forward pass
                with torch.cuda.amp.autocast(enabled=self.use_amp):
                    outputs = self.model(inputs)
                    
                    # Temperature scaling
                    if hasattr(outputs, '_direction_logits'):
                        outputs._direction_logits = self.apply_temperature_scaling(outputs._direction_logits)
                    
                    loss = self.loss_fn(outputs, targets)
            
            # Curriculum learning –≤–µ—Å–∞
            if self.curriculum_learning:
                curriculum_weights = self.get_curriculum_weights(targets, self.current_epoch)
                loss = (loss * curriculum_weights).mean()
            
            # Backward pass
            if self.use_amp:
                self.scaler.scale(loss).backward()
                if (batch_idx + 1) % self.gradient_accumulation_steps == 0:
                    self.scaler.unscale_(self.optimizer)
                    if self.gradient_clip > 0:
                        torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.gradient_clip)
                    self.scaler.step(self.optimizer)
                    self.scaler.update()
                    self.optimizer.zero_grad(set_to_none=True)
                    
                    # –û–±–Ω–æ–≤–ª—è–µ–º EMA
                    self._update_ema()
            else:
                loss.backward()
                if (batch_idx + 1) % self.gradient_accumulation_steps == 0:
                    if self.gradient_clip > 0:
                        torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.gradient_clip)
                    self.optimizer.step()
                    self.optimizer.zero_grad(set_to_none=True)
                    
                    # –û–±–Ω–æ–≤–ª—è–µ–º EMA
                    self._update_ema()
            
            # –ù–∞–∫–∞–ø–ª–∏–≤–∞–µ–º –º–µ—Ç—Ä–∏–∫–∏
            batch_loss = loss.detach() * self.gradient_accumulation_steps
            epoch_loss += batch_loss
            
            # –û–±–Ω–æ–≤–ª—è–µ–º –ø—Ä–æ–≥—Ä–µ—Å—Å –±–∞—Ä
            progress_bar.set_postfix({
                'loss': f'{batch_loss.item():.4f}',
                'lr': f'{self.optimizer.param_groups[0]["lr"]:.2e}'
            })
        
        # –°—Ä–µ–¥–Ω–∏–µ –º–µ—Ç—Ä–∏–∫–∏ –∑–∞ —ç–ø–æ—Ö—É
        num_batches = len(train_loader)
        epoch_metrics['loss'] = (epoch_loss / num_batches).item()
        
        return epoch_metrics
    
    def save_checkpoint(self, epoch: int, metrics: Dict[str, float], is_best: bool = False):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ checkpoint —Å –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π –æ fine-tuning"""
        checkpoint = {
            'epoch': epoch,
            'model_state_dict': self.model.state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict(),
            'scheduler_state_dict': self.scheduler.state_dict() if self.scheduler else None,
            'metrics': metrics,
            'temperature': self.temperature.item(),
            'fine_tuning_config': {
                'original_checkpoint': self.checkpoint_path,
                'freeze_backbone': self.freeze_backbone,
                'unfreeze_layers': self.unfreeze_layers,
                'mixup_alpha': self.mixup_alpha,
                'noise_injection_std': self.noise_injection_std
            }
        }
        
        if self.use_ema and self.ema_model is not None:
            checkpoint['ema_state_dict'] = self.ema_model.state_dict()
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º
        timestamp = time.strftime('%Y%m%d_%H%M%S')
        if is_best:
            path = Path(f'models_saved/best_finetuned_model_{timestamp}.pth')
        else:
            path = Path(f'models_saved/checkpoint_finetuned_epoch_{epoch}_{timestamp}.pth')
            
        torch.save(checkpoint, path)
        self.logger.info(f"üíæ Checkpoint —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {path}")
        
        return path


def create_fine_tuner(config: Dict, checkpoint_path: str, device: Optional[torch.device] = None) -> FineTuner:
    """–§–∞–±—Ä–∏—á–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è FineTuner"""
    if device is None:
        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    
    # –°–æ–∑–¥–∞–µ–º –º–æ–¥–µ–ª—å
    model = UnifiedPatchTST(config).to(device)
    
    # –°–æ–∑–¥–∞–µ–º FineTuner
    fine_tuner = FineTuner(
        model=model,
        config=config,
        checkpoint_path=checkpoint_path,
        device=device
    )
    
    return fine_tuner