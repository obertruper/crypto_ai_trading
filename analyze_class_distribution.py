#!/usr/bin/env python3
"""
–ê–Ω–∞–ª–∏–∑ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –∫–ª–∞—Å—Å–æ–≤ –≤ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –ø–æ–Ω–∏–º–∞–Ω–∏—è –¥–∏—Å–±–∞–ª–∞–Ω—Å–∞
"""

import torch
import numpy as np
import pandas as pd
from collections import Counter
import matplotlib.pyplot as plt
import seaborn as sns
from pathlib import Path
from datetime import datetime

from data.precomputed_dataset import create_precomputed_data_loaders
from utils.logger import get_logger

logger = get_logger("ClassDistributionAnalysis")

def analyze_direction_distribution(dataloader, dataset_name="Dataset"):
    """
    –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–ª–∞—Å—Å–æ–≤ direction –≤ –¥–∞—Ç–∞—Å–µ—Ç–µ
    """
    logger.info(f"\n{'='*60}")
    logger.info(f"üìä –ê–Ω–∞–ª–∏–∑ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –∫–ª–∞—Å—Å–æ–≤ –¥–ª—è {dataset_name}")
    logger.info(f"{'='*60}")
    
    all_directions = {
        'direction_15m': [],
        'direction_1h': [],
        'direction_4h': [],
        'direction_12h': []
    }
    
    all_returns = {
        'future_return_15m': [],
        'future_return_1h': [],
        'future_return_4h': [],
        'future_return_12h': []
    }
    
    total_samples = 0
    
    # –°–æ–±–∏—Ä–∞–µ–º –¥–∞–Ω–Ω—ã–µ
    for batch_idx, (inputs, targets, info) in enumerate(dataloader):
        if batch_idx % 10 == 0:
            logger.info(f"–û–±—Ä–∞–±–æ—Ç–∫–∞ –±–∞—Ç—á–∞ {batch_idx}...")
        
        # –ü—Ä–∏–≤–æ–¥–∏–º targets –∫ –ø—Ä–∞–≤–∏–ª—å–Ω–æ–π —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏
        if targets.dim() == 3 and targets.shape[1] == 1:
            targets = targets.squeeze(1)
        
        batch_size = targets.shape[0]
        total_samples += batch_size
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º direction –∫–ª–∞—Å—Å—ã (–∏–Ω–¥–µ–∫—Å—ã 4-7)
        for i, timeframe in enumerate(['15m', '1h', '4h', '12h']):
            direction_key = f'direction_{timeframe}'
            directions = targets[:, 4 + i].cpu().numpy()
            all_directions[direction_key].extend(directions)
            
            # –¢–∞–∫–∂–µ —Å–æ–±–∏—Ä–∞–µ–º returns –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
            return_key = f'future_return_{timeframe}'
            returns = targets[:, i].cpu().numpy()
            all_returns[return_key].extend(returns)
    
    logger.info(f"\n‚úÖ –í—Å–µ–≥–æ –ø—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–æ: {total_samples} –ø—Ä–∏–º–µ—Ä–æ–≤")
    
    # –ê–Ω–∞–ª–∏–∑ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –∫–ª–∞—Å—Å–æ–≤
    results = {}
    
    for timeframe in ['15m', '1h', '4h', '12h']:
        direction_key = f'direction_{timeframe}'
        directions = np.array(all_directions[direction_key])
        
        # –ü–æ–¥—Å—á–µ—Ç –∫–ª–∞—Å—Å–æ–≤
        class_counts = Counter(directions.astype(int))
        total = len(directions)
        
        logger.info(f"\nüìà –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–ª–∞—Å—Å–æ–≤ –¥–ª—è {timeframe}:")
        logger.info(f"   LONG (0):  {class_counts.get(0, 0):6d} ({class_counts.get(0, 0)/total*100:5.1f}%)")
        logger.info(f"   SHORT (1): {class_counts.get(1, 0):6d} ({class_counts.get(1, 0)/total*100:5.1f}%)")
        logger.info(f"   FLAT (2):  {class_counts.get(2, 0):6d} ({class_counts.get(2, 0)/total*100:5.1f}%)")
        
        # –†–∞—Å—á–µ—Ç –≤–µ—Å–æ–≤ –∫–ª–∞—Å—Å–æ–≤ –¥–ª—è –±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∏
        class_weights = {}
        for cls in [0, 1, 2]:
            if class_counts.get(cls, 0) > 0:
                # –ò—Å–ø–æ–ª—å–∑—É–µ–º sqrt –¥–ª—è –±–æ–ª–µ–µ —É–º–µ—Ä–µ–Ω–Ω–æ–π –∫–æ—Ä—Ä–µ–∫—Ü–∏–∏
                weight = np.sqrt(total / (3.0 * class_counts.get(cls, 1)))
                class_weights[cls] = weight
            else:
                class_weights[cls] = 1.0
        
        # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –≤–µ—Å–æ–≤
        mean_weight = np.mean(list(class_weights.values()))
        normalized_weights = {k: v/mean_weight for k, v in class_weights.items()}
        
        logger.info(f"   –†–µ–∫–æ–º–µ–Ω–¥—É–µ–º—ã–µ –≤–µ—Å–∞ –∫–ª–∞—Å—Å–æ–≤:")
        logger.info(f"   LONG:  {normalized_weights[0]:.3f}")
        logger.info(f"   SHORT: {normalized_weights[1]:.3f}")
        logger.info(f"   FLAT:  {normalized_weights[2]:.3f}")
        
        # –ê–Ω–∞–ª–∏–∑ returns
        return_key = f'future_return_{timeframe}'
        returns = np.array(all_returns[return_key])
        
        logger.info(f"\n   –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ returns:")
        logger.info(f"   Mean: {np.mean(returns):.4f}%")
        logger.info(f"   Std:  {np.std(returns):.4f}%")
        logger.info(f"   Min:  {np.min(returns):.4f}%")
        logger.info(f"   Max:  {np.max(returns):.4f}%")
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        results[timeframe] = {
            'class_distribution': dict(class_counts),
            'class_weights': normalized_weights,
            'total_samples': total,
            'return_stats': {
                'mean': float(np.mean(returns)),
                'std': float(np.std(returns)),
                'min': float(np.min(returns)),
                'max': float(np.max(returns))
            }
        }
    
    return results

def create_distribution_plots(results, save_path="experiments/class_distribution"):
    """
    –°–æ–∑–¥–∞–µ—Ç –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –∫–ª–∞—Å—Å–æ–≤
    """
    Path(save_path).mkdir(parents=True, exist_ok=True)
    
    # –ì—Ä–∞—Ñ–∏–∫ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –∫–ª–∞—Å—Å–æ–≤
    fig, axes = plt.subplots(2, 2, figsize=(12, 10))
    axes = axes.ravel()
    
    for idx, timeframe in enumerate(['15m', '1h', '4h', '12h']):
        ax = axes[idx]
        
        dist = results[timeframe]['class_distribution']
        classes = ['LONG', 'SHORT', 'FLAT']
        counts = [dist.get(0, 0), dist.get(1, 0), dist.get(2, 0)]
        
        # –ë–∞—Ä–ø–ª–æ—Ç
        bars = ax.bar(classes, counts, color=['green', 'red', 'gray'])
        
        # –î–æ–±–∞–≤–ª—è–µ–º –ø—Ä–æ—Ü–µ–Ω—Ç—ã
        total = sum(counts)
        for bar, count in zip(bars, counts):
            height = bar.get_height()
            ax.annotate(f'{count/total*100:.1f}%',
                       xy=(bar.get_x() + bar.get_width() / 2, height),
                       xytext=(0, 3),
                       textcoords="offset points",
                       ha='center', va='bottom')
        
        ax.set_title(f'–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–ª–∞—Å—Å–æ–≤ - {timeframe}')
        ax.set_ylabel('–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–∏–º–µ—Ä–æ–≤')
        ax.set_ylim(0, max(counts) * 1.1)
    
    plt.tight_layout()
    plt.savefig(f"{save_path}/class_distribution.png", dpi=300)
    plt.close()
    
    # –ì—Ä–∞—Ñ–∏–∫ —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ–º—ã—Ö –≤–µ—Å–æ–≤
    fig, ax = plt.subplots(figsize=(10, 6))
    
    timeframes = ['15m', '1h', '4h', '12h']
    x = np.arange(len(timeframes))
    width = 0.25
    
    long_weights = [results[tf]['class_weights'][0] for tf in timeframes]
    short_weights = [results[tf]['class_weights'][1] for tf in timeframes]
    flat_weights = [results[tf]['class_weights'][2] for tf in timeframes]
    
    ax.bar(x - width, long_weights, width, label='LONG', color='green', alpha=0.8)
    ax.bar(x, short_weights, width, label='SHORT', color='red', alpha=0.8)
    ax.bar(x + width, flat_weights, width, label='FLAT', color='gray', alpha=0.8)
    
    ax.set_xlabel('–¢–∞–π–º—Ñ—Ä–µ–π–º')
    ax.set_ylabel('–†–µ–∫–æ–º–µ–Ω–¥—É–µ–º—ã–π –≤–µ—Å –∫–ª–∞—Å—Å–∞')
    ax.set_title('–†–µ–∫–æ–º–µ–Ω–¥—É–µ–º—ã–µ –≤–µ—Å–∞ –∫–ª–∞—Å—Å–æ–≤ –¥–ª—è –±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∏')
    ax.set_xticks(x)
    ax.set_xticklabels(timeframes)
    ax.legend()
    ax.grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.savefig(f"{save_path}/recommended_weights.png", dpi=300)
    plt.close()
    
    logger.info(f"\n‚úÖ –ì—Ä–∞—Ñ–∏–∫–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ {save_path}/")

def generate_recommendations(results):
    """
    –ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ –∞–Ω–∞–ª–∏–∑–∞
    """
    logger.info(f"\n{'='*60}")
    logger.info("üéØ –†–ï–ö–û–ú–ï–ù–î–ê–¶–ò–ò –î–õ–Ø –£–õ–£–ß–®–ï–ù–ò–Ø –û–ë–£–ß–ï–ù–ò–Ø")
    logger.info(f"{'='*60}")
    
    # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –æ–±—â–∏–π –¥–∏—Å–±–∞–ª–∞–Ω—Å
    all_distributions = []
    for tf in results.values():
        dist = tf['class_distribution']
        total = sum(dist.values())
        all_distributions.append({
            'LONG': dist.get(0, 0) / total,
            'SHORT': dist.get(1, 0) / total,
            'FLAT': dist.get(2, 0) / total
        })
    
    # –°—Ä–µ–¥–Ω–∏–µ –∑–Ω–∞—á–µ–Ω–∏—è
    avg_long = np.mean([d['LONG'] for d in all_distributions])
    avg_short = np.mean([d['SHORT'] for d in all_distributions])
    avg_flat = np.mean([d['FLAT'] for d in all_distributions])
    
    logger.info(f"\nüìä –°—Ä–µ–¥–Ω–∏–π –¥–∏—Å–±–∞–ª–∞–Ω—Å –∫–ª–∞—Å—Å–æ–≤:")
    logger.info(f"   LONG:  {avg_long*100:.1f}%")
    logger.info(f"   SHORT: {avg_short*100:.1f}%")
    logger.info(f"   FLAT:  {avg_flat*100:.1f}%")
    
    # –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏
    logger.info(f"\nüí° –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏:")
    
    if avg_flat > 0.7:
        logger.info("   1. ‚ö†Ô∏è –ö–†–ò–¢–ò–ß–ï–°–ö–ò–ô –¥–∏—Å–±–∞–ª–∞–Ω—Å - FLAT –¥–æ–º–∏–Ω–∏—Ä—É–µ—Ç!")
        logger.info("      - –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å Focal Loss —Å gamma=3-5")
        logger.info("      - –ü—Ä–∏–º–µ–Ω–∏—Ç—å SMOTE –∏–ª–∏ ADASYN –¥–ª—è –±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∏")
        logger.info("      - –£–≤–µ–ª–∏—á–∏—Ç—å –ø–æ—Ä–æ–≥ –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è FLAT (—Å–µ–π—á–∞—Å —Å–ª–∏—à–∫–æ–º —á—É–≤—Å—Ç–≤–∏—Ç–µ–ª—å–Ω—ã–π)")
    
    if avg_long < 0.1 or avg_short < 0.1:
        logger.info("   2. ‚ö†Ô∏è –ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –ø—Ä–∏–º–µ—Ä–æ–≤ LONG –∏–ª–∏ SHORT")
        logger.info("      - –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å weighted sampling –≤ DataLoader")
        logger.info("      - –£–≤–µ–ª–∏—á–∏—Ç—å –≤–µ—Å–∞ —Ä–µ–¥–∫–∏—Ö –∫–ª–∞—Å—Å–æ–≤ –≤ loss")
        logger.info("      - –†–∞—Å—Å–º–æ—Ç—Ä–µ—Ç—å data augmentation –¥–ª—è minority –∫–ª–∞—Å—Å–æ–≤")
    
    # –°—Ä–µ–¥–Ω–∏–µ –≤–µ—Å–∞ –¥–ª—è –∫–æ–Ω—Ñ–∏–≥–∞
    avg_weights = {
        'LONG': np.mean([results[tf]['class_weights'][0] for tf in results]),
        'SHORT': np.mean([results[tf]['class_weights'][1] for tf in results]),
        'FLAT': np.mean([results[tf]['class_weights'][2] for tf in results])
    }
    
    logger.info(f"\nüìù –†–µ–∫–æ–º–µ–Ω–¥—É–µ–º—ã–µ –≤–µ—Å–∞ –¥–ª—è config.yaml:")
    logger.info(f"   class_weights: [{avg_weights['LONG']:.3f}, {avg_weights['SHORT']:.3f}, {avg_weights['FLAT']:.3f}]")
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –≤ —Ñ–∞–π–ª
    with open("experiments/class_distribution/recommendations.txt", "w") as f:
        f.write(f"–ê–Ω–∞–ª–∏–∑ –æ—Ç {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")
        f.write(f"–°—Ä–µ–¥–Ω–∏–π –¥–∏—Å–±–∞–ª–∞–Ω—Å –∫–ª–∞—Å—Å–æ–≤:\n")
        f.write(f"LONG:  {avg_long*100:.1f}%\n")
        f.write(f"SHORT: {avg_short*100:.1f}%\n")
        f.write(f"FLAT:  {avg_flat*100:.1f}%\n\n")
        f.write(f"–†–µ–∫–æ–º–µ–Ω–¥—É–µ–º—ã–µ –≤–µ—Å–∞ –∫–ª–∞—Å—Å–æ–≤:\n")
        f.write(f"[{avg_weights['LONG']:.3f}, {avg_weights['SHORT']:.3f}, {avg_weights['FLAT']:.3f}]\n")

def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –∞–Ω–∞–ª–∏–∑–∞"""
    logger.info("üöÄ –ó–∞–ø—É—Å–∫ –∞–Ω–∞–ª–∏–∑–∞ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –∫–ª–∞—Å—Å–æ–≤...")
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é
    import yaml
    with open('config/config.yaml', 'r') as f:
        config = yaml.safe_load(f)
    
    # –°–æ–∑–¥–∞–µ–º dataloaders
    logger.info("üì• –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö...")
    train_loader, val_loader, test_loader = create_precomputed_data_loaders(config)
    
    # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –∫–∞–∂–¥—ã–π –¥–∞—Ç–∞—Å–µ—Ç
    results = {}
    
    logger.info("\nüîç –ê–Ω–∞–ª–∏–∑ –æ–±—É—á–∞—é—â–µ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞...")
    results['train'] = analyze_direction_distribution(train_loader, "Training Dataset")
    
    logger.info("\nüîç –ê–Ω–∞–ª–∏–∑ –≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞...")
    results['val'] = analyze_direction_distribution(val_loader, "Validation Dataset")
    
    # –°–æ–∑–¥–∞–µ–º –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏
    create_distribution_plots(results['train'], "experiments/class_distribution/train")
    create_distribution_plots(results['val'], "experiments/class_distribution/val")
    
    # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏
    generate_recommendations(results['train'])
    
    logger.info("\n‚úÖ –ê–Ω–∞–ª–∏–∑ –∑–∞–≤–µ—Ä—à–µ–Ω! –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ experiments/class_distribution/")

if __name__ == "__main__":
    main()