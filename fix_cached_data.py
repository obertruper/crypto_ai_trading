#!/usr/bin/env python3
"""
–°–∫—Ä–∏–ø—Ç –¥–ª—è –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏—è –∏ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏ –∫—ç—à–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
–ü—Ä–∏–º–µ–Ω—è–µ—Ç –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—é –∫ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–º parquet —Ñ–∞–π–ª–∞–º
"""

import pandas as pd
import numpy as np
from pathlib import Path
from sklearn.preprocessing import RobustScaler
import pickle
from tqdm import tqdm
import argparse

from utils.logger import get_logger
from data.constants import get_feature_columns


def identify_problematic_columns(df: pd.DataFrame) -> dict:
    """–û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–æ–ª–æ–Ω–æ–∫ —Å —ç–∫—Å—Ç—Ä–µ–º–∞–ª—å–Ω—ã–º–∏ –∑–Ω–∞—á–µ–Ω–∏—è–º–∏"""
    logger = get_logger("DataAnalysis")
    
    problematic_cols = {
        'extreme_values': [],  # > 1e9
        'volume_based': [],
        'price_based': [],
        'ratio_based': []
    }
    
    # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —Å–ª—É–∂–µ–±–Ω—ã–µ –∫–æ–ª–æ–Ω–∫–∏
    service_cols = ['id', 'symbol', 'datetime', 'timestamp', 'sector']
    feature_cols = [col for col in df.columns if col not in service_cols]
    
    for col in feature_cols:
        if col not in df.columns:
            continue
            
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ —ç–∫—Å—Ç—Ä–µ–º–∞–ª—å–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è
        max_val = df[col].max()
        if pd.notna(max_val) and abs(max_val) > 1e9:
            problematic_cols['extreme_values'].append((col, max_val))
        
        # –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è –∫–æ–ª–æ–Ω–æ–∫
        col_lower = col.lower()
        if any(pattern in col_lower for pattern in ['volume', 'turnover', 'obv', 'liquidity', 'cmf', 'mfi']):
            problematic_cols['volume_based'].append(col)
        elif any(pattern in col_lower for pattern in ['price', 'vwap', 'high', 'low', 'open', 'close']):
            problematic_cols['price_based'].append(col)
        elif any(pattern in col_lower for pattern in ['ratio', 'rsi', 'stoch', 'bb_', 'pct', 'toxicity']):
            problematic_cols['ratio_based'].append(col)
    
    logger.info(f"üîç –ù–∞–π–¥–µ–Ω–æ –ø—Ä–æ–±–ª–µ–º–Ω—ã—Ö –∫–æ–ª–æ–Ω–æ–∫:")
    logger.info(f"   - –° —ç–∫—Å—Ç—Ä–µ–º–∞–ª—å–Ω—ã–º–∏ –∑–Ω–∞—á–µ–Ω–∏—è–º–∏ (>1e9): {len(problematic_cols['extreme_values'])}")
    logger.info(f"   - –û–±—ä–µ–º–Ω—ã–µ –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã: {len(problematic_cols['volume_based'])}")
    logger.info(f"   - –¶–µ–Ω–æ–≤—ã–µ –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã: {len(problematic_cols['price_based'])}")
    logger.info(f"   - Ratio –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã: {len(problematic_cols['ratio_based'])}")
    
    return problematic_cols


def normalize_data(df: pd.DataFrame, scaler_path: str = None, fit_scaler: bool = True) -> pd.DataFrame:
    """–ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö —Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º —Å—Ç—Ä—É–∫—Ç—É—Ä—ã"""
    logger = get_logger("DataNormalization")
    
    logger.info("üîß –ù–∞—á–∞–ª–æ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏ –¥–∞–Ω–Ω—ã—Ö...")
    
    # –°–æ–∑–¥–∞–µ–º –∫–æ–ø–∏—é –¥–ª—è —Ä–∞–±–æ—Ç—ã
    df_norm = df.copy()
    
    # –ü–æ–ª—É—á–∞–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –∫–æ–ª–æ–Ω–∫–∞—Ö
    cols_info = identify_problematic_columns(df)
    
    # –°–ª—É–∂–µ–±–Ω—ã–µ –∫–æ–ª–æ–Ω–∫–∏ –∫–æ—Ç–æ—Ä—ã–µ –Ω–µ —Ç—Ä–æ–≥–∞–µ–º
    service_cols = ['id', 'symbol', 'datetime', 'timestamp', 'sector']
    target_prefixes = ['future_return_', 'long_', 'short_', 'best_direction']
    
    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º feature –∫–æ–ª–æ–Ω–∫–∏ –¥–ª—è –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏
    feature_cols = [col for col in df.columns 
                   if col not in service_cols 
                   and not any(col.startswith(prefix) for prefix in target_prefixes)]
    
    logger.info(f"üìä –ö–æ–ª–æ–Ω–æ–∫ –¥–ª—è –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏: {len(feature_cols)}")
    
    # 1. Log-—Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏—è –¥–ª—è –æ–±—ä–µ–º–Ω—ã—Ö –∫–æ–ª–æ–Ω–æ–∫
    for col in cols_info['volume_based']:
        if col in feature_cols:
            logger.info(f"   üìà Log-—Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏—è: {col}")
            # –ó–∞—â–∏—Ç–∞ –æ—Ç –æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π
            df_norm[col] = np.log1p(np.clip(df_norm[col], 0, None))
    
    # 2. –ö–ª–∏–ø–ø–∏–Ω–≥ —ç–∫—Å—Ç—Ä–µ–º–∞–ª—å–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π (–∫—Ä–æ–º–µ ratio)
    for col in feature_cols:
        if col not in cols_info['ratio_based']:
            q99 = df_norm[col].quantile(0.99)
            q01 = df_norm[col].quantile(0.01)
            
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ —ç–∫—Å—Ç—Ä–µ–º–∞–ª—å–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è
            if abs(q99) > 1e6 or abs(q01) < -1e6:
                logger.info(f"   ‚úÇÔ∏è –ö–ª–∏–ø–ø–∏–Ω–≥ {col}: [{q01:.2f}, {q99:.2f}]")
                df_norm[col] = np.clip(df_norm[col], q01, q99)
    
    # 3. RobustScaler –¥–ª—è –≤—Å–µ—Ö feature –∫–æ–ª–æ–Ω–æ–∫
    if feature_cols:
        logger.info("üéØ –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ RobustScaler...")
        
        scaler = RobustScaler(quantile_range=(5, 95))
        
        if fit_scaler:
            # –û–±—É—á–∞–µ–º scaler
            scaler_data = df_norm[feature_cols].values
            scaler.fit(scaler_data)
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º scaler
            if scaler_path:
                Path(scaler_path).parent.mkdir(parents=True, exist_ok=True)
                with open(scaler_path, 'wb') as f:
                    pickle.dump(scaler, f)
                logger.info(f"üíæ Scaler —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {scaler_path}")
        else:
            # –ó–∞–≥—Ä—É–∂–∞–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–π scaler
            if scaler_path and Path(scaler_path).exists():
                with open(scaler_path, 'rb') as f:
                    scaler = pickle.load(f)
                logger.info(f"üì• Scaler –∑–∞–≥—Ä—É–∂–µ–Ω: {scaler_path}")
            else:
                logger.warning("‚ö†Ô∏è Scaler –Ω–µ –Ω–∞–π–¥–µ–Ω, —Å–æ–∑–¥–∞–µ–º –Ω–æ–≤—ã–π")
                scaler.fit(df_norm[feature_cols].values)
        
        # –ü—Ä–∏–º–µ–Ω—è–µ–º scaler
        df_norm[feature_cols] = scaler.transform(df_norm[feature_cols].values)
        
        # –§–∏–Ω–∞–ª—å–Ω—ã–π –∫–ª–∏–ø–ø–∏–Ω–≥ –ø–æ—Å–ª–µ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏
        logger.info("üéØ –§–∏–Ω–∞–ª—å–Ω—ã–π –∫–ª–∏–ø–ø–∏–Ω–≥ [-10, 10]...")
        df_norm[feature_cols] = np.clip(df_norm[feature_cols], -10, 10)
    
    # 4. –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    logger.info("‚úÖ –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏:")
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —ç–∫—Å—Ç—Ä–µ–º–∞–ª—å–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è –ø–æ—Å–ª–µ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏
    extreme_after = 0
    for col in feature_cols:
        max_val = df_norm[col].max()
        if pd.notna(max_val) and abs(max_val) > 100:
            extreme_after += 1
            logger.warning(f"   ‚ö†Ô∏è {col}: max = {max_val:.2f}")
    
    logger.info(f"   - –ö–æ–ª–æ–Ω–æ–∫ —Å —ç–∫—Å—Ç—Ä–µ–º–∞–ª—å–Ω—ã–º–∏ –∑–Ω–∞—á–µ–Ω–∏—è–º–∏: {extreme_after}")
    
    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
    for col in feature_cols[:5]:  # –ü–µ—Ä–≤—ã–µ 5 –¥–ª—è –ø—Ä–∏–º–µ—Ä–∞
        logger.info(f"   - {col}: mean={df_norm[col].mean():.3f}, std={df_norm[col].std():.3f}")
    
    return df_norm


def main():
    parser = argparse.ArgumentParser(description='–ò—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –∏ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –∫—ç—à–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö')
    parser.add_argument('--backup', action='store_true', help='–°–æ–∑–¥–∞—Ç—å —Ä–µ–∑–µ—Ä–≤–Ω—ã–µ –∫–æ–ø–∏–∏')
    parser.add_argument('--scaler-path', default='models_saved/data_scaler.pkl', 
                       help='–ü—É—Ç—å –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è/–∑–∞–≥—Ä—É–∑–∫–∏ scaler')
    parser.add_argument('--dry-run', action='store_true', 
                       help='–¢–æ–ª—å–∫–æ –∞–Ω–∞–ª–∏–∑ –±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏—è —Ñ–∞–π–ª–æ–≤')
    
    args = parser.parse_args()
    
    logger = get_logger("FixCachedData")
    
    logger.info("="*80)
    logger.info("üîß –ò—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –∫—ç—à–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö")
    logger.info("="*80)
    
    # –ü—É—Ç–∏ –∫ —Ñ–∞–π–ª–∞–º
    data_dir = Path("data/processed")
    files = {
        'train': data_dir / "train_data.parquet",
        'val': data_dir / "val_data.parquet",
        'test': data_dir / "test_data.parquet"
    }
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å—É—â–µ—Å—Ç–≤–æ–≤–∞–Ω–∏—è —Ñ–∞–π–ª–æ–≤
    for split, file_path in files.items():
        if not file_path.exists():
            logger.error(f"‚ùå –§–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω: {file_path}")
            logger.info("üí° –°–Ω–∞—á–∞–ª–∞ –∑–∞–ø—É—Å—Ç–∏—Ç–µ: python prepare_trading_data.py")
            return
    
    # –°–æ–∑–¥–∞–Ω–∏–µ —Ä–µ–∑–µ—Ä–≤–Ω—ã—Ö –∫–æ–ø–∏–π
    if args.backup and not args.dry_run:
        backup_dir = data_dir / "backup"
        backup_dir.mkdir(exist_ok=True)
        
        logger.info("üíæ –°–æ–∑–¥–∞–Ω–∏–µ —Ä–µ–∑–µ—Ä–≤–Ω—ã—Ö –∫–æ–ø–∏–π...")
        for split, file_path in files.items():
            backup_path = backup_dir / f"{split}_data_original.parquet"
            if not backup_path.exists():
                import shutil
                shutil.copy(file_path, backup_path)
                logger.info(f"   ‚úÖ {backup_path.name}")
    
    # –û–±—Ä–∞–±–æ—Ç–∫–∞ —Ñ–∞–π–ª–æ–≤
    for split, file_path in files.items():
        logger.info(f"\n{'='*60}")
        logger.info(f"üìÅ –û–±—Ä–∞–±–æ—Ç–∫–∞ {split} –¥–∞–Ω–Ω—ã—Ö: {file_path.name}")
        logger.info(f"{'='*60}")
        
        # –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö
        logger.info("üì• –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö...")
        df = pd.read_parquet(file_path)
        logger.info(f"   –†–∞–∑–º–µ—Ä: {len(df):,} –∑–∞–ø–∏—Å–µ–π, {len(df.columns)} –∫–æ–ª–æ–Ω–æ–∫")
        
        # –ê–Ω–∞–ª–∏–∑ –ø—Ä–æ–±–ª–µ–º
        problems = identify_problematic_columns(df)
        
        if args.dry_run:
            logger.info("\nüîç –†–µ–∂–∏–º –∞–Ω–∞–ª–∏–∑–∞ (dry-run)")
            
            # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Ç–æ–ø-10 –ø—Ä–æ–±–ª–µ–º–Ω—ã—Ö –∫–æ–ª–æ–Ω–æ–∫
            if problems['extreme_values']:
                logger.info("\nüìä –¢–æ–ø-10 –∫–æ–ª–æ–Ω–æ–∫ —Å —ç–∫—Å—Ç—Ä–µ–º–∞–ª—å–Ω—ã–º–∏ –∑–Ω–∞—á–µ–Ω–∏—è–º–∏:")
                sorted_problems = sorted(problems['extreme_values'], 
                                       key=lambda x: abs(x[1]), reverse=True)
                for col, val in sorted_problems[:10]:
                    logger.info(f"   - {col}: {val:.2e}")
            continue
        
        # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è
        fit_scaler = (split == 'train')  # –û–±—É—á–∞–µ–º scaler —Ç–æ–ª—å–∫–æ –Ω–∞ train
        df_normalized = normalize_data(df, 
                                     scaler_path=args.scaler_path,
                                     fit_scaler=fit_scaler)
        
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ
        logger.info(f"üíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö...")
        df_normalized.to_parquet(file_path, index=False)
        logger.info(f"   ‚úÖ –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ: {file_path}")
        
        # –û—Å–≤–æ–±–æ–∂–¥–∞–µ–º –ø–∞–º—è—Ç—å
        del df, df_normalized
    
    logger.info("\n" + "="*80)
    logger.info("‚úÖ –û–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞!")
    logger.info("üöÄ –¢–µ–ø–µ—Ä—å –º–æ–∂–Ω–æ –∑–∞–ø—É—Å–∫–∞—Ç—å –æ–±—É—á–µ–Ω–∏–µ: python main.py --mode train")
    logger.info("="*80)


if __name__ == "__main__":
    main()